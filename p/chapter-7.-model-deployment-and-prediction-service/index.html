<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 7. Model Deployment and Prediction Service ML App Logic data engineering → feature engineering → model → metrics Deploy & Inference Deploy: a loose term that generally means making your model running and accessible Inference: the process of generating predictions To be deployed: model will have to leave the development environment model can be deployed to a staging environment for testing a production environment to be used by end users 1. Machine Learning Deployment Myths 1. Myth1: You Only Deploy One or Two ML Models at a Time 2. Myth2: If we Don’t Do Anything, Model Performance Remains the same. “software rot” or “bit rot” software program degrades over time even if nothing has been changed. ML Systems suffer from data distribution shifts 3. Myth3: You Won’t Need to Update Your Model as Much “How often SHOULD I update my models?” → “How often CAN I update my models?” Model’s performance decays over time → want to update model as fast as possible 4. Myth4: Most ML Engineers Don’t Need to Worry About Scale scale eg) a system that serves hundreds of queries per second of millions of users a month 2. Batch Prediction versus Online Prediction types of predictions Batch prediction, which use only batch features Online prediction that uses only batch features (eg. precomputed embeddings) Online prediction(Streaming prediction) that use both batch features and streaming features 1. Online Prediction when predictions are generated and returned as soon as requests for these predictions arrive on-demand prediction, synchronous prediction 2. Batch Prediction when predictions are generated periodically or whenever triggered. predictions are store somewhere like in-memory or SQL Tables → retrieved as needed asynchronous prediction 3. From Batch Prediction to Online Prediction 1. Online Prediction easy to start problem with online prediction: model might take too long to generate predictions to solve… compute predictions in advance → store in database → fetch then when request arrive → called batch prediction 2. Batch Prediction predictions are precomputed → trick to reduce the inference latency good to generate a lot of predictions and don’t need the results immediately problem of batch prediction: Less responsive to users’ change preferences Need to know what requests to generate predictions in advance 3. Online prediction becomes default As hardware becomes more powerful → Online prediction becomes default To overcome the latency challenge of online prediction: A (near) real-time pipeline that can work with incoming data: extract streaming features → input them into a model → return prediction in a near real time A model that can generate predictions at a speed acceptable to its end users 4. Unifying Batch Pipeline and Streaming Pipeline using sliding features In training this feature is computed in batch Whereas during inference this feature is computed in a streaming pipeline Apache Flink 5. Model Compression Deployed model takes too long to generate predictions: make it do inference faster → inference optimization make the model smaller → model compression originally, to make model fit on edge device make the hardware it’s deployed on run faster model compression low-rank optimization knowledge distillation pruning quantization 1. Low-Rank Factorization key-idea replace high-dimensional tensors with low-dimensional tensors compact convolutional filters replace over-parameterized (having too many parameters) convolutional filters to compact convolutional filters compact blocks to both reduce the number of parameters and increase speed eg) 3x3 conv → 1x1 conv 2. Knowledge Distillation smaller model (student) is train to mimic a larger model or ensemble model (teacher) can work regardless of the architectural differences between teacher and student disadvantages highly dependent on the availability of a teacher network 3. Pruning in neural network, it means remove entire nodes of a neural network changing its architecture and reducing its number of parameters find parameters least useful to predictions and set them to zero(0). do not change architecture, only the number of nonzero parameters sparse architecture make a neural network more sparse require less storage than dense structure 4. Quantization most general and commonly used model compression method reduce model size by using fewer bits to represent its parameters advantage reduce memory size improves the computational speed allows to increase batch size less precision speeds up computation disadvantage rounding numbers → rounding errors small rounding errors → large performance change lower-precision training increasingly popular Fixed-point inference for edge device ML on the Cloud and on the Edge where your model’s computation will happen? ⇒ due to cost of cloud, trend are moving to edge "><title>Chapter 7. Model Deployment and Prediction Service</title>
<link rel=canonical href=https://aiden-jeon.github.io/blog/p/chapter-7.-model-deployment-and-prediction-service/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Chapter 7. Model Deployment and Prediction Service"><meta property='og:description' content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 7. Model Deployment and Prediction Service ML App Logic data engineering → feature engineering → model → metrics Deploy & Inference Deploy: a loose term that generally means making your model running and accessible Inference: the process of generating predictions To be deployed: model will have to leave the development environment model can be deployed to a staging environment for testing a production environment to be used by end users 1. Machine Learning Deployment Myths 1. Myth1: You Only Deploy One or Two ML Models at a Time 2. Myth2: If we Don’t Do Anything, Model Performance Remains the same. “software rot” or “bit rot” software program degrades over time even if nothing has been changed. ML Systems suffer from data distribution shifts 3. Myth3: You Won’t Need to Update Your Model as Much “How often SHOULD I update my models?” → “How often CAN I update my models?” Model’s performance decays over time → want to update model as fast as possible 4. Myth4: Most ML Engineers Don’t Need to Worry About Scale scale eg) a system that serves hundreds of queries per second of millions of users a month 2. Batch Prediction versus Online Prediction types of predictions Batch prediction, which use only batch features Online prediction that uses only batch features (eg. precomputed embeddings) Online prediction(Streaming prediction) that use both batch features and streaming features 1. Online Prediction when predictions are generated and returned as soon as requests for these predictions arrive on-demand prediction, synchronous prediction 2. Batch Prediction when predictions are generated periodically or whenever triggered. predictions are store somewhere like in-memory or SQL Tables → retrieved as needed asynchronous prediction 3. From Batch Prediction to Online Prediction 1. Online Prediction easy to start problem with online prediction: model might take too long to generate predictions to solve… compute predictions in advance → store in database → fetch then when request arrive → called batch prediction 2. Batch Prediction predictions are precomputed → trick to reduce the inference latency good to generate a lot of predictions and don’t need the results immediately problem of batch prediction: Less responsive to users’ change preferences Need to know what requests to generate predictions in advance 3. Online prediction becomes default As hardware becomes more powerful → Online prediction becomes default To overcome the latency challenge of online prediction: A (near) real-time pipeline that can work with incoming data: extract streaming features → input them into a model → return prediction in a near real time A model that can generate predictions at a speed acceptable to its end users 4. Unifying Batch Pipeline and Streaming Pipeline using sliding features In training this feature is computed in batch Whereas during inference this feature is computed in a streaming pipeline Apache Flink 5. Model Compression Deployed model takes too long to generate predictions: make it do inference faster → inference optimization make the model smaller → model compression originally, to make model fit on edge device make the hardware it’s deployed on run faster model compression low-rank optimization knowledge distillation pruning quantization 1. Low-Rank Factorization key-idea replace high-dimensional tensors with low-dimensional tensors compact convolutional filters replace over-parameterized (having too many parameters) convolutional filters to compact convolutional filters compact blocks to both reduce the number of parameters and increase speed eg) 3x3 conv → 1x1 conv 2. Knowledge Distillation smaller model (student) is train to mimic a larger model or ensemble model (teacher) can work regardless of the architectural differences between teacher and student disadvantages highly dependent on the availability of a teacher network 3. Pruning in neural network, it means remove entire nodes of a neural network changing its architecture and reducing its number of parameters find parameters least useful to predictions and set them to zero(0). do not change architecture, only the number of nonzero parameters sparse architecture make a neural network more sparse require less storage than dense structure 4. Quantization most general and commonly used model compression method reduce model size by using fewer bits to represent its parameters advantage reduce memory size improves the computational speed allows to increase batch size less precision speeds up computation disadvantage rounding numbers → rounding errors small rounding errors → large performance change lower-precision training increasingly popular Fixed-point inference for edge device ML on the Cloud and on the Edge where your model’s computation will happen? ⇒ due to cost of cloud, trend are moving to edge "><meta property='og:url' content='https://aiden-jeon.github.io/blog/p/chapter-7.-model-deployment-and-prediction-service/'><meta property='og:site_name' content="Aiden's Camp"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='ml-system'><meta property='article:published_time' content='2022-11-09T00:00:00+00:00'><meta property='article:modified_time' content='2022-11-09T00:00:00+00:00'><meta name=twitter:title content="Chapter 7. Model Deployment and Prediction Service"><meta name=twitter:description content="Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 7. Model Deployment and Prediction Service ML App Logic data engineering → feature engineering → model → metrics Deploy & Inference Deploy: a loose term that generally means making your model running and accessible Inference: the process of generating predictions To be deployed: model will have to leave the development environment model can be deployed to a staging environment for testing a production environment to be used by end users 1. Machine Learning Deployment Myths 1. Myth1: You Only Deploy One or Two ML Models at a Time 2. Myth2: If we Don’t Do Anything, Model Performance Remains the same. “software rot” or “bit rot” software program degrades over time even if nothing has been changed. ML Systems suffer from data distribution shifts 3. Myth3: You Won’t Need to Update Your Model as Much “How often SHOULD I update my models?” → “How often CAN I update my models?” Model’s performance decays over time → want to update model as fast as possible 4. Myth4: Most ML Engineers Don’t Need to Worry About Scale scale eg) a system that serves hundreds of queries per second of millions of users a month 2. Batch Prediction versus Online Prediction types of predictions Batch prediction, which use only batch features Online prediction that uses only batch features (eg. precomputed embeddings) Online prediction(Streaming prediction) that use both batch features and streaming features 1. Online Prediction when predictions are generated and returned as soon as requests for these predictions arrive on-demand prediction, synchronous prediction 2. Batch Prediction when predictions are generated periodically or whenever triggered. predictions are store somewhere like in-memory or SQL Tables → retrieved as needed asynchronous prediction 3. From Batch Prediction to Online Prediction 1. Online Prediction easy to start problem with online prediction: model might take too long to generate predictions to solve… compute predictions in advance → store in database → fetch then when request arrive → called batch prediction 2. Batch Prediction predictions are precomputed → trick to reduce the inference latency good to generate a lot of predictions and don’t need the results immediately problem of batch prediction: Less responsive to users’ change preferences Need to know what requests to generate predictions in advance 3. Online prediction becomes default As hardware becomes more powerful → Online prediction becomes default To overcome the latency challenge of online prediction: A (near) real-time pipeline that can work with incoming data: extract streaming features → input them into a model → return prediction in a near real time A model that can generate predictions at a speed acceptable to its end users 4. Unifying Batch Pipeline and Streaming Pipeline using sliding features In training this feature is computed in batch Whereas during inference this feature is computed in a streaming pipeline Apache Flink 5. Model Compression Deployed model takes too long to generate predictions: make it do inference faster → inference optimization make the model smaller → model compression originally, to make model fit on edge device make the hardware it’s deployed on run faster model compression low-rank optimization knowledge distillation pruning quantization 1. Low-Rank Factorization key-idea replace high-dimensional tensors with low-dimensional tensors compact convolutional filters replace over-parameterized (having too many parameters) convolutional filters to compact convolutional filters compact blocks to both reduce the number of parameters and increase speed eg) 3x3 conv → 1x1 conv 2. Knowledge Distillation smaller model (student) is train to mimic a larger model or ensemble model (teacher) can work regardless of the architectural differences between teacher and student disadvantages highly dependent on the availability of a teacher network 3. Pruning in neural network, it means remove entire nodes of a neural network changing its architecture and reducing its number of parameters find parameters least useful to predictions and set them to zero(0). do not change architecture, only the number of nonzero parameters sparse architecture make a neural network more sparse require less storage than dense structure 4. Quantization most general and commonly used model compression method reduce model size by using fewer bits to represent its parameters advantage reduce memory size improves the computational speed allows to increase batch size less precision speeds up computation disadvantage rounding numbers → rounding errors small rounding errors → large performance change lower-precision training increasingly popular Fixed-point inference for edge device ML on the Cloud and on the Edge where your model’s computation will happen? ⇒ due to cost of cloud, trend are moving to edge "><link rel="shortcut icon" href=/blog/icons/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-419F58RW9W"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-419F58RW9W")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/imgs/avatar_hu6933059792947527008.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🔥</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Aiden's Camp</a></h1><h2 class=site-description>Welcome to Aiden's Camp</h2></div></header><ol class=menu-social><li><a href=https://github.com/Aiden-Jeon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/jongseob-jeon/ target=_blank title=Linkedin rel=me><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 310 310"><g id="XMLID_801_"><path id="XMLID_802_" d="M72.16 99.73H9.927c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5H72.16c2.762.0 5-2.238 5-5V104.73c0-2.76100000000001-2.238-5-5-5z"/><path id="XMLID_803_" d="M41.066.341C18.422.341.0 18.743.0 41.362.0 63.991 18.422 82.4 41.066 82.4c22.626.0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341z"/><path id="XMLID_804_" d="M230.454 94.761c-24.995.0-43.472 10.745-54.679 22.954V104.73c0-2.761-2.238-5-5-5h-59.599c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5h62.097c2.762.0 5-2.238 5-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306.0 27.317 20.818 27.317 48.034v97.204c0 2.762 2.238 5 5 5H305c2.762.0 5-2.238 5-5V194.995C310 145.43 300.549 94.761 230.454 94.761z"/></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/categories><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/blog/tags><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#1-machine-learning-deployment-myths>1. Machine Learning Deployment Myths</a><ul><li><a href=#1-myth1-you-only-deploy-one-or-two-ml-models-at-a-time>1. Myth1: You Only Deploy One or Two ML Models at a Time</a></li><li><a href=#2-myth2-if-we-dont-do-anything-model-performance-remains-the-same>2. Myth2: If we Don’t Do Anything, Model Performance Remains the same.</a></li><li><a href=#3-myth3-you-wont-need-to-update-your-model-as-much>3. Myth3: You Won’t Need to Update Your Model as Much</a></li><li><a href=#4-myth4-most-ml-engineers-dont-need-to-worry-about-scale>4. Myth4: Most ML Engineers Don’t Need to Worry About Scale</a></li></ul></li><li><a href=#2-batch-prediction-versus-online-prediction>2. Batch Prediction versus Online Prediction</a><ul><li><a href=#1-online-prediction>1. Online Prediction</a></li><li><a href=#2-batch-prediction>2. Batch Prediction</a></li></ul></li><li><a href=#3-from-batch-prediction-to-online-prediction>3. From Batch Prediction to Online Prediction</a><ul><li><a href=#1-online-prediction-1>1. Online Prediction</a></li><li><a href=#2-batch-prediction-1>2. Batch Prediction</a></li><li><a href=#3-online-prediction-becomes-default>3. Online prediction becomes default</a></li></ul></li><li><a href=#4-unifying-batch-pipeline-and-streaming-pipeline>4. Unifying Batch Pipeline and Streaming Pipeline</a></li><li><a href=#5-model-compression>5. Model Compression</a><ul><li><a href=#1-low-rank-factorization>1. Low-Rank Factorization</a></li><li><a href=#2-knowledge-distillation>2. Knowledge Distillation</a></li><li><a href=#3-pruning>3. Pruning</a></li><li><a href=#4-quantization>4. Quantization</a></li></ul></li><li><a href=#ml-on-the-cloud-and-on-the-edge>ML on the Cloud and on the Edge</a></li></ul></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/mlops/>Mlops</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/chapter-7.-model-deployment-and-prediction-service/>Chapter 7. Model Deployment and Prediction Service</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Nov 09, 2022</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>4 minute read</time></div></footer></div></header><section class=article-content><p>Summary of <a class=link href=https://learning.oreilly.com/library/view/designing-machine-learning/9781098107956/ target=_blank rel=noopener>Designing Machine Learning Systems</a> written by Chip Huyen.</p><hr><h1 id=chapter-7-model-deployment-and-prediction-service><a href=#chapter-7-model-deployment-and-prediction-service class=header-anchor></a>Chapter 7. Model Deployment and Prediction Service</h1><ul><li>ML App Logic<ul><li>data engineering → feature engineering → model → metrics</li></ul></li><li>Deploy & Inference<ul><li>Deploy: a loose term that generally means making your model running and accessible</li><li>Inference: the process of generating predictions</li></ul></li><li>To be deployed:<ul><li>model will have to leave the development environment</li><li>model can be deployed to<ul><li>a staging environment for testing</li><li>a production environment to be used by end users</li></ul></li></ul></li></ul><h2 id=1-machine-learning-deployment-myths><a href=#1-machine-learning-deployment-myths class=header-anchor></a>1. Machine Learning Deployment Myths</h2><h3 id=1-myth1-you-only-deploy-one-or-two-ml-models-at-a-time><a href=#1-myth1-you-only-deploy-one-or-two-ml-models-at-a-time class=header-anchor></a>1. Myth1: You Only Deploy One or Two ML Models at a Time</h3><h3 id=2-myth2-if-we-dont-do-anything-model-performance-remains-the-same><a href=#2-myth2-if-we-dont-do-anything-model-performance-remains-the-same class=header-anchor></a>2. Myth2: If we Don’t Do Anything, Model Performance Remains the same.</h3><ul><li>“software rot” or “bit rot”<ul><li>software program degrades over time even if nothing has been changed.</li></ul></li><li>ML Systems suffer from data distribution shifts</li></ul><h3 id=3-myth3-you-wont-need-to-update-your-model-as-much><a href=#3-myth3-you-wont-need-to-update-your-model-as-much class=header-anchor></a>3. Myth3: You Won’t Need to Update Your Model as Much</h3><ul><li>“How often <em>SHOULD</em> I update my models?” → “How often <em>CAN</em> I update my models?”</li><li>Model’s performance decays over time → want to update model as fast as possible</li></ul><h3 id=4-myth4-most-ml-engineers-dont-need-to-worry-about-scale><a href=#4-myth4-most-ml-engineers-dont-need-to-worry-about-scale class=header-anchor></a>4. Myth4: Most ML Engineers Don’t Need to Worry About Scale</h3><ul><li>scale<ul><li>eg) a system that serves hundreds of queries per second of millions of users a month</li></ul></li></ul><h2 id=2-batch-prediction-versus-online-prediction><a href=#2-batch-prediction-versus-online-prediction class=header-anchor></a>2. Batch Prediction versus Online Prediction</h2><ul><li>types of predictions<ol><li>Batch prediction, which use only batch features</li><li>Online prediction that uses only batch features (eg. precomputed embeddings)</li><li>Online prediction(Streaming prediction) that use both batch features and streaming features</li></ol></li></ul><h3 id=1-online-prediction><a href=#1-online-prediction class=header-anchor></a>1. Online Prediction</h3><ul><li>when predictions are generated and returned as soon as requests for these predictions arrive</li><li>on-demand prediction, synchronous prediction</li></ul><h3 id=2-batch-prediction><a href=#2-batch-prediction class=header-anchor></a>2. Batch Prediction</h3><ul><li>when predictions are generated periodically or whenever triggered.</li><li>predictions are store somewhere like in-memory or SQL Tables → retrieved as needed</li><li>asynchronous prediction</li></ul><h2 id=3-from-batch-prediction-to-online-prediction><a href=#3-from-batch-prediction-to-online-prediction class=header-anchor></a>3. From Batch Prediction to Online Prediction</h2><h3 id=1-online-prediction-1><a href=#1-online-prediction-1 class=header-anchor></a>1. Online Prediction</h3><ul><li>easy to start</li><li>problem with online prediction:<ul><li><em>model might take too long to generate predictions</em></li><li>to solve…<ul><li>compute predictions in advance → store in database → fetch then when request arrive</li><li>→ called batch prediction</li></ul></li></ul></li></ul><h3 id=2-batch-prediction-1><a href=#2-batch-prediction-1 class=header-anchor></a>2. Batch Prediction</h3><ul><li>predictions are precomputed → trick to reduce the inference latency</li><li>good to generate a lot of predictions and don’t need the results immediately</li><li>problem of batch prediction:<ol><li>Less responsive to users’ change preferences</li><li>Need to know what requests to generate predictions in advance</li></ol></li></ul><h3 id=3-online-prediction-becomes-default><a href=#3-online-prediction-becomes-default class=header-anchor></a>3. Online prediction becomes default</h3><ul><li>As hardware becomes more powerful → Online prediction becomes default</li><li>To overcome the latency challenge of online prediction:<ol><li>A (near) real-time pipeline that can work with incoming data:<ul><li>extract streaming features → input them into a model → return prediction in a near real time</li></ul></li><li>A model that can generate predictions at a speed acceptable to its end users</li></ol></li></ul><h2 id=4-unifying-batch-pipeline-and-streaming-pipeline><a href=#4-unifying-batch-pipeline-and-streaming-pipeline class=header-anchor></a>4. Unifying Batch Pipeline and Streaming Pipeline</h2><ul><li>using sliding features<ul><li>In training this feature is computed in batch</li><li>Whereas during inference this feature is computed in a streaming pipeline<ul><li>Apache Flink</li></ul></li></ul></li></ul><h2 id=5-model-compression><a href=#5-model-compression class=header-anchor></a>5. Model Compression</h2><ul><li>Deployed model takes too long to generate predictions:<ol><li>make it do inference faster<ul><li>→ inference optimization</li></ul></li><li>make the model smaller<ul><li>→ model compression<ul><li>originally, to make model fit on edge device</li></ul></li></ul></li><li>make the hardware it’s deployed on run faster</li></ol></li><li>model compression<ol><li>low-rank optimization</li><li>knowledge distillation</li><li>pruning</li><li>quantization</li></ol></li></ul><h3 id=1-low-rank-factorization><a href=#1-low-rank-factorization class=header-anchor></a>1. Low-Rank Factorization</h3><ul><li>key-idea<ul><li>replace high-dimensional tensors with low-dimensional tensors</li></ul></li><li>compact convolutional filters<ul><li>replace over-parameterized (having too many parameters) convolutional filters to compact convolutional filters</li><li>compact blocks to both reduce the number of parameters and increase speed<ul><li>eg) 3x3 conv → 1x1 conv</li></ul></li></ul></li></ul><h3 id=2-knowledge-distillation><a href=#2-knowledge-distillation class=header-anchor></a>2. Knowledge Distillation</h3><ul><li>smaller model (student) is train to mimic a larger model or ensemble model (teacher)</li><li>can work regardless of the architectural differences between teacher and student</li><li>disadvantages<ul><li>highly dependent on the availability of a teacher network</li></ul></li></ul><h3 id=3-pruning><a href=#3-pruning class=header-anchor></a>3. Pruning</h3><ul><li>in neural network, it means<ol><li>remove entire nodes of a neural network<ul><li>changing its architecture and reducing its number of parameters</li></ul></li><li>find parameters least useful to predictions and set them to zero(0).<ul><li>do not change architecture, only the number of nonzero parameters</li><li>sparse architecture<ul><li>make a neural network more sparse</li><li>require less storage than dense structure</li></ul></li></ul></li></ol></li></ul><h3 id=4-quantization><a href=#4-quantization class=header-anchor></a>4. Quantization</h3><ul><li>most general and commonly used model compression method</li><li>reduce model size by using fewer bits to represent its parameters</li><li>advantage<ul><li>reduce memory size</li><li>improves the computational speed<ol><li>allows to increase batch size</li><li>less precision speeds up computation</li></ol></li></ul></li><li>disadvantage<ul><li>rounding numbers → rounding errors</li><li>small rounding errors → large performance change</li></ul></li><li>lower-precision training increasingly popular</li><li>Fixed-point inference for edge device</li></ul><h2 id=ml-on-the-cloud-and-on-the-edge><a href=#ml-on-the-cloud-and-on-the-edge class=header-anchor></a>ML on the Cloud and on the Edge</h2><ul><li>where your model’s computation will happen?</li><li>⇒ due to cost of cloud, trend are moving to edge</li></ul></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/ml-system/>Ml-System</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/chapter-9.-continual-learning-and-test-in-production/><div class=article-details><h2 class=article-title>Chapter 9. Continual Learning and Test in Production</h2></div></a></article><article><a href=/blog/p/chapter-8.-data-distribution-shifts-and-monitoring/><div class=article-details><h2 class=article-title>Chapter 8. Data Distribution Shifts and Monitoring</h2></div></a></article><article><a href=/blog/p/chapter-3.-data-engineer-fundamentals/><div class=article-details><h2 class=article-title>Chapter 3. Data Engineer Fundamentals</h2></div></a></article><article><a href=/blog/p/apache-kafka-101/><div class=article-details><h2 class=article-title>Apache Kafka 101</h2></div></a></article><article><a href=/blog/p/kafka-setup/><div class=article-details><h2 class=article-title>Kafka Setup</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=aiden-jeon/aiden-jeon.github.io issue-term=pathname label=comment crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Aiden's Camp</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>