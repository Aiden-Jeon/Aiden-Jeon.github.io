<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Feature-Based Explanations $m$: model $x=(x_1, x_2, &mldr;, x_n)$: an instance with variable $n$ eg) $x_i$ 는 문장 $x$의 $i$번 째 토큰 Importance weights explanations는 각 feature $x_i$별로 중요도를 숫자로 표현\nSubsets explanations는 $x$가 예측에 영향을 미친 subset $x$를 제공\n1. Importance Weights 1.1) Feature-Additive Weights Feature-additivity란 모든 feature의 중요도의 합이 모델의 예측에서 모델의 편향을 뺀 값과 동일해야 하는 속성을 말합니다.\n분류 문제에서는 모델의 예측은 각 클래스별 확률이고, 보통 이 중 가장 높은 확률값으로 반환합니다. 모델의 편향이란 reference, baseline 입력과 같이 아무런 정보(no information)도 없는 예측입니다. 예를 들어서 이미지 처리에서는 검은색 이미지, 자연어 처리에서는 zero-vector embedding을 baseline으로 사용합니다.\n"><title>Feature-Based Explanations</title>
<link rel=canonical href=https://aiden-jeon.github.io/blog/p/feature-based-explanations/><link rel=stylesheet href=/blog/scss/style.min.663803bebe609202d5b39d848f2d7c2dc8b598a2d879efa079fa88893d29c49c.css><meta property='og:title' content="Feature-Based Explanations"><meta property='og:description' content="Feature-Based Explanations $m$: model $x=(x_1, x_2, &mldr;, x_n)$: an instance with variable $n$ eg) $x_i$ 는 문장 $x$의 $i$번 째 토큰 Importance weights explanations는 각 feature $x_i$별로 중요도를 숫자로 표현\nSubsets explanations는 $x$가 예측에 영향을 미친 subset $x$를 제공\n1. Importance Weights 1.1) Feature-Additive Weights Feature-additivity란 모든 feature의 중요도의 합이 모델의 예측에서 모델의 편향을 뺀 값과 동일해야 하는 속성을 말합니다.\n분류 문제에서는 모델의 예측은 각 클래스별 확률이고, 보통 이 중 가장 높은 확률값으로 반환합니다. 모델의 편향이란 reference, baseline 입력과 같이 아무런 정보(no information)도 없는 예측입니다. 예를 들어서 이미지 처리에서는 검은색 이미지, 자연어 처리에서는 zero-vector embedding을 baseline으로 사용합니다.\n"><meta property='og:url' content='https://aiden-jeon.github.io/blog/p/feature-based-explanations/'><meta property='og:site_name' content="Aiden's Camp"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='xai'><meta property='article:published_time' content='2021-07-18T00:00:00+00:00'><meta property='article:modified_time' content='2021-07-18T00:00:00+00:00'><meta name=twitter:title content="Feature-Based Explanations"><meta name=twitter:description content="Feature-Based Explanations $m$: model $x=(x_1, x_2, &mldr;, x_n)$: an instance with variable $n$ eg) $x_i$ 는 문장 $x$의 $i$번 째 토큰 Importance weights explanations는 각 feature $x_i$별로 중요도를 숫자로 표현\nSubsets explanations는 $x$가 예측에 영향을 미친 subset $x$를 제공\n1. Importance Weights 1.1) Feature-Additive Weights Feature-additivity란 모든 feature의 중요도의 합이 모델의 예측에서 모델의 편향을 뺀 값과 동일해야 하는 속성을 말합니다.\n분류 문제에서는 모델의 예측은 각 클래스별 확률이고, 보통 이 중 가장 높은 확률값으로 반환합니다. 모델의 편향이란 reference, baseline 입력과 같이 아무런 정보(no information)도 없는 예측입니다. 예를 들어서 이미지 처리에서는 검은색 이미지, 자연어 처리에서는 zero-vector embedding을 baseline으로 사용합니다.\n"><link rel="shortcut icon" href=/blog/icons/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-419F58RW9W"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-419F58RW9W")}</script></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/blog/><img src=/blog/imgs/avatar_hu6933059792947527008.png width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🔥</span></figure><div class=site-meta><h1 class=site-name><a href=/blog>Aiden's Camp</a></h1><h2 class=site-description>Welcome to Aiden's Camp</h2></div></header><ol class=menu-social><li><a href=https://github.com/Aiden-Jeon target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://www.linkedin.com/in/jongseob-jeon/ target=_blank title=Linkedin rel=me><svg fill="#000" height="800" width="800" id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 310 310"><g id="XMLID_801_"><path id="XMLID_802_" d="M72.16 99.73H9.927c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5H72.16c2.762.0 5-2.238 5-5V104.73c0-2.76100000000001-2.238-5-5-5z"/><path id="XMLID_803_" d="M41.066.341C18.422.341.0 18.743.0 41.362.0 63.991 18.422 82.4 41.066 82.4c22.626.0 41.033-18.41 41.033-41.038C82.1 18.743 63.692.341 41.066.341z"/><path id="XMLID_804_" d="M230.454 94.761c-24.995.0-43.472 10.745-54.679 22.954V104.73c0-2.761-2.238-5-5-5h-59.599c-2.762.0-5 2.239-5 5v199.928c0 2.762 2.238 5 5 5h62.097c2.762.0 5-2.238 5-5V205.74c0-33.333 9.054-46.319 32.29-46.319 25.306.0 27.317 20.818 27.317 48.034v97.204c0 2.762 2.238 5 5 5H305c2.762.0 5-2.238 5-5V194.995C310 145.43 300.549 94.761 230.454 94.761z"/></g></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/blog/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/blog/categories><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg>
<span>Categories</span></a></li><li><a href=/blog/tags><svg class="icon icon-tabler icon-tabler-tag" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11 3l9 9a1.5 1.5.0 010 2l-6 6a1.5 1.5.0 01-2 0L3 11V7a4 4 0 014-4h4"/><circle cx="9" cy="9" r="2"/></svg>
<span>Tags</span></a></li><li><a href=/blog/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li><a href=/blog/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/blog/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ul><li><a href=#1-importance-weights>1. Importance Weights</a><ul><li><a href=#11-feature-additive-weights>1.1) Feature-Additive Weights</a><ul><li><a href=#shapley-values>Shapley values</a></li></ul></li><li><a href=#12-non-feature-additive-weights>1.2) Non-Feature-Additive Weights</a></li></ul></li><li><a href=#2-minimal-sufficient-subsets>2. Minimal Sufficient Subsets</a></li></ul></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/blog/categories/machine-learning/>Machine-Learning</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/blog/p/feature-based-explanations/>Feature-Based Explanations</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Jul 18, 2021</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>3 minute read</time></div></footer></div></header><section class=article-content><h1 id=feature-based-explanations><a href=#feature-based-explanations class=header-anchor></a>Feature-Based Explanations</h1><ul><li>$m$: model</li><li>$x=(x_1, x_2, &mldr;, x_n)$: an instance with variable $n$<ul><li>eg) $x_i$ 는 문장 $x$의 $i$번 째 토큰</li></ul></li></ul><p><strong>Importance weights explanations</strong>는 각 feature $x_i$별로 중요도를 숫자로 표현<br><strong>Subsets explanations</strong>는 $x$가 예측에 영향을 미친 subset $x$를 제공</p><h2 id=1-importance-weights><a href=#1-importance-weights class=header-anchor></a>1. Importance Weights</h2><h3 id=11-feature-additive-weights><a href=#11-feature-additive-weights class=header-anchor></a>1.1) Feature-Additive Weights</h3><p><em>Feature-additivity</em>란 모든 feature의 중요도의 합이 모델의 예측에서 모델의 편향을 뺀 값과 동일해야 하는 속성을 말합니다.</p><p>분류 문제에서는 모델의 예측은 각 클래스별 확률이고, 보통 이 중 가장 높은 확률값으로 반환합니다. 모델의 편향이란 reference, baseline 입력과 같이 아무런 정보(no information)도 없는 예측입니다. 예를 들어서 이미지 처리에서는 검은색 이미지, 자연어 처리에서는 zero-vector embedding을 baseline으로 사용합니다.</p><p>더할 수 있는 성질과 유사하게 예측에 대한 설명에 사용된 모든 피쳐들은 occlusion, omission 방법이 가능합니다. Occlusion이란 특정 feature를 baseline feature로 대체하는 것을, Omission은 특정 feature를 완전히 제거하는 것을 의미합니다. 예를 들어 자연어 처리에서 Omission이란 문장의 길이를 변수로 사용하는 방법 입니다. Feature에 변형을 가하는 occlusion과 omission은 out-of-distribution 입력으로 신뢰할 수 없는 중요치를 제공하게 됩니다.</p><p>Feature-additivitiy 제약 조건 아래, feature ${\{ x_{i} \}}<em>{i}$ 는 $\{w</em>{i}(m,x)\}_i$ 의 가중치를 갖게 됩니다.</p><p>$$\sum_{i=1}^{\left| \mathbf{x} \right|}{w_{i}(m,\mathbf{x})=m(\mathbf{x})-m(\mathbf{b})}\tag{1}$$</p><p>수식에서 $m$은 모델, $\mathbf{x}$는 instance, $\mathbf{b}$는 베이스라인 input을 의미합니다.
회귀 모델에서 $m(\mathbf{x})$는 모델이 예측한 실제 값이며, 분류 모델에서는 클래로 예측한 확률입니다.</p><h4 id=shapley-values><a href=#shapley-values class=header-anchor></a>Shapley values</h4><p><a class=link href=https://arxiv.org/abs/1705.07874 target=_blank rel=noopener>Lundberg and Lee (2017)</a> 연구는 feature-additive 설명 방법을 하나로 통합하기 위해 진행된 연구입니다. 연구에서는 게임이론의 Shapley values만이 연구에서 정의한 세 가지의 특성을 만족하는 유일한 방법임을 증명했습니다.</p><ol><li>Local Accuracy (completeness)<br>이 특성을 만족하기 위해서는 위에서 제시한 (1)식을 만족해야 합니다. 모든 feature-additive 방법들이 제공하는 중요도가 이 식을 만족하지는 않습니다. 예를 들으서 LIME은 $\mathbf{x}$의 주변에서 선형 회귀 모델을 학습합기 때문에 같은 값을 재현할 수 가 없습니다.</li><li>Missingness<br>이 특성을 만족하기 위해서는 $\mathbf{x}$에 존재하지 않는 feature는 중요도가 0이어야 합니다. 예를 들어서 모델 $m$과 문장 $\mathbf{x}$이 있을 때, $\mathbf{x}$에 존재하는 토큰들만 0 또는 다른 값의 중요도를 가져야합니다. 문장 $\mathbf{x}$에 존재하지 않는 다른 단어들은 0의 중요도여야 합니다.</li><li>Consistency<br>이 특성을 만족하기 위해서는 두 개의 모델 $m$, $m&rsquo;$ 있을 때 어떤 feature $x_i$가 모델 $m&rsquo;$ 보다 $m$ 에서 더 큰 기여(marginal contribution)를 했다면 모델 $m$에서의 feature $x_i$의 기여도가 $m&rsquo;$의 $x_i$의 기여도 보다 더 크게 나와야 합니다.</li></ol><h3 id=12-non-feature-additive-weights><a href=#12-non-feature-additive-weights class=header-anchor></a>1.2) Non-Feature-Additive Weights</h3><p>대부분의 importance weights explainer들은 feature-additivity 특성을 기본으로 합니다. 다만 일부 연구에서는 이러한 특성을 신경쓰지 않습니다. 예를 들어 LS-Tree는 구분 분석 트리(parse tree)를 언어 데이터에 사용해 문장 내 토큰 간의 상호작용을 감지하고 이를 정량화하는데 가중치를 사용 할 수 있도록 합니다.</p><h2 id=2-minimal-sufficient-subsets><a href=#2-minimal-sufficient-subsets class=header-anchor></a>2. Minimal Sufficient Subsets</h2><p>다른 유명한 예측을 설명하는 방법으로는 minimal sufficient subset(mss) 가 있습니다. Minimal sufficient subset란 입력으로 받은 feature중 일부분만 사용했을 때 전체 feature를 사용한 값과 같은 예측을 할 수 있는 feature subset을 말합니다.</p><p>Minimal sufficient subset를 구하기 위해서는 우선 feature의 부분 집합 $\mathbf{x}_\mathbf{s}$ 를 모델 $m$으로 예측해야 합니다.</p><p>이 때 위에서 설명한 omission과 deletion을 이용해 $\mathbf{x}$를 $\mathbf{x}_\mathbf{s}$를 만들 수 있습니다.</p><p>그런데 만약 모델이 feature의 부분 집합으로는 예측할 수 없고 feature 전체를 필요로 하는 경우에는 이 방법을 적용할 수 없습니다. 전체 feature를 필요하는 경우 이 방법으로 이용한 설명에는 아무런 정보가 없습니다(uninformative).
그럼에도 일부 feature만 사용하는 모델에 대해서는 좋은 설명 방법이 됩니다. 예를 들어서 computer vision에서는 모든 픽셀이 문제를 푸는데 필요하지 않습니다. 비슷하게 감정분석에서는 일부 구문만 있어도 충분히 감정을 분류할 수 있습니다.</p><p>또한 여러 개의 minimal sufficient subset이 존재할 수 있는데, 이 때 각 minimal sufficient subset들은 인스턴스의 예측을 설명하는 하나의 독립적인 잠재적 설명(potential explanation)이 됩니다. 이런 경우에는 모델의 전체적인 면(complete view) 를 보여주기 위해서는 가능한 모든 minimal sufficient subset를 보여주어야 합니다.</p><p>Minimal Sufficient Subsets를 이용하는 설명 방법으로는 L2X, SIS, Anchors, INVASE 등이 있습니다.
L2X는 전체 feature를 이용한 예측과 subset feature를 이용한 예측의 상호 정보(mutual information)을 극대화 시키는 subset를 학습합니다.
다만, L2X를 계산하기 위해서는 최소의 원소 개수(cardinality of a minimal sufficient subset)를 모수로서 정해야 합니다.
이러한 요소는 L2X를 실제 문제에 적용을 어렵게 합니다.
이를 극복한 알고리즘이 INVASE인데, 이 알고리즘은 각 인스턴스 별로 최소의 원소 개수를 다르게 지정할 수 있습니다.
L2X와 INVASE 알고리즘은 한 개의 subset만을 제공합니다. 반대로 SIS는 서로 겹치지(overlap) 않는 minimal sufficient subset를 제공합니다.</p></section><footer class=article-footer><section class=article-tags><a href=/blog/tags/xai/>Xai</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/blog/p/properties-of-explanations/><div class=article-details><h2 class=article-title>Properties of Explanations</h2></div></a></article><article><a href=/blog/p/types-of-explanatory-methods/><div class=article-details><h2 class=article-title>Types of Explanatory Methods</h2></div></a></article><article><a href=/blog/p/introduction-to-xai/><div class=article-details><h2 class=article-title>Introduction to XAI</h2></div></a></article><article><a href=/blog/p/lime-with-code/><div class=article-details><h2 class=article-title>LIME with code</h2></div></a></article><article><a href=/blog/p/pandas%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-tf-idf-%EA%B5%AC%ED%95%98%EA%B8%B0/><div class=article-details><h2 class=article-title>Pandas를 이용한 tf-idf 구하기</h2></div></a></article></div></div></aside><script src=https://utteranc.es/client.js repo=aiden-jeon/aiden-jeon.github.io issue-term=pathname label=comment crossorigin=anonymous async></script><style>.utterances{max-width:unset}</style><script>let utterancesLoaded=!1;function setUtterancesTheme(e){let t=document.querySelector(".utterances iframe");t&&t.contentWindow.postMessage({type:"set-theme",theme:`github-${e}`},"https://utteranc.es")}addEventListener("message",e=>{if(e.origin!=="https://utteranc.es")return;utterancesLoaded=!0,setUtterancesTheme(document.documentElement.dataset.scheme)}),window.addEventListener("onColorSchemeChange",e=>{if(!utterancesLoaded)return;setUtterancesTheme(e.detail)})</script><footer class=site-footer><section class=copyright>&copy;
2020 -
2024 Aiden's Camp</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.27.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/blog/ts/main.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>