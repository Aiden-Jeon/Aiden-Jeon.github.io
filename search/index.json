[{"content":"Apache Kafka 101 Apache Kafka 101\n1. Events Kafka many use cases ⇒ start from event streaming platform Event What is Event? things that have happened combined with the description of what happend ⇒ notification + state eg) IOT Business Process Change User Interaction Microservice Output Notification the element of when-ness that can be used to trigger some other activity State Usually small (less than a megabyte) normally represented in some structured format (JSON, an object serialized with Apache Avro or Protocol Buffers) Key / Value kafka models events as key/value pair Internally: sequence of bytes externally: structured objects represented in language’s type system (JSON, JSON Schema, Avro, or Protobuf) 3. Topics Topic most fundamental unit of Kafka something like a table in a relational database Named container for similar events create different topics to hold different kinds of events filtered and transformed versions of the same kind of event → can duplicate data between topics A topic is a log of events Logs Append only: when writing a new message into a log, it always goes on the end Can only seek by offset, not indexed Immutable: once something has happened, it is difficult to make it un-happen Topics are durable Logs are durable Retention is configurable expire data after it has reached a certain age topic overall reached a certain size Logs on Kafka topics are files stored on disk write on event to a topic, it is durable as it would be of you had written it to any database you ever trusted. 4. Partitioning Kafka: Distributed System no one topic could ever get too big aspire to accommodate too many reads and writes What is Partitioning? takes the single topic log → breaks it into multiple logs → each of which can live on a separate node on cluster need a way of deciding which messages to write to which partition message with no key message with a key Message with no key round-robin among all the topic’s partitions pros) all partitions get an even share of the data cons) don’t preserve any kind of ordering of the input message Message with a key destination partition will be computed from a has of the key output of hash function mode of # of partitions guarantee that messages having same key always land in same partition pros) always in order cons) what if very active key? → a larger and more active partition ⇒ risk is small in practice and manageable 6. Brokers Kafka Brokers Actual Computers Kafka is composed of a network of machines called brokers machines can be an computer, instance, or container running the Kafka process Each broker hosts some set of Kafka partitions handles incoming requests to write new events to those partitions to read events from them handles replication of partitions 7. Replication Copies of data for fault tolerance if we store each partition to one broker → susceptible to failure → we need copy partitions data to several brokers One lead partition and N-1 followers Leader: writes and reads happens Follower: works together to replicate those new writes Automatic process → developer don’t need to worry about it 8. Producers Kafka Producer application using Kafka : Producer and Consumer Producing and Consuming: how to interface with cluster API surface of the producer library is fairly lightweight 9. Consumers Kafka Consumer many consumers read one topic reading does not destroy message Rebalancing processes using same group ID → fairly split data to consumer traditional topic keep ordering guarantee in place sacrifice the ability to scale out consumers 11. Ecosystem Infrastructure doesn’t contribute value directly to customers best case ← provided by community or an infrastructure vendor eg) Kafka Connect Confluent Schema Registry Kafka Streams ksqlDB 12. Kafka Connect Job of Kafka connect the data those other systems to get into Kafka topics data in Kafka topics to get into those system What does Kafka Connect Do? Data integration system and ecosystem a client application External client process; does not run on Brokers if something is not a broker is an producer or and consumer Horizontally scalable Fault-tolerant How Kafka Connect Works Connect worker runs one or more connectors Connectors pluggable software component interfacing with the external system Also exist as runtime entities Source connector (acts as Producer) reads data from and external system and produces it to a Kafka topi Sink connector (acts as Consumer) subscribes to one or more Kafka topics and writes the messages it reads to an external system Benefits of Kafka Connect large ecosystem of connectors 14. Confluent Schema Registry To solve two problems New consumers of existing topics will emerge → need to understand the format of the message in the topic The format of those message will evolve as the business evolves → the schemas of domain objects is a constantly moving target must have a way of agreeing on the schema of messages in any given topic What is Schema Registry Sever Process external to Kafka brokers Job: maintain a database of schemas schema: that have been written into topics in the cluster for which it is responsible. database: internal Kafka topic and cached in Schema Registry for lower latency access Consumer/Producer API Component Process calls on API at the Schema Registry REST endpoint presents the schema of the new message Response Produce side if same as last message → produce succeed if different from last message but matches the compatibility rules defined for the topic → produce succeed violates compatibility rules → produce fail in a way that the application code can detect Consume side Consumer API prevents incompatible message from being consumed Support Formats JSON Schema Avro Protocol Buffers 16. Kafka Stream Consumer tend to grow in complexity started from stateless transformation (masking, changing format) →being complexity → stateful state memory allocated in program’s heap → fault-tolerant liability Kafka Stream Functional Java API Filtering, grouping, aggregating, joining, and more Scalable, fault-tolerant state management Integrates within your services as a library Runs in the context of your application Does not require special infrastructure ","date":"2022-12-06T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/apache-kafka-101/","title":"Apache Kafka 101"},{"content":"Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 9. Continual Learning and Test in Production continual learning to adapt our models to data distribution shifts infrastructural problem test in production model is retrained to adapt to the changing environment evaluating it on a stationary set also test in production monitoring \u0026amp; test in production monitoring: passively keeping track of the outputs test in production: proactively choosing which model to produce outputs Goal: to understand a model’s performance and figure out when to update it Goal of continual learning to safely and efficiently automate the update Continual Learning Misunderstand in term of “Continual learning” A models updates itself with every incoming sample in production problems catastrophic forgetting tendency of a neural network to completely and abruptly forget previous learned information upon learning new information make training more expensive Update their model in micro-batch (512, 1024) updated model shouldn’t be deployed until it’s been evaluated existing model → champion model replica model → challenger model Don’t need to update model frequently don’t have enough traffic model don’t decay that fast 1. Stateless Retraining Versus Stateful Training continual learning isn’t about retraining frequency → manner in which model is retrained 1. Stateless retraining the model is trained from scratch\nfor example\n1 2 3 \u0026lt;--\u0026gt; (model v1) \u0026lt;--\u0026gt; (model v2) \u0026lt;--\u0026gt; (model v3) require a lot more data\n2. Stateful training the model continues training on new data\nfine-tuning or incremental learning\nfor example\n1 2 3 \u0026lt;--\u0026gt; (model v1) \u0026lt;-\u0026gt; (model v1.1) \u0026lt;-\u0026gt; (model v1.2) allows to update model with less data\n3. Types of model updates Model iteration A new feature is added to an existing model architecture model architecture is changed to be stateful training knowledge transfer model surgery Data iteration model architecture and features remain the same but refresh this model with new data means stateful training 2. Why Continual Learning Continual Learning setting up infrastructure so that you can update your model and deploy these changes as fast as you want Use case to combat data distribution shifts, especially when the shifts happens suddenly to adapt to rare events Continuous cold start problem arises when your model has to make predictions for a new user without any historical data 3. Continual Learning Challenges 1. Fresh data access challenge 2. Evaluation challenged The biggest challenge of continual learning making sure that this update is good enough to be deployed The risk of catastrophic failures amplify with continual learning The more frequently you update your models → the more opportunities there are for updates to fail make your models more susceptible to coordinated manipulation and adversarial attack evaluation pipeline evaluation takes time → can be another bottleneck for model update frequency 3. Algorithm challenged 4. Four Stages of continual learning Stage 1: Manual, stateless retraining Stage 2: Automated retraining Stage 3: Automated, stateful retraining Stage 4: Continual learning 5. How Often to Update Your Model 1. Value of data freshness Q) How often to update a model? → How much the model performance will improve with updating? To figure out the gain training your model on the data from different time windows in the past and see how the performance changes\nfor example\n1 2 3 4 \u0026lt;--\u0026gt; model A \u0026lt;--\u0026gt; model B \u0026lt;--\u0026gt; model C Test data 2. Model iteration versus data iteration model iteration data iterating doesn’t give you much performance gain → spend resources on finding a better model data iteration finding a better model architecture requires 100X compute for training and gives 1% performance gain whereas data iteration requires 1X compute and also gives 1% performance gain Test in Production To sufficiently evaluate models use mixture of offline evaluation and online evaluation Offline evaluation Good old test split to evaluate models → not sufficient to evaluate new model →backtest backtest method of testing a predictive model on data from a specific period of time in the past not quite sufficient 1. Shadow Deployment the safest way to deploy model steps Deploy the candidate model in parallel with the existing model For each incoming request, route it both models to make predictions, but only serve the existing model’s prediction to user Log the predictions from the new model for analysis purpose Replace model when new model’s predictions are satisfactory But expensive: doubling cost 2. A/B Testing a way to compare two variants of an object testing responses to these two variants → determining which of two variants is more effective steps Deploy the candidate model alongside the existing model A percentage of traffic is routed to the new model, the rest is routed to the existing model Monitor and analyze the predictions and user feedback, if any, from both models to determine whether the difference in the two models’ performance is statistically significant A/B testing requires A/B testing consists of a randomized experiment A/B test should be run on a sufficient number of samples to gain enough confidence about the outcome 3. Canary Release technique to reduce the risk of introducing a new software version in production by slow rolling out the change to a small subset of users before rolling it out to everybody steps Deploy the candidate model alongside the existing model. candidate model is called canary A portion of the traffic is routed to the candidate model If its performance is satisfactory increase the traffic to the candidate model. If not, abort the canary and route all traffic to the existing model Stop when either the canary serves all the traffic or the canary is aborted 4. Interleaving Experiments Reliably identifies the best algorithms with considerably smaller sample size compare to traditional A/B testomg A/B testing**********************************:********************************** core metrics are compared Interleaving: compared by measuring user preferences 5. Bandits A/B testing randomly route traffic stateless Bandits allow to determine how to route traffic stateful a lot more data-efficient that A/B testing require less data reduce opportunity cost as they route traffic to the better model more quickly A/B testing 630,000 to get a 95% confidence interval 12,000 samples to determine a lot more difficult to implement bandits requires computing and keeping track of models’ payoffs ","date":"2022-11-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-9.-continual-learning-and-test-in-production/","title":"Chapter 9. Continual Learning and Test in Production"},{"content":"Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 8. Data Distribution Shifts and Monitoring Deploying a model isn’t the end of process Model’s performance degrades over time in production Once models has deployed, still have to continually monitor its performance to detect issue → deploy updates to fix issues 1. Causes of ML System Failures Failure one or more expectations of the system is violated Traditional software system’s operational expectations system executes its logic with in expected operational metrics latency, throughput ML System operational expectations and ML performance metrics operational expectation violates → easier to detect ML performance metric violates → harder to detect 1. Software System Failures Dependency failure Deployment failure Hardware failure Downtime or Crashing 2. ML-Specific Failures 1. Production data differing from training data model generalizes to unseen data generate accurate predictions for unseen data Assumption: unseen data comes from a stationary distribution that is the same as the training data distribution → incorrect in most case underlying distribution of the real-world data is unlikely to be the same as the training data distribution the real-word isn’t stationary 2. Edge cases edge cases: the data samples so extreme that cause the model to make catastrophic mistakes outlier vs edge case outlier refers to data an example that differs significantly differs from other examples edge case refers to performance an example where a model performs significantly worse than other examples 3. Degenerate Feedback Loops feedback loop the time it takes from when a prediction is show until the time feedback on the prediction is provided. degenerate feedback loop predictions themselves influence feedback → influences the next iteration of the model created when systems’s outputs are used → to generate the system’s future inputs ⇒ influence the system’s output eg) recommendation system 4. Detecting Degenerate Feedback Loop 5. Correcting Degenerate Feedback Loop 2. Data Distribution Shifts Data distribution shifts phenomenon in supervised learning when the data a model works with changes over time → causes this model’s prediction to become less accurate as time passes Source distribution Target distribution 1. Types of Data Distribution Shifts Covariate Shift: when $P(X)$ changes but $P(Y|X)$ remains the same Label Shift: when $P(Y)$ changes but $P(X|Y)$ remains the same Concept Drift: when $P(Y|X)$ changes but $P(X)$ remains the same 1. Covariate shift one of most widely studied forms model development during data selection process difficult to collect data training data is artificially altered (under-sampling, over-sampling) model’s learning process active learning In production major change in the environment the way application is used 2. Label shift a.k.a prior shift, target shift closely related to covariate shift, methods for detecting and adapting models are similar 3. Concept Drift a.k.a posterior shift same input, different output usually cyclic or seasonal 2. General Data Distribution Shifts feature change new features are added old features are removed set of all possible values of a feature changed Label schema change set of possible value for Y change 3. Detecting Data Distribution Shifts monitoring model’s accuracy-related metrics Input Output Joint dist 1. Statistical method compare statistics two-sample hypothesis test (two-sample test) Kolmogrov-Smirnov test (KS test) non-parametric test can used for one-dimensional data Least-Square Density Difference Maximum Mean Discrepancy (MMD) Learned Kernel MMD 2. Time scale windows for detecting shifts shifts across two dimensions: spatial: happens across points temporal: happens across time → to detect: treat input data as time-series data 4. Addressing Data Distribution Shifts Assume data shifts are inevitable → periodically retrain their model To make a model work with a new distribution in production: Train models using massive datasets Adopt a trained model to a target distribution without new labels Domain Adoption under Target and Conditional Shift On Learning Invariant Representations for Domain Adoption Retrain model using the labeled data from the target distribution whether to train model from scratch (stateless training) continuing training the existing model (stateful training) what data to use 3. Monitoring and Observability monitoring refers to act of tracking, measuring, and logging different metrics that can help us determine when something goes wrong operational metrics: health of systems network machine applications observability setting up our system (instrumentation) in a way that give us visibility into our system to help us investigate what meant wrong part of monitoring 1. ML-Specific Metrics Types model accuracy-related metrics predictions features raw inputs from 1 to 4 easier to monitor ←→ harder to monitor closer to business metrics ←→ less likely to be caused by human errors 1. Monitoring accuracy-related metrics direct metrics to help decide whether a model’s performance has degraded 2. Monitoring predictions most common artifact to monitor easy to visualize monitor predictions for distribution shifts 3. Monitoring features feature validation ensuring that features follow an expected schema 4. Monitoring raw inputs 2. Monitoring Toolbox logs dashboards alerts 3. Observability better visibility into understanding the complex behavior of software using [outputs] collected from the system at run time telemetry system’s outputs collected at runtime remote measures logs and metrics collected from remote component such as cloud services applications on customer device ","date":"2022-11-10T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-8.-data-distribution-shifts-and-monitoring/","title":"Chapter 8. Data Distribution Shifts and Monitoring"},{"content":"Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 7. Model Deployment and Prediction Service ML App Logic data engineering → feature engineering → model → metrics Deploy \u0026amp; Inference Deploy: a loose term that generally means making your model running and accessible Inference: the process of generating predictions To be deployed: model will have to leave the development environment model can be deployed to a staging environment for testing a production environment to be used by end users 1. Machine Learning Deployment Myths 1. Myth1: You Only Deploy One or Two ML Models at a Time 2. Myth2: If we Don’t Do Anything, Model Performance Remains the same. “software rot” or “bit rot” software program degrades over time even if nothing has been changed. ML Systems suffer from data distribution shifts 3. Myth3: You Won’t Need to Update Your Model as Much “How often SHOULD I update my models?” → “How often CAN I update my models?” Model’s performance decays over time → want to update model as fast as possible 4. Myth4: Most ML Engineers Don’t Need to Worry About Scale scale eg) a system that serves hundreds of queries per second of millions of users a month 2. Batch Prediction versus Online Prediction types of predictions Batch prediction, which use only batch features Online prediction that uses only batch features (eg. precomputed embeddings) Online prediction(Streaming prediction) that use both batch features and streaming features 1. Online Prediction when predictions are generated and returned as soon as requests for these predictions arrive on-demand prediction, synchronous prediction 2. Batch Prediction when predictions are generated periodically or whenever triggered. predictions are store somewhere like in-memory or SQL Tables → retrieved as needed asynchronous prediction 3. From Batch Prediction to Online Prediction 1. Online Prediction easy to start problem with online prediction: model might take too long to generate predictions to solve… compute predictions in advance → store in database → fetch then when request arrive → called batch prediction 2. Batch Prediction predictions are precomputed → trick to reduce the inference latency good to generate a lot of predictions and don’t need the results immediately problem of batch prediction: Less responsive to users’ change preferences Need to know what requests to generate predictions in advance 3. Online prediction becomes default As hardware becomes more powerful → Online prediction becomes default To overcome the latency challenge of online prediction: A (near) real-time pipeline that can work with incoming data: extract streaming features → input them into a model → return prediction in a near real time A model that can generate predictions at a speed acceptable to its end users 4. Unifying Batch Pipeline and Streaming Pipeline using sliding features In training this feature is computed in batch Whereas during inference this feature is computed in a streaming pipeline Apache Flink 5. Model Compression Deployed model takes too long to generate predictions: make it do inference faster → inference optimization make the model smaller → model compression originally, to make model fit on edge device make the hardware it’s deployed on run faster model compression low-rank optimization knowledge distillation pruning quantization 1. Low-Rank Factorization key-idea replace high-dimensional tensors with low-dimensional tensors compact convolutional filters replace over-parameterized (having too many parameters) convolutional filters to compact convolutional filters compact blocks to both reduce the number of parameters and increase speed eg) 3x3 conv → 1x1 conv 2. Knowledge Distillation smaller model (student) is train to mimic a larger model or ensemble model (teacher) can work regardless of the architectural differences between teacher and student disadvantages highly dependent on the availability of a teacher network 3. Pruning in neural network, it means remove entire nodes of a neural network changing its architecture and reducing its number of parameters find parameters least useful to predictions and set them to zero(0). do not change architecture, only the number of nonzero parameters sparse architecture make a neural network more sparse require less storage than dense structure 4. Quantization most general and commonly used model compression method reduce model size by using fewer bits to represent its parameters advantage reduce memory size improves the computational speed allows to increase batch size less precision speeds up computation disadvantage rounding numbers → rounding errors small rounding errors → large performance change lower-precision training increasingly popular Fixed-point inference for edge device ML on the Cloud and on the Edge where your model’s computation will happen? ⇒ due to cost of cloud, trend are moving to edge ","date":"2022-11-09T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-7.-model-deployment-and-prediction-service/","title":"Chapter 7. Model Deployment and Prediction Service"},{"content":"Summary of Designing Machine Learning Systems written by Chip Huyen.\nChapter 3. Data Engineer Fundamentals To retrive stored data\n→ important to know\nnot only how it’s formatted but also how it’s structured Database: data sotrage engine\ntransactional Analytical Data in Production\nPipeline Data → multiple processes and service Raw data → featur engineering service → predictions Types Historical data in data storage engines Streaming data in real-time transports → requires different processing paradigms 1. Data Sources 1. User input data data explicitly input by users\nText, image, videos, upload files, etc… Users possibly can input wrong data → easily malformatted\nEg) text is too long or too short Users have little patience\n→ expect to get result bach immediately\n→ user input data tends to require fast processing\n2. System-generated data data generated by different components of systems Types Various types of logs Eg) memory usage, … system outputs Eg) model predictions, … Property less likely to be malformatted volume of logs can grow very quickly → cause two problems Hard to know where to lokk because signals are lost in noise How to store? → discard old data and get only recent data 3. Internal databases generated by various services and enterprise applications in a company\nManages inventory, customer relationship, users\n→ can be used by ML Models\n4. Third-party data first-party data Company already collects about users and customers second-party data collected by another compnay on their own customers that they make available to you Probably have to pay for it Third-party data companies collect data on the public who aren’t direct customers Eg) unique advertiser ID 2. Data Formats ‘How to store data?’ Chech List\nhow do I store multimodal data? how do I store my data so that it’s cheap and still fast to access? how do I store complex models so that the can be loaded and run correctly on different hardwares? Data serialiazations\nConverting a data structure Object state Into a format that can be Stored Transmitted And reconstructed later Format Binary / Text Human-Readable Use case JSON Text O Everywhere CSV Text O Everywhere Parquet Binary X Hadoop Auro Binary Primary X Hadoop Protobuf Binary Primary X Google, TF Record Pickle Binary X Python, Python serializations 1. JSON (JavaScript Object Notation) Human-readable Key-value Simple Capable of handling data different levles of structedness 2. Row-Major versus Column-Major Row-Major CSV Use case) Accessing all the examples collected today Data is stored and retrived row by row Good for accessing samples Better when you have to do a lot of writes Column-Major Parquet Use case) accessing the timestamp of all examples Data is stored and retrived column by column Good for accessing features Better when you have to do a lot of column-based readsd Numpy(Row) vs Pandas(Column) 3. Text versus binary format human-readable vs compact Eg) “1000000” Text: 7byte Binary: int32 → 32bits or 4bytes 3. Data Models How data is represented.\n1.Relation Model data is organized into relations : each relation is a set of tuples column: unordered Column1 Column2 … Tuple (row): unordered 2. NoSQL (Not Only SQL) document model graph model 3. Structured versus Unstructed data structured data follows a predefined data model → data schema make easier to analyze data warehouse to store data that has been processed into formats ready to be used unstructured data doesn’t adhere to a schema eg) 1 2 3 lisa,43 hack,23 xxx,xxx,xxx \u0026lt;- possible 4. Data Storage Engine and Processing two types of workloads that database are optimized for\ntransactional processing analytical processing 1. Transactional and Analytical Processing Online Transaction Processing (OLTP) inserted, updated, deleted often involve users so need 2 requirements: low latency: processed fast high availability: processing system needs to be available any time Transactional Database Transactional Database\ndesigned: to process online transactions requirements: low latency and high availability ACID\nAtomcity To guarantee that all steps in a transaction are completed successfully as a group. eg) User payment fails → do not assign driver Consistency To guarantee that all transactions coming through must follow predefined rules eg) a transaction must be made by a valid user Isolation To guarantee that two transaction happen at the same time as if they were isolated. eg) Do not book same driver to two users Durability To guarantee that once a transaction is committed, it will remain committed even in the case of a system failure. eg) often ordered a ride and phone dies, still want ride to come BASE (Basically Available, Soft state, and Eventually consistency)\ndon’t necessarily need to be ACID often row-major\nnot be efficient for question like: “average of something in one month” → Online Analytical Processing (OLAP) Both OLTP \u0026amp; OLAP → Outdated\nSeparation of OLTP \u0026amp; OLAP was due to limitation of technology Storage and processing are tightly coupled store same data in different databases decouple storage and processing (Big Query, Snowflake, IBM, Teradata) 2. ETL: Extract, Transform, and Load Flow Different sources ⇒ Extract → transformed → loaded ⇒ target destination Difficult to keep data structured ⇒ ELT ELT → Inefficient to search data Hybrid solution (Databricks, Snowflake) 5. Models of Dataflow dataflow data is passed from one process to another Three main models of dataflow Data passing through databases Data passing through services using requests by REST and RPC APIs Data passing through a real-time transport like Apache Kafka and Amazon Kinesis 1. Data Processing Through Database The easiest way to pass data between two process To pass data from process A to process B eg) process A write data into database and process B just read it Doesn’t work Two process are running in different companies Requires both process to access data from databases read/write from database can be slow make it unsuitable for low latency 2. Data Passing Through Services send data directly through a network Request-driven process communicate through requests → microservice architecture styles of request REST (Representational State Transfer) designed for requests over network predominant style for public APIs RPC (Remote Procedure Call) tries to make a request to a remote network service look the same as calling a function or method in programming language requests between services owned by the same organization 3. Data Passing Through Real-Time Transport request-driven : synchronous target service has to listen to the request for the request to go through Broker coordinates data passing among services each service only has to communicate with the broker use in-memory storage Real-time transport: in-memory storage for data passing among services Event A piece of data broadcast to a real-time transport (a.k.a event-bus) Architecture Request-driven architecture rely more on logic than on data Event-driven architecture works better for system that are data-heavy Common types of real-time transports pubsub (publish-subscribe) overview any service can publish to different topics in a real-time transport any service that subscribes to a topic can read all events in that topic retention policy data will be retained in the real-time transport for a certain period time of time before deleted move to a permanent storage (Amazon S3) service produce don’t care about what services consume their data Apache Kafka, Amazon Kinesis Message Queue responsible for getting the message to right consumer Apache RocketMQ, RabbitMQ ","date":"2022-11-07T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-3.-data-engineer-fundamentals/","title":"Chapter 3. Data Engineer Fundamentals"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 모델 경로를 받아 API를 띄우는 Dockerfile을 작성합니다. 요구사항 작성한 API에 필요한 모델을 /mnt/models 경로에서 가져오도록 코드를 수정합니다. 빌드된 이미지를 실행합니다. 모델이 있는 경로를 /mnt/models 로 volume mapping 합니다. example을 통해 정상적으로 예측하는지 확인합니다. DB에 데이터가 쌓이고 있는지 확인합니다. Dockerfile 앞서 작성한 dockerfile 에서 모델을 copy하는 부분을 삭제합니다.\n그리고 전역 변수로 모델의 경로를 입력합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FROM python:3.9-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN pip install -U pip \u0026amp;\u0026amp;\\ pip install \u0026#34;fastapi[all]\u0026#34; sqlalchemy psycopg2-binary scikit-learn mlflow ENV MODEL_PATH=\u0026#34;/mnt/models\u0026#34; COPY app/ . ENTRYPOINT [\u0026#34;uvicorn\u0026#34;, \u0026#34;crud:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--reload\u0026#34;] Build 다음과 같이 폴더를 갖고 있는 경로에서 이미지를 빌드합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . ├── Dockerfile ├── app │ ├── __init__.py │ ├── crud.py │ ├── database.py │ ├── models.py │ └── schemas.py └── model ├── MLmodel ├── conda.yaml ├── model.pkl ├── python_env.yaml └── requirements.txt 1 docker build . -t docker_without_model Run 도커에 volume mapping을 한 후 실행합니다.\n1 docker run -p $(pwd)/model:/mnt/models docker_without_model 실행 후 swagger 페이지를 확인합니다.\n","date":"2022-09-26T19:50:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-4.-model-api-4-docker-without-model/","title":"[ Chapter 4. Model API ] 4) Docker without model"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 컨테이너에 모델 정보를 복사해 API를 띄우는 Dockerfile을 작성합니다. 요구사항 작성한 API에 필요한 모델을 이미지에 복사하는 Dockerfile을 작성합니다. 이 이미지를 실행하면 별도의 모델 정보를 요구하지 않습니다. 빌드된 이미지를 실행합니다. example을 통해 정상적으로 예측하는지 확인합니다. DB에 데이터가 쌓이고 있는지 확인합니다. Crud 앞서 작성한 crud 파일에서 모델을 불러오는 부분을 다음과 같이 수정합니다.\n모델의 경로가 전역 변수로 선언되어 있지 않는 경우 /mnt/model 에 있는 모델을 load 하도록 합니다.\n1 2 MODEL_PATH = os.getenv(\u0026#34;MODEL_PATH\u0026#34;, \u0026#34;/mnt/model\u0026#34;) MODEL = mlflow.pyfunc.load_model(MODEL_PATH) Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 FROM python:3.9-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN pip install -U pip \u0026amp;\u0026amp;\\ pip install \u0026#34;fastapi[all]\u0026#34; sqlalchemy psycopg2-binary scikit-learn mlflow COPY app/ . COPY model /mnt/model ENTRYPOINT [\u0026#34;uvicorn\u0026#34;, \u0026#34;crud:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--reload\u0026#34;] 모델을 불러오는데 필요한 패키지들을 설치합니다.\nAPI를 실행하는데 필요한 코드를 복사합니다.\n그리고 모델 폴더를 /mnt/model에 복사합니다.\nBuild 다음과 같이 폴더를 갖고 있는 경로에서 이미지를 빌드합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 . ├── Dockerfile ├── app │ ├── __init__.py │ ├── crud.py │ ├── database.py │ ├── models.py │ └── schemas.py └── model ├── MLmodel ├── conda.yaml ├── model.pkl ├── python_env.yaml └── requirements.txt 1 docker build . -t docker_with_model Run 실행 후 swagger 페이지를 확인합니다.\n1 docker run docker_with_model ","date":"2022-09-26T19:30:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-4.-model-api-3-docker-with-model/","title":"[ Chapter 4. Model API ] 3) Docker with model"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 predict 로 요청받은 데이터와 그 결과를 DB에 저장합니다. 요구사항 database.py 를 작성해 db에 연결합니다. models.py 를 작성합니다. raw_data table columns: 입력으로 받은 column들 timestamp: 요청받은 시간 prediction table columns: iris_class 모델의 결과와 timestamp: 요청받은 시간 작성한 내용으로 crud.py 를 수정합니다. request를 날린후 raw_data table 과 prediction table을 확인합니다. query문을 작성해 input 과 prediction 을 같이 확인합니다. 질문사항 모델을 디버깅하기 위해서 입력값과 결과값을 머지하기 위해서는 어떻게 해야할까요? timestamp or uuid ? 왜 input_schema 와 output_schema가 필요할까요? Database 앞서 작성했던 자료를 참고해 다음과 같이 database.py를 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 import os from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker SQLALCHEMY_DATABASE_URL = os.getenv(\u0026#34;SQLALCHEMY_DATABASE_URL\u0026#34;, \u0026#34;postgresql://postgres:mypassword@localhost:5432/postgres\u0026#34;) engine = create_engine(SQLALCHEMY_DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() Models Database에서 사용할 models.py 를 정의합니다.\nInput Model 입력으로 받은 데이터에서 필요한 추가 column인 timestamp를 추가해 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sqlalchemy import Column, Integer, DateTime, Float from database import Base class DataIn(Base): __tablename__ = \u0026#34;raw_data\u0026#34; id = Column(Integer, primary_key=True, index=True) timestamp = Column(DateTime) sepal_width = Column(Float) sepal_length = Column(Float) petal_width = Column(Float) petal_length = Column(Float) Output Model 모델의 예측결과를 iris_class column에 저장합니다.\n1 2 3 4 5 6 class DataOut(Base): __tablename__ = \u0026#34;prediction\u0026#34; id = Column(Integer, primary_key=True, index=True) timestamp = Column(DateTime) iris_class = Column(Integer) Crud database 와 models 내용을 app 에 추가해 작성합니다.\n필요한 패키지를 불러옵니다.\n1 2 3 4 5 6 7 8 9 10 11 import os from datetime import datetime import mlflow import pandas as pd from fastapi import Depends, FastAPI from sqlalchemy.orm import Session from database import SessionLocal, engine from models import Base, DataIn, DataOut from schemas import PredictIn, PredictOut 다음으로 db 의 table을 자동으로 생성합니ㅏㄷ.\n1 Base.metadata.create_all(bind=engine) app을 선언하고 db를 불러올 수 있는 코드를 작성합니다.\n1 2 3 4 5 6 7 8 9 app = FastAPI() def get_db(): db = SessionLocal() try: yield db finally: db.close() 사용할 모델을 전역 변수로 선언합니다.\n1 MODEL = mlflow.pyfunc.load_model(\u0026#34;../model\u0026#34;) predict 함수를 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @app.post(\u0026#34;/predict\u0026#34;, response_model=PredictOut) def predict(data: PredictIn, db: Session = Depends(get_db)): now = datetime.utcnow() df = pd.DataFrame([data.dict()]) pred = int(MODEL.predict(df)) data_in = DataIn(timestamp=now, **data.dict()) db.add(data_in) db.commit() db.refresh(data_in) data_out = DataOut(timestamp=now, iris_class=pred) db.add(data_out) db.commit() db.refresh(data_out) return PredictOut(iris_class=pred) request 를 요청받은 시간을 now 에 저장합니다.\n그 이후 모델의 예측을 수행합니다.\n예측을 수행한 후 입력으로 들어온 데이터를 commit 합니다.\n다음으로 모델의 결과를 commit 합니다.\n실행 uvicorn 을 이용해 app을 실행합니다.\n1 uvicorn crud:app --reload web에서 예제를 실행합니다.\npsql을 이용해 database를 확인합니다.\nraw_data 1 2 3 4 5 postgres=# SELECT * FROM raw_data; id | timestamp | sepal_width | sepal_length | petal_width | petal_length ----+----------------------------+-------------+--------------+-------------+-------------- 1 | 2022-09-26 05:47:41.517851 | 0 | 0 | 0 | 0 (1 rows) prediction 1 2 3 4 5 postgres=# SELECT * FROM prediction; id | timestamp | iris_class ----+----------------------------+------------ 1 | 2022-09-26 05:47:41.517851 | 2 (1 rows) ","date":"2022-09-26T19:00:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-4.-model-api-2-write-data-to-db/","title":"[ Chapter 4. Model API ] 2) Write Data to DB"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 iris 데이터를 입력받아 예측할 수 있는 API를 작성합니다. 요구사항 POST predict/ \u0026amp; Request Body {data} 를 이용할 경우 모델의 예측값을 반환하는 API를 작성합니다. API 명세서를 작성해봅니다. schemas.py 에 Pydantic을 사용해 input_schema와 output_schema를 작성합니다. input_schema: column 명은 앞선 db 챕터에서 작성한 이름을 기준으로 합니다. output_schema 모델의 결과를 iris_class 의 key값과 같이 반환합니다. 작성한 내용으로 crud.py를 작성합니다. 예측에 사용하는 모델은 mlflow에서 다운로드 받거나 바로 학습해서 사용해도 무관합니다. 이 후 챕터에서는 mlflow에서 모델을 다운로드 받아서 사용하게 됩니다. 작성된 predict API에 데이터를 전달해 예측값을 받아 봅니다. API Specification iris 데이터를 입력으로 받는 명세서를 작성하면 다음과 같습니다.\nHeader: POST /predict Request body: 1 2 3 4 5 6 { \u0026#34;sepal_width\u0026#34;: 0, \u0026#34;sepal_length\u0026#34;: 0, \u0026#34;petal_width\u0026#34;: 0, \u0026#34;petal_length\u0026#34;: 0 } Response Body: 1 2 3 { \u0026#34;iris_class\u0026#34;: 2, } Schemas 입력으로 받을 input_schema와 예측값을 반환하는 output_schema를 작성합니다.\nInput Schema 입력으로 받는 값은 db에서 변환한 iris 의 column들 입니다.\n1 2 3 4 5 6 7 8 from pydantic import BaseModel class PredictIn(BaseModel): sepal_width: float sepal_length: float petal_width: float petal_length: float Output Schema 반환하는 값은 모델의 예측값으로 그 key 값은 iris_class 입니다.\n1 2 class PredictOut(BaseModel): iris_class: int Crud 위에서 작성한 schema를 사용한 app 을 작성합니다.\nApp 우선 필요한 패키지와 FastAPI를 사용하는 app을 작성합니다.\n1 2 3 4 5 6 7 8 9 import os import mlflow import pandas as pd from fastapi import FastAPI from schemas import PredictIn, PredictOut app = FastAPI() Load Model 이제 예측에 사용할 모델을 불러와야 합니다. 매번 모델을 부르는건 비효율적이니 한 번만 불리도록 전역 변수로 설정합니다.\n다운로드 받은 모델이 위치하는 경로에서 모델을 불러올 수 있도록 합니다.\n여기서는 한 디렉토리 아래에 모델이 저장되어 있어서 아래와 같이 작성했습니다.\n1 MODEL = mlflow.pyfunc.load_model(\u0026#34;../model/\u0026#34;) POST 이제 predict 함수를 작성합니다.\n1 2 3 4 5 @app.post(\u0026#34;/predict\u0026#34;, response_model=PredictOut) def predict(data: PredictIn): df = pd.DataFrame([data.dict()]) pred = int(MODEL.predict(df)) return PredictOut(iris_class=pred) API를 통해 입력받는 값은 json 형태로 들어옵니다. 모델에서 예측을 하려면 numpy array 나 pandas dataframe 형태여야 하기 때문에 변환이 필요합니다. BaseModel.dict() 를 사용하면 쉽게 python dictionary 형태로 바꿀 수 있습니다.\n1 2 3 4 5 6 7 \u0026gt;\u0026gt;\u0026gt; data.dict() { \u0026#34;sepal_width\u0026#34;: 0, \u0026#34;sepal_length\u0026#34;: 0, \u0026#34;petal_width\u0026#34;: 0, \u0026#34;petal_length\u0026#34;: 0 } 이를 dataframe으로 변환합니다. 이 때 dataframe은 2차원의 형태의 데이터를 요구하기 때문에 list로 한번 감싸줍니다.\n1 2 3 \u0026gt;\u0026gt;\u0026gt; pd.DataFrame([data.dict()]) sepal_width sepal_length petal_width petal_length 0 0 0 0 0 이제 모델로 예측을 진행합니다. 모델로 예측을 할 경우 예측값은 numpy array로 나옵니다. 그런데 위에서 정의한 output_schema 에서는 int 를 입력해야 하기 때문에 int 로 변환 시켜줍니다.\n1 pred = int(MODEL.predict(df)) 마지막으로 PredictOut schema에 값을 입력후 반환합니다.\n1 return PredictOut(iris_class=pred) 실행 uvicorn 을 이용해 app을 실행합니다.\n1 uvicorn crud:app --reload web에서 예제를 실행하면 다음과 같이 나옵니다.\n","date":"2022-09-26T18:00:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-4.-model-api-1-api-with-model/","title":"[ Chapter 4. Model API ] 1) API with Model"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞서 작성한 스크립트를 실행할 수 있는 Dockerfile을 작성하고 이미지를 빌드합니다. 요구사항 작성한 script를 build할 수 있는 dockerfile을 작성합니다. 작성한 dockerfile을 이용해 이미지를 build 합니다. 빌드된 이미지를 실행합니다. api가 정상적으로 동작하는지 확인합니다. Dockerfile 작성한 app을 실행할 수 있는 Dockerfile을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM python:3.9-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN pip install -U pip \u0026amp;\u0026amp;\\ pip install \u0026#34;fastapi[all]\u0026#34; sqlalchemy psycopg2-binary COPY app/ . ENTRYPOINT [\u0026#34;uvicorn\u0026#34;, \u0026#34;crud:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--reload\u0026#34;] docker-compose db 와 app 이 함께 실행될 수 있도록 docker-compose 를 이용하도록 하겠습니다.\n우선 다음과 같은 docker-compose.yaml을 Dockerfile과 app과 같은 위치에 있도록 작성합니다.\n1 2 3 4 5 6 7 8 . ├── Dockerfile ├── app │ ├── crud.py │ ├── database.py │ ├── models.py │ └── schemas.py └── docker-compose.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 services: db: image: postgres:latest ports: - \u0026#34;5432:5432\u0026#34; environment: POSTGRES_PASSWORD: \u0026#34;mypassword\u0026#34; app: platform: linux/amd64 build: ./ depends_on: - db ports: - \u0026#34;8000:8000\u0026#34; environment: - SQLALCHEMY_DATABASE_URL=postgresql://postgres:mypassword@db:5432/postgres db 이름으로 database 관련된 정보를 작성합니다.\napp 이름으로 fastapi 관련된 정보를 작성합니다.\n이 때 build 옵션을 통해 docker 이미지를 빌드하고 실행할 수 있습니다. 같은 위치에 app과 dockerfile을 두었으므로 ./ 를 입력합니다.\ndepends_on 옵션을 통해 app 이 db가 실행된 뒤에 실행되도록 합니다.\ndatabase.py 에서 docker-compose가 띄운 db 정보를 알 수 있도록 SQLALCHEMY_DATABASE_URL 을 환경변수로 입력합니다.\ndb의 주소는 service name을 통해 접근할 수 있습니다.\n","date":"2022-09-23T20:50:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-3.-api-5-build-fastapi-docker/","title":"[ Chapter 3. API ] 5) Build FastAPI Docker"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞서 작성한 fastapi의 정보를 postgesql db에 작성합니다. 요구사항 Postgresql 서버를 실행합니다. 데이터가 저장되고 있는 도커와는 다른 서버를 실행해주세요. 포트가 사용중이라면 5433 으로 포워딩합니다. 다음 과정에 따라 create 함수로 입력 받는 값을 postgresql 에 저장하게 합니다. ORM(Object Relational Mapping) 에 대해서 알아봅니다. [ORMs] database.py 파일을 생성합니다.[File Structure] 튜토리얼을 따라 postgresql 과 연결할 수 있는 session_maker를 작성합니다. [Create the SQLAlchemy parts] models.py 파일을 생성합니다. tablename은 users로 합니다. 튜토리얼을 따라 postgresql 에서 사용하는 모델을 생성합니다. [Create the database model] Column Type Key id Integer Primary, Index name String Unique, Index nickname String schemas.py 파일을 생성합니다. [create-pydantic-models-schemas-for-reading-returning] crud.py 파일을 생성 후 아래 함수를 작성합니다. [Crud Utils] get_user creat_user 구현 database [Create the SQLAlchemy parts]을 따라서 database.py 파일을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 # database.py import os from sqlalchemy import create_engine from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker SQLALCHEMY_DATABASE_URL = os.getenv(\u0026#34;SQLALCHEMY_DATABASE_URL\u0026#34;, \u0026#34;postgresql://postgres:mypassword@localhost:5432/postgres\u0026#34;) engine = create_engine(SQLALCHEMY_DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() 이 중 SQLALCHEMY_DATABASE_URL 값은 나중에 docker에서 값을 변경해서 전달 할 수 있도록 os.getenv를 통해 가져오도록 합니다.\n로컬에서 진행하는 튜토리얼에서는 기본 값으로 사용하도록 합니다.\nmodels [Create the database model]을 따라서 models.py 파일을 작성합니다.\ntablename은 요구사항에 맞춰 users로 정합니다. 나머지 column들도 주어진 요구사항에 맞춰서 작성합니다.\nColumn Type Key id Integer Primary, Index name String Unique, Index nickname String id : Column(Integer, primary_key=True, index=True) name: Column(String, unique=True, index=True) nickname: Column(String) 1 2 3 4 5 6 7 8 9 10 11 12 # models.py from sqlalchemy import Column, Integer, String from database import Base class User(Base): __tablename__ = \u0026#34;users\u0026#34; id = Column(Integer, primary_key=True, index=True) name = Column(String, unique=True, index=True) nickname = Column(String) schemas [create-pydantic-models-schemas-for-reading-returning]을 따라서 schemas.py 파일을 작성합니다.\n이 전 포스트에서 작성한 pydantic model을 활용합니다.\n우선 UseCreateIn, UserCreateOut 모두에서 사용할 UserBase를 작성합니다.\nUseCreateIn, UserCreateOut는 작성한 UserBase 를 상속받습니다\nUserCreateOut 는 추가로 status 와 id를 입력합니다.\n마지막으로 database 에서 사용할 User class를 작성합니다. db에서 사용해야 하니 orm_mode 를 True 로 입력합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # schemas.py from pydantic import BaseModel class UserBase(BaseModel): name: str nickname: str class UserCreateIn(UserBase): pass class UserCreateOut(UserBase): status: str id: int class User(UserBase): id: int class Config: orm_mode = True crud [Crud Utils]을 따라서 crud.py 를 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # crud.py from fastapi import Depends, FastAPI from sqlalchemy.orm import Session from models import User, Base from schemas import UserCreateIn, UserCreateOut from database import SessionLocal, engine Base.metadata.create_all(bind=engine) app = FastAPI() # Dependency def get_db(): db = SessionLocal() try: yield db finally: db.close() @app.get(\u0026#34;/user\u0026#34;) def get_user(user_id: int, db: Session = Depends(get_db)): return db.query(User).filter(User.id == user_id).first() @app.post(\u0026#34;/user\u0026#34;, response_model=UserCreateOut) def create_user(user: UserCreateIn, db: Session = Depends(get_db)): db_user = User(name=user.name, nickname=user.nickname) db.add(db_user) db.commit() db.refresh(db_user) return UserCreateOut(name=db_user.name, nickname=db_user.nickname, status=\u0026#34;success\u0026#34;, id=db_user.id) 실행 작성한 app을 실행하고 databse에 저장되는지 확인합니다.\n우선 database 가 실행되지 않았다면 실행합니다.\n1 docker run -p 5432:5432 -e POSTGRES_PASSWORD=mypassword -d postgres 다음으로 uvicorn을 이용해 app을 실행합니다.\n1 uvicorn crud:app --reload POST /user 을 통해 유저를 생성합니다.\n생성 후 psql 로 생성되었는지 확인합니다.\n1 psql -h localhost -p 5432 -U postgres postgres 정상적으로 추가가 된 것을 확인할 수 있습니다.\n1 2 3 4 5 postgres=# SELECT * FROM users; id | name | nickname ----+--------+---------- 1 | string | string (1 row) ","date":"2022-09-23T20:40:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-3.-api-4-fastapi-with-postgresql/","title":"[ Chapter 3. API ] 4) FastAPI with Postgresql"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞서 작성한 API 코드를 pydantic을 이용해 작성합니다. 요구사항 앞서 작성한 Create API를 Path Parameter로 작성합니다. Create API에서 입력으로 받아야 하는 값들을 pydantic.BaseModel 로 수정합니다. [Request Body] Class CreateIn(BaseModel): create 함수를 수정합니다. [Use the model] create 후 반환하는 값을 Response Model로 수정합니다. [Response Model] Class CreateOut(BaseModel): status, id 를 반환합니다. id는 memory에 들어있는 length를 반환합니다. eg) 첫 번째 입력값의 id는 0입니다. create 함수의 반환 값을 ResponseModel로 수정합니다. Pydantic으로 작성하기 전과 후의 api의 차이점을 비교합니다. 구현 Pydantic pydantic을 이용해 create api 에서 받을 입력 스펙과 출력 스펙을 정의합니다.\n입력으로 받을 정보는 name과 nickname입니다.\n1 2 3 4 5 6 from pydantic import BaseModel class CreateIn(BaseModel): name: str nickname: str 생성 후 전달할 정보는 status와 id 입니다.\n1 2 3 class CreateOut(BaseModel): status: str id: int api 위에서 정의한 schema를 이용해 api 코드를 작성합니다.\n데코레이터의 response_model 에 CreateOut을 입력합니다.\n입력으로 받은 값을 create_in 으로 정의하고 type hint로 CreateIn을 입력합니다.\n입력으로 받은 값에서 필요한 name 과 nickname은 create_in 의 attribute로 있습니다.\n반환할 때는 CreateOut class의 init값에 입력해서 반환합니다.\n1 2 3 4 @app.post(\u0026#34;/nickname\u0026#34;, response_model=CreateOut) def create_query(create_in: CreateIn): KEY_VALUE_STORE[create_in.name] = create_in.nickname return CreateOut(status=\u0026#34;success\u0026#34;, id=len(KEY_VALUE_STORE)) 실행 위의 코드를 crud_pydantic.py 에 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 from pydantic import BaseModel from fastapi import FastAPI KEY_VALUE_STORE = {} app = FastAPI() class CreateIn(BaseModel): name: str nickname: str class CreateOut(BaseModel): status: str id: int @app.post(\u0026#34;/nickname\u0026#34;, response_model=CreateOut) def create_query(create_in: CreateIn): KEY_VALUE_STORE[create_in.name] = create_in.nickname return CreateOut(status=\u0026#34;success\u0026#34;, id=len(KEY_VALUE_STORE)) uvicorn으로 실행합니다.\n1 uvicorn crud_pydantic:app --reload 비교 pydantic을 사용하기 전과 후의 swagger를 비교하면 다음과 같습니다. pydantic을 사용할 경우 request_body로 받을 때의 예시와 response 에 대한 예시가 생성된 것을 확인할 수 있습니다.\nBefore After ","date":"2022-09-23T20:00:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-3.-api-3-fastapi-crud-with-pydantic/","title":"[ Chapter 3. API ] 3) FastAPI CRUD with Pydantic"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 FastAPI를 이용해 CRUD가 되는 api를 작성합니다. Path Parameter 와 Query Parameter의 차이점을 알아봅니다. 요구사항 다음의 CRUD를 수행하는 fastapi의 명세서를 작성합니다. Create: 이름과 별명을 입력할 수 있습니다. Read: 이름을 입력하면 별명을 얻을 수 있습니다. 만약 입력된 이름이 없다면 400 code 와 함께 Name not found 메세지를 반환합니다. 이를 위해서 fastapi.HTTPException 를 사용합니다. [FastAPI Handling Errors] Update: 이름과 새로운 별명을 입력하면 이름의 별명이 업데이트 됩니다. 만약 입력된 이름이 없다면 400 code 와 함께 Name not found 메세지를 반환합니다. Delete: 이름을 입력하면 해당 이름과 별명을 삭제합니다. 만약 입력된 이름이 없다면 400 code 와 함께 Name not found 메세지를 반환합니다. 작성한 명세서를 구현합니다. API에 필요한 정보들은 메모리에 저장합니다. fastapi의 swagger에서 다음 시나리오를 확인합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello2\u0026quot;, \u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;} 로 create 합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello2\u0026quot;, \u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;} 로 update 합니다. 에러가 발생하는지 확인합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} 로 read 합니다. 정상적으로 “world\u0026quot; 를 반환하는 지 확인합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;2} 로 update 합니다. 주어{\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} 로 read 합니다. 정상적으로 “world2\u0026quot; 를 반환하는 지 확인합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} 로 delete 합니다. {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} 로 read합니다. Path Parameter 와 Query Parameter의 차이점은 무엇인가요? 명세서 예시 이름과 별명을 입력받는 명세서\nPath Parameter\nRequest Header\nPOST /example_1 Request Body\n1 2 3 4 { \u0026#34;name\u0026#34;: \u0026#34;hello\u0026#34;, \u0026#34;nickname\u0026#34;: \u0026#34;world\u0026#34; } Response\n1 2 3 { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Query Parameter\nRequest Header\nPOST /example_2/name/{name}/nickname/{nickname} Request Body\n1 {} Response\n1 2 3 { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } API Specification 주어진 기능에 따른 명세서를 작성하면 다음 표들과 같습니다.\n1. Create 이름과 별명을 입력할 수 있습니다\nParameter Request Head Request Body Response Body Path POST /nickname {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} Query POST /nickname/name/{name}/nickname/{nickname] {} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} 2. Read 이름을 입력하면 별명을 얻을 수 잇습니다.\nParameter Request Head Request Body Response Body Path GET /nickname {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} {\u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;} Query GET /nickname/name/{name} {} {\u0026quot;nickname\u0026quot;: \u0026quot;world\u0026quot;} 3. Update 이름과 새로운 별명을 입력하면 이름의 별명이 업데이트 됩니다. Parameter Request Head Request Body Response Body Path PUT /nickname {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;, \u0026quot;nickname\u0026quot;: \u0026quot;dlrow\u0026quot;} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} Query PUT /nickname/name/{name}/nickname/{nickname} {} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} 4. Delete Parameter Request Head Request Body Response Body Path DELETE /nickname {\u0026quot;name\u0026quot;: \u0026quot;hello\u0026quot;} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} Query DELETE /nickname/name/{name} {} {\u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;} 구현 위에 적힌 명세서 내용을 구현합니다.\nMemory 우선 입력된 key:value 를 저장하기 위해서 dict 를 선업하고 app을 생성합니다.\n1 2 3 4 5 from fastapi import FastAPI, HTTPException KEY_VALUE_STORE = {} app = FastAPI() Response Fail 만약 없는 name에 대한 요청을 할 경우 발생할 에러인 HTTPException 선업합니다.\nstatus code는 정의된 것처럼 400을 메세지는 Name not found. 를 입력합니다.\n1 NAME_NOT_FOUND = HTTPException(status_code=400, detail=\u0026#34;Name not found.\u0026#34;) Create 입력받은 name과 nickname을 KEY_VALUE_STORE 에 저장합니다.\npath 1 2 3 4 @app.post(\u0026#34;/nickname\u0026#34;) def create_path(name: str, nickname: str): KEY_VALUE_STORE[name] = nickname return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} query 1 2 3 4 @app.post(\u0026#34;/nickname/name/{name}/nickname/{nickname}\u0026#34;) def create_query(name: str, nickname: str): KEY_VALUE_STORE[name] = nickname return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} Read 입력받은 name이 KEY_VALUE_STORE에 없다면 HTTPException 를 raise 합니다.\npath 1 2 3 4 5 @app.get(\u0026#34;/nickname\u0026#34;) def get_path(name: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND return {\u0026#34;nickname\u0026#34;: KEY_VALUE_STORE[name]} query 1 2 3 4 5 @app.get(\u0026#34;/nickname/name/{name}\u0026#34;) def get_query(name: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND return {\u0026#34;nickname\u0026#34;: KEY_VALUE_STORE[name]} Uodate path 1 2 3 4 5 6 @app.put(\u0026#34;/nickname\u0026#34;) def update_path(name: str, nickname: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND KEY_VALUE_STORE[name] = nickname return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} query 1 2 3 4 5 6 @app.put(\u0026#34;/nickname/name/{name}/nickname/{nickname}\u0026#34;) def update_query(name: str, nickname: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND KEY_VALUE_STORE[name] = nickname return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} Delete path 1 2 3 4 5 6 @app.delete(\u0026#34;/nickname\u0026#34;) def delete_path(name: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND del KEY_VALUE_STORE[name] return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} query 1 2 3 4 5 6 @app.delete(\u0026#34;/nickname/name/{name}\u0026#34;) def delete_query(name: str): if name not in KEY_VALUE_STORE: raise NAME_NOT_FOUND del KEY_VALUE_STORE[name] return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;} 실행 전체코드 위의 코드를 tutorial.py 로 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # tutorial.py from fastapi import FastAPI from enum import Enum app = FastAPI() @app.get(\u0026#34;/\u0026#34;) async def root(): return {\u0026#34;message\u0026#34;: \u0026#34;Hello World\u0026#34;} @app.get(\u0026#34;/items/{item_id}\u0026#34;) async def read_item(item_id: int): return {\u0026#34;item_id\u0026#34;: item_id} @app.get(\u0026#34;/users/me\u0026#34;) async def read_user_me(): return {\u0026#34;user_id\u0026#34;: \u0026#34;the current user\u0026#34;} @app.get(\u0026#34;/users/{user_id}\u0026#34;) async def read_user(user_id: str): return {\u0026#34;user_id\u0026#34;: user_id} @app.get(\u0026#34;/users\u0026#34;) async def read_users(): return [\u0026#34;Rick\u0026#34;, \u0026#34;Morty\u0026#34;] @app.get(\u0026#34;/users\u0026#34;) async def read_users2(): return [\u0026#34;Bean\u0026#34;, \u0026#34;Elfo\u0026#34;] class ModelName(str, Enum): alexnet = \u0026#34;alexnet\u0026#34; resnet = \u0026#34;resnet\u0026#34; lenet = \u0026#34;lenet\u0026#34; @app.get(\u0026#34;/models/{model_name}\u0026#34;) async def get_model(model_name: ModelName): if model_name is ModelName.alexnet: return {\u0026#34;model_name\u0026#34;: model_name, \u0026#34;message\u0026#34;: \u0026#34;Deep Learning FTW!\u0026#34;} if model_name.value == \u0026#34;lenet\u0026#34;: return {\u0026#34;model_name\u0026#34;: model_name, \u0026#34;message\u0026#34;: \u0026#34;LeCNN all the images\u0026#34;} return {\u0026#34;model_name\u0026#34;: model_name, \u0026#34;message\u0026#34;: \u0026#34;Have some residuals\u0026#34;} @app.get(\u0026#34;/files/{file_path:path}\u0026#34;) async def read_file(file_path: str): return {\u0026#34;file_path\u0026#34;: file_path} 그리고 uvicorn 명령어로 실행합니다.\n1 uvicorn run tutorial:app --reload 실행후 http://127.0.0.1:8000/docs에 접속하면 다음과 같이 작성된 swagger를 확인할 수 있습니다.\n","date":"2022-09-20T16:50:00+09:00","image":"https://aiden-jeon.github.io/blog/p/chapter-3.-api-2-fastapi-crud/swagger_hu1247325439913060603.png","permalink":"https://aiden-jeon.github.io/blog/p/chapter-3.-api-2-fastapi-crud/","title":"[ Chapter 3. API ] 2) FastAPI CRUD"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 mlflow에 저장된 모델을 다운로드 받을 수 있는 스크립트를 작성합니다. 요구사항 run id를 입력하면 mlflow에 저장된 모델을 다운로드 받을 수 있는 스크립트를 작성합니다. python sdk: mlflow.MlflowClient bash: rclone 앞선 챕터에서 로깅된 run_id를 이용해 모델을 다운로드 합니다. 다운로드 받은 모델을 mlflow.pyfunc.load_model 을 이용해 load 합니다. Download Model Run List mlfow에 저장되어 있는 run list를 확인합니다.\n1 2 3 4 5 6 import mlflow client = mlflow.MlflowClient(\u0026#34;http://localhost:5000\u0026#34;) runs = client.list_run_infos(experiment_id=0) run_ids = [run.run_id for run in runs] 해당 스크립트를 실행하면 run_ids 에 다음과 같이 저장되어 있습니다.\n1 2 \u0026gt;\u0026gt;\u0026gt; run_ids [\u0026#34;1e3a1e8a345d4365a971dded8693938b\u0026#34;] 이 run_id는 mlflow web 에서 표시되는 것 과 동일한 것을 확인할 수 있습니다. Download Artifact 다음으로 run_id 의 모델을 다운로드 받을 수 있는 코드를 작성합니다. 이 때 주의해야 할 점은 run_id의 정보로 얻을 수 있는 artifact_path는 전체 artifact 들의 root 주소입니다. 그래서 필요한 모델만을 받기 위해서 list_artifacts로 전체 파일 및 폴더 목록을 확인하고 모델이 저장되어 있는 폴더만을 다운로드 받도록 합니다.\n1 2 3 4 5 6 7 8 9 import os import mlflow os.makedirs(\u0026#34;download\u0026#34;, exist_ok=True) run_id = \u0026#34;1e3a1e8a345d4365a971dded8693938b\u0026#34; client = mlflow.MlflowClient(\u0026#34;http://localhost:5000\u0026#34;) artifact = client.list_artifacts(run_id)[0].path client.download_artifacts(run_id=run_id, path=artifact, dst_path=download) 위 코드를 실행시키면 아래와 같이 download 폴더 밑에 iris_model 이 있음을 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 . ├── download │ └── iris_model │ ├── MLmodel │ ├── conda.yaml │ ├── model.pkl │ ├── python_env.yaml │ └── requirements.txt Load Model 이제 다운로드 받은 모델을 load 하는 코드를 작성합니다.\n1 2 3 4 import mlflow dst_path = \u0026#34;download/iris_model\u0026#34; model = mlflow.pyfunc.load_model(dst_path) load된 모델을 확인하면 다음과 같습니다.\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; model mlflow.pyfunc.load_model(\u0026#34;download/iris_model\u0026#34;) mlflow.pyfunc.loaded_model: artifact_path: iris_model flavor: mlflow.sklearn run_id: 1e3a1e8a345d4365a971dded8693938b load된 모델이 iris 데이터를 predict 할 수 있는지 확인합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 from sklearn.datasets import load_iris df, _ = load_iris(return_X_y=True, as_frame=True) rename_rule = { \u0026#34;sepal length (cm)\u0026#34;: \u0026#34;sepal_width\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;: \u0026#34;sepal_length\u0026#34;, \u0026#34;petal length (cm)\u0026#34;: \u0026#34;petal_width\u0026#34;, \u0026#34;petal width (cm)\u0026#34;: \u0026#34;petal_length\u0026#34;, } df = df.rename(columns=rename_rule) sample = df.sample(1) pred = model.predict(sample) 데이터와 예측값을 확인하면 다음과 같습니다.\n1 2 3 4 5 6 \u0026gt;\u0026gt;\u0026gt; sample sepal_width sepal_length petal_width petal_length 2 4.7 3.2 1.3 0.2 \u0026gt;\u0026gt;\u0026gt; pred [0] 이렇게 load된 모델이 정상적으로 predict 할 수 있음을 확인했습니다.\n","date":"2022-09-20T13:45:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-4-download-model-from-mlflow/","title":"[ Chapter 2. Model Registry ] 4) Download Model from MLFlow"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 모델을 학습하고 mlflow server에 저장합니다. 요구사항 앞선 챕터에서 추출한 데이터를 이용해 모델을 학습합니다.\neg) from sklearn.svm import SVC 모델을 학습하는 스크립트의 위치는 mlflow server 가 띄어진 위치와 같아야 합니다. 1 2 3 4 5 . ├── mlruns │ └── 0 │ └── meta.yaml └── train.py 학습이 끝난 모델을 built-in method를 사용해 mlflow server에 저장합니다.\nPython의 mlflow 패키지를 이용합니다. pip install mlflow mlflow를 이용해 logging 하는 방법은 두 가지가 있습니다. fluent client [MLFlow Client] mlflow에는 모델을 저장하는 방법은 두 가지가 있습니다. artifact 처럼 다루기 [MLFLow log_artifact] built-in method 사용하기 MLFlow built-in Model Flavors MLFLow pyfunc log_model 저장된 모델을 mlflow website에서 확인합니다.\n모델이 어떻게 저장되어 있는지 확인합니다. [MLFlow Storage Format] Train 모델을 학습하는 스크립트를 작성합니다. 이 때 데이터는 이 전 챕터에서 사용한 데이터를 이용합니다.\n1 2 3 4 5 6 7 from sklearn.svm import SVC X = df.drop([\u0026#34;id\u0026#34;, \u0026#34;target\u0026#34;], axis=\u0026#34;columns\u0026#34;) y = df[\u0026#34;target\u0026#34;] classifier = SVC() classifier.fit(X, y) Save Model mlflow를 사용할 수 있는 방법중 fluent 와 built-in method를 사용해 모델을 저장하겠습니다.\nSet Tracking URI 우선 앞선 챕터에서 띄운 mlflow server에 로깅할 수 있도록 set_tracking_uri를 이용해 mlflow 의 주소를 지정합니다.\n1 2 3 4 import mlflow mlflow.set_tracking_uri(\u0026#34;http://localhost:5000\u0026#34;) Log Model 다음으로 built-in method 중 학습한 모델이 scikit-learn 모델이기 때문에 mlflow.sklearn 를 이용해 모델을 저장합니다.\n1 mlflow.sklearn.log_model(model, \u0026#34;iris_model\u0026#34;) 위 명령어를 실행하면 다음과 같이 mlruns에 run_id 폴더가 생성되고 밑에 필요한 정보들이 추가됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 . ├── mlruns │ └── 0 │ ├── 8893c67cc1a44a29bc38bdde4559508c │ │ ├── artifacts │ │ │ └── iris_model │ │ │ ├── MLmodel │ │ │ ├── conda.yaml │ │ │ ├── model.pkl │ │ │ ├── python_env.yaml │ │ │ └── requirements.txt │ │ ├── meta.yaml │ │ ├── metrics │ │ ├── params │ │ └── tags │ │ ├── mlflow.log-model.history │ │ ├── mlflow.source.git.commit │ │ ├── mlflow.source.name │ │ ├── mlflow.source.type │ │ └── mlflow.user │ └── meta.yaml └── train.py Web http://localhost:5000 에 접속하면 다음과 같이 나옵니다. 생성된 run을 클릭합니다. 위와 같이 artifacts에 iris_model 폴더 밑에 모델과 메타 정보들이 저장된 것을 확인할 수 있습니다.\n","date":"2022-09-20T13:30:00+09:00","image":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-3-train-model-and-save-to-mlflow/run_hu13074674388358592129.png","permalink":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-3-train-model-and-save-to-mlflow/","title":"[ Chapter 2. Model Registry ] 3) Train Model and Save to MLFlow"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞선 챕터에서 만든 DB에서 데이터를 추출합니다. 요구사항 pandas.read_sql 함수를 이용합니다. id column을 기준으로 최신 데이터 100개를 추출하는 쿼리문을 작성합니다. 데이터를 추출합니다. 1. pandas.read_sql 함수를 이용합니다. pandas.read_sql 는 입력 argument로 Query문과 DB Connection을 받습니다.\n2. id column을 기준으로 최신 데이터 100개를 추출하는 쿼리문을 작성합니다. 요구사항을 Query문으로 작성하면 다음과 같습니다.\n1 SELECT * FROM iris_data ORDER BY id DESC LIMIT 10; psql 에서 해당 쿼리문을 입력하면 다음과 같이 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 postgres=# SELECT * FROM iris_data ORDER BY id DESC LIMIT 10; id | sepal_width | sepal_length | petal_width | petal_length | target ------+-------------+--------------+-------------+--------------+-------- 5789 | 4.4 | 3 | 1.3 | 0.2 | 0 5788 | 5.1 | 3.7 | 1.5 | 0.4 | 0 5787 | 5 | 3.4 | 1.6 | 0.4 | 0 5786 | 5.3 | 3.7 | 1.5 | 0.2 | 0 5785 | 5 | 3 | 1.6 | 0.2 | 0 5784 | 6.1 | 3 | 4.9 | 1.8 | 2 5783 | 6.7 | 3 | 5 | 1.7 | 1 5782 | 6.9 | 3.1 | 5.4 | 2.1 | 2 5781 | 6.8 | 3.2 | 5.9 | 2.3 | 2 5780 | 5.9 | 3.2 | 4.8 | 1.8 | 1 (10 rows) 3. 데이터를 추출합니다. postgres 에 연결할 수 있는 db connection을 생성 후 쿼리문과 db 커넥션을 이용해 데이터를 불러옵니다.\n1 2 3 4 5 6 import pandas as pd import psycopg2 db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) df = pd.read_sql(\u0026#34;SELECT * FROM iris_data ORDER BY id DESC LIMIT 10\u0026#34;, db_connect) 추출된 데이터를 확인하면 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt;\u0026gt;\u0026gt; df id sepal_width sepal_length petal_width petal_length target 0 5824 6.0 2.2 5.0 1.5 2 1 5823 5.7 4.4 1.5 0.4 0 2 5822 6.0 2.7 5.1 1.6 1 3 5821 6.3 2.3 4.4 1.3 1 4 5820 4.8 3.4 1.9 0.2 0 5 5819 4.5 2.3 1.3 0.3 0 6 5818 6.7 3.3 5.7 2.1 2 7 5817 5.2 4.1 1.5 0.1 0 8 5816 5.7 4.4 1.5 0.4 0 9 5815 5.4 3.0 4.5 1.5 1 ","date":"2022-09-20T13:10:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-2-get-data-from-db/","title":"[ Chapter 2. Model Registry ] 2) Get Data from DB"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 로컬에서 MLFLow Server를 띄어봅시다. 요구사항 로컬에 mlflow server를 띄웁니다. mlflow server 를 사용해 서버를 띄웁니다. mlflow server는 \u0026ldquo;3.모델 학습 및 저장\u0026rdquo; 와 작성할 스크립트와 같은 디렉토리에서 실행되어야 합니다. mlflow ui website를 확인합니다. Script Install mlflow 를 설치합니다.\n1 pip install mlflow Run 다음 명령어로 mlflow를 실행합니다.\n1 mlflow server 실행되면 다음과 같이 출력됩니다.\n1 2 3 4 5 6 7 8 \u0026gt; mlflow server [2022-09-20 11:55:20 +0900] [23908] [INFO] Starting gunicorn 20.1.0 [2022-09-20 11:55:20 +0900] [23908] [INFO] Listening at: http://127.0.0.1:5000 (23908) [2022-09-20 11:55:20 +0900] [23908] [INFO] Using worker: sync [2022-09-20 11:55:20 +0900] [23909] [INFO] Booting worker with pid: 23909 [2022-09-20 11:55:21 +0900] [23910] [INFO] Booting worker with pid: 23910 [2022-09-20 11:55:21 +0900] [23911] [INFO] Booting worker with pid: 23911 [2022-09-20 11:55:21 +0900] [23912] [INFO] Booting worker with pid: 23912 실행된 후에는 디렉토리에 mlruns 폴더가 생성됩니다.\n1 2 3 4 . └── mlruns └── 0 └── meta.yaml Web http://localhost:5000 에 접속합니다.\n","date":"2022-09-19T17:50:00+09:00","image":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-1-run-mlflow-server/mlflow-ui_hu11472424883553230026.png","permalink":"https://aiden-jeon.github.io/blog/p/chapter-2.-model-registry-1-run-mlflow-server/","title":"[ Chapter 2. Model Registry ] 1) Run Mlflow Server"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞서 작성한 스크립트를 실행할 수 있는 Dockerfile을 작성하고 이미지를 빌드합니다. 요구사항 작성한 script를 build할 수 있는 dockerfile을 작성합니다. 작성한 dockerfile을 이용해 이미지를 build 합니다. 빌드된 이미지를 실행합니다. 이미지가 실행될 때의 host에 대해서 생각해 봅니다. psql 등을 이용해 DB에 데이터가 계속해서 쌓이고 있는지 확인합니다. Docker Dockerfile 앞서 작성한 스크립트를 실행할 수 있는 Dockerfile을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM python:3.9-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN pip install -U pip \u0026amp;\u0026amp;\\ pip install scikit-learn pandas psycopg2-binary COPY insert_row_loop.py insert_row_loop.py ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;insert_row_loop.py\u0026#34;] base가 되는 이미지는 python:3.9-slim을 사용하도록 하겠습니다. 복사할 파일을 같은 곳에 묶어 두기 위해서 WORKDIR 은 /app 으로 설정합니다. 다음으로, pip install 할 수 있는 apt pacakge들을 설치합니다. 앞서 만든 스크립트에서 필요한 패키지인 scikit-learn, pandas와 psycopg2-binary 를 설치합니다. insert_row_loop.py 를 복사합니다. 이미지가 실행될 때 실행할 명령어를 입력합니다. Build Image 다음 명령어를 통해 docker 이미지를 build합니다.\n1 docker build . -t insert_row_loop Run Image Build된 이미지를 실행합니다.\n1 docker run insert_row_loop 하지만, 위 이미지를 실행하면 다음과 같은 에러가 출력되면 실행되지 않습니다.\n1 2 3 4 5 6 7 8 9 10 11 Traceback (most recent call last): File \u0026#34;/app/insert_row_loop.py\u0026#34;, line 45, in \u0026lt;module\u0026gt; db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) File \u0026#34;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\u0026#34;, line 122, in connect conn = _connect(dsn, connection_factory=connection_factory, **kwasync) psycopg2.OperationalError: could not connect to server: Connection refused Is the server running on host \u0026#34;localhost\u0026#34; (127.0.0.1) and accepting TCP/IP connections on port 5432? could not connect to server: Cannot assign requested address Is the server running on host \u0026#34;localhost\u0026#34; (::1) and accepting TCP/IP connections on port 5432? 이런 이유가 나오는 이유는 도커의 네트워킹과 관련되어 있습니다.\n기존의 스크립트를 실행할 때는 localhost 라는 네트워크를 공유하고 있습니다. 그래서 localhost:5432 로의 통신이 가능했습니다. 하지만 이미지가 실행되면 독립된 네트워크를 갖게 됩니다. 즉, 실행된 컨테이너 입장에서는 localhost:5432란 없는 주소입니다.\n이를 위해서는 실행되는 두 컨테이너 사이에 네트워크를 연결시켜주어야 합니다.\nDocker Network Create Network 아래 명령어를 실행하면 현재 docker에서 사용할 수 있는 목록들이 나옵니다.\n1 docekr network ls 따로 설정한 적이 없다면 아래와 같이 3개의 네트워크 리스트가 나옵니다.\n1 2 3 4 5 \u0026gt; docker network ls NETWORK ID NAME DRIVER SCOPE b28719341e4b bridge bridge local 0d5dc37e7a9d host host local 703a7cd222a5 none null local 위 3개의 네트워크들은 도커가 실행되면 기본으로 생성되는 네트워크들입니다. 도커에서는 디폴트 네트워크를 이용하는 것 보다는 사용자가 직접 네트워크를 생성해 사용하는 것을 권장하고 있습니다.\n아래 명령어를 통해 네트워크를 생성합니다.\n1 docker network create my-network 네트워크가 생성되었는지 확인해봅니다.\n1 2 3 4 5 6 \u0026gt; docker network ls NETWORK ID NAME DRIVER SCOPE b28719341e4b bridge bridge local 0d5dc37e7a9d host host local a6bc214efdb5 my-network bridge local 703a7cd222a5 none null local docker network inspect \u0026lt;NETWORK_NAME\u0026gt; 명령어를 통해 생성한 네트워크를 확인해 봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 \u0026gt; docker network inspect my-network [ { \u0026#34;Name\u0026#34;: \u0026#34;my-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;a6bc214efdb582d2e28f25be9f7ed835767757be0f20c15659461b683829b3af\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2022-09-19T07:16:58.08159655Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.22.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.22.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: {}, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] 생성한 직후에는 연결된 컨테이너가 없어서 Containers 가 비어있는 걸 확인할 수 있습니다.\nConnect Conatiner to Network docker network connect \u0026lt;NETWORK_NAME\u0026gt; \u0026lt;CONTAINER_NAME\u0026gt; 을 이용해 기존에 실행한 DB 컨테이너를 생성한 네트워크에 연결합니다.\n실행중인 db 컨테이너의 이름을 확인합니다.\n1 2 3 docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 972d4b7e0c6e postgres:latest \u0026#34;docker-entrypoint.s…\u0026#34; 3 days ago Up 3 days 0.0.0.0:5432-\u0026gt;5432/tcp db-db-1 실행한 DB 컨테이너의 이름인 db-db-1 를 my-network에 연결합니다.\n1 docker network connect my-network db-db-1 연결이 되었는지 확인해 봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 docker network inspect my-network [ { \u0026#34;Name\u0026#34;: \u0026#34;my-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;a6bc214efdb582d2e28f25be9f7ed835767757be0f20c15659461b683829b3af\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2022-09-19T07:16:58.08159655Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.22.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.22.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;972d4b7e0c6e8505ba6e8df798b35999ca8320b52777d9840bff0d86e44068e3\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;db-db-1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;054692d99556026e28d56eee40f45a839dbbdeab2bd5e853bef5618d25a86c49\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:16:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.22.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] Container에 db-db-1이 추가된 것을 확인할 수 있습니다.\nRun docker with network 이제 데이터를 삽입하는 이미지를 실행합니다.\n--network 옵션을 이용해 실행하는 이미지에 네트워크를 추가할 수 있습니다.\n1 docker run --network \u0026#34;my-network\u0026#34; insert_row_loop 위 명령어를 실행해도 여전히 에러가 발생합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; docker run --network \u0026#34;my-network\u0026#34; insert_row_loop Traceback (most recent call last): File \u0026#34;/app/insert_row_loop.py\u0026#34;, line 45, in \u0026lt;module\u0026gt; db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) File \u0026#34;/usr/local/lib/python3.9/site-packages/psycopg2/__init__.py\u0026#34;, line 122, in connect conn = _connect(dsn, connection_factory=connection_factory, **kwasync) psycopg2.OperationalError: could not connect to server: Connection refused Is the server running on host \u0026#34;localhost\u0026#34; (127.0.0.1) and accepting TCP/IP connections on port 5432? could not connect to server: Cannot assign requested address Is the server running on host \u0026#34;localhost\u0026#34; (::1) and accepting TCP/IP connections on port 5432? 이는 연결된 네트워크가 localhost가 아닌 my-network로 연결되기 때문입니다. script 파일에서 db 연결을 위한 host 부분을 아래와 같이 수정합니다. 이 때 사용하는 host 이름은 연결하려는 컨테이너의 이름입니다.\n1 db_connect = psycopg2.connect(host=\u0026#34;my-network\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) 다시 이미지를 빌드하고 실행합니다.\n1 2 docker build . -t insert_row_loop docker run --network \u0026#34;my-network\u0026#34; insert_row_loop 주의: m1 맥북은 --platform=linux/amd64 를 입력해야 정성작으로 실행됩니다.\n이제 제대로 실행되는 것을 확인할 수 있습니다. psql 명령어를 이용해 계속해서 데이터가 들어오고 있는지 확인해 봅니다.\n또한 network도 제대로 연결되었는지 확인해봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 \u0026gt; docker network inspect my-network [ { \u0026#34;Name\u0026#34;: \u0026#34;my-network\u0026#34;, \u0026#34;Id\u0026#34;: \u0026#34;a6bc214efdb582d2e28f25be9f7ed835767757be0f20c15659461b683829b3af\u0026#34;, \u0026#34;Created\u0026#34;: \u0026#34;2022-09-19T07:16:58.08159655Z\u0026#34;, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;EnableIPv6\u0026#34;: false, \u0026#34;IPAM\u0026#34;: { \u0026#34;Driver\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;Options\u0026#34;: {}, \u0026#34;Config\u0026#34;: [ { \u0026#34;Subnet\u0026#34;: \u0026#34;172.22.0.0/16\u0026#34;, \u0026#34;Gateway\u0026#34;: \u0026#34;172.22.0.1\u0026#34; } ] }, \u0026#34;Internal\u0026#34;: false, \u0026#34;Attachable\u0026#34;: false, \u0026#34;Ingress\u0026#34;: false, \u0026#34;ConfigFrom\u0026#34;: { \u0026#34;Network\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;ConfigOnly\u0026#34;: false, \u0026#34;Containers\u0026#34;: { \u0026#34;490a5ae2948f82caf6df7d5147e83763b600629701de905dcb8eb732ed737ce9\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;vigorous_mirzakhani\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;fdec99903513900ae49864079da5d2fb3cb3c09d031a4880cefc2c551a586c1a\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:16:00:03\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.22.0.3/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; }, \u0026#34;972d4b7e0c6e8505ba6e8df798b35999ca8320b52777d9840bff0d86e44068e3\u0026#34;: { \u0026#34;Name\u0026#34;: \u0026#34;db-db-1\u0026#34;, \u0026#34;EndpointID\u0026#34;: \u0026#34;054692d99556026e28d56eee40f45a839dbbdeab2bd5e853bef5618d25a86c49\u0026#34;, \u0026#34;MacAddress\u0026#34;: \u0026#34;02:42:ac:16:00:02\u0026#34;, \u0026#34;IPv4Address\u0026#34;: \u0026#34;172.22.0.2/16\u0026#34;, \u0026#34;IPv6Address\u0026#34;: \u0026#34;\u0026#34; } }, \u0026#34;Options\u0026#34;: {}, \u0026#34;Labels\u0026#34;: {} } ] container가 두개로 늘어난 것을 확인할 수 있습니다.\nDocker Entrypoint 그런데 db 컨테이너의 이름이 변경이 될 때 마다 docker 를 매번 빌드할 수 없으므로 이를 argument로 받아서 실행될 수 있게 변경해 보겠습니다.\n우선 파이썬 스크립트에서 다음과 같이 argument를 받을 수 있게 수정합니다.\n1 2 3 4 5 6 7 8 9 if __name__ == \u0026#34;__main__\u0026#34;: from argparse import ArgumentParser parser = ArgumentParser() parser.add_argument(\u0026#34;--host\u0026#34;) args = parser.parse_args() db_connect = psycopg2.connect(host=args.host, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) df = get_data() insert_row_loop(db_connect, df.sample(1)) 도커 파일에서 argument를 받을 수 있도록 수정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 FROM python:3.9-slim WORKDIR /app RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ build-essential \\ software-properties-common \\ git \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* RUN pip install -U pip \u0026amp;\u0026amp;\\ pip install psycopg2-binary scikit-learn pandas COPY insert_row_loop.py insert_row_loop.py ENTRYPOINT [\u0026#34;python\u0026#34;, \u0026#34;insert_row_loop.py\u0026#34;, \u0026#34;--host\u0026#34;] 새로운 이미지를 빌드하고 실행합니다. 이 때 argument로 db 컨테이너의 이름을 입력합니다.\n1 2 docker build . -t insert_row_loop docker run --network \u0026#34;my-network\u0026#34; insert_row_loop \u0026#34;db-db-1\u0026#34; ","date":"2022-09-19T14:50:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-1.-database-4-insert-row-loop-dockerfile/","title":"[ Chapter 1. Database ] 4) Insert Row Loop Dockerfile"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 postgres에 계속해서 데이터를 insert 하는 스크립트를 작성합니다. 요구사항 앞서 작성한 스크립트를 기반으로 5초마다 iris 데이터 하나를 삽입하는 python script를 작성합니다. psql 등을 이용해 데이터가 계속해서 삽입되고 있는지 확인합니다. Script 앞선 포스트에서 작성한 insert_row 함수에 while문을 추가합니다.\n그리고 요구사항에 맞춰서 time 패키지의 sleep 함수를 이용해 5초간 멈추게합니다.\n1 2 3 4 5 6 import time def insert_row_loop(db_connect, df): while True: insert_row(db_connect, df.sample(1)) time.sleep(5) 전체 코드는 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 # insert_data_loop.py import time import pandas as pd import psycopg2 from sklearn.datasets import load_iris def get_data(): X, y = load_iris(return_X_y=True, as_frame=True) df = pd.concat([X, y], axis=\u0026#34;columns\u0026#34;) rename_rule = { \u0026#34;sepal length (cm)\u0026#34;: \u0026#34;sepal_width\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;: \u0026#34;sepal_length\u0026#34;, \u0026#34;petal length (cm)\u0026#34;: \u0026#34;petal_width\u0026#34;, \u0026#34;petal width (cm)\u0026#34;: \u0026#34;petal_length\u0026#34;, } df = df.rename(columns=rename_rule) return df def insert_row(db_connect, data): insert_row_query = f\u0026#34;\u0026#34;\u0026#34; INSERT INTO iris_data (sepal_width, sepal_length, petal_width, petal_length, target) VALUES ( {data.sepal_width.values[0]}, {data.sepal_length.values[0]}, {data.petal_width.values[0]}, {data.petal_length.values[0]}, {data.target.values[0]} ); \u0026#34;\u0026#34;\u0026#34; print(insert_row_query) with db_connect.cursor() as cur: cur.execute(insert_row_query) db_connect.commit() def insert_row_loop(db_connect, df): while True: insert_row(db_connect, df.sample(1)) time.sleep(5) if __name__ == \u0026#34;__main__\u0026#34;: db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) df = get_data() insert_row_loop(db_connect, df) Run 스크립트를 실행합니다.\n1 python insert_data_loop.py psql을 이용해 계속해서 데이터가 삽입중인지 확인합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 postgres=# SELECT * FROM iris_data; id | sepal_width | sepal_length | petal_width | petal_length | target ----+-------------+--------------+-------------+--------------+-------- 1 | 5.9 | 3 | 5.1 | 1.8 | 2 2 | 5.7 | 3 | 4.2 | 1.2 | 1 3 | 5.7 | 3 | 4.2 | 1.2 | 1 (3 rows) postgres=# SELECT * FROM iris_data; id | sepal_width | sepal_length | petal_width | petal_length | target ----+-------------+--------------+-------------+--------------+-------- 1 | 5.9 | 3 | 5.1 | 1.8 | 2 2 | 5.7 | 3 | 4.2 | 1.2 | 1 3 | 5.7 | 3 | 4.2 | 1.2 | 1 4 | 5.7 | 3 | 4.2 | 1.2 | 1 (4 rows) ","date":"2022-09-19T14:30:00+09:00","permalink":"https://aiden-jeon.github.io/blog/p/chapter-1.-database-3-insert-row-loop/","title":"[ Chapter 1. Database ] 3) Insert Row Loop"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 앞서 띄운 postgres 서버에 테이블을 생성하고 row를 삽입해 봅시다. 요구사항 Python의 psycopg2 패키지를 이용합니다. pip install psycopg2-binary psycopg2 를 사용해 iris_data 이름을 가진 table을 만들어 봅시다. [Postgres Create Table] table은 다음과 같은 column 을 갖고 있어야 합니다. id sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target serial (primary key) float float float float int table을 생성할 때 아래 내용을 참고합니다. 파이썬에서 표시되는 float64 는 어떻게 처리해야 할까요? [Postgres Data Types] column 명은 어떻게 입력해야 할까요? [Postgres Names] primary key는 어떤 역할을 하나요? 꼭 넣어야 할까요? +) serial 타입은 어떤 역할을 할까요? 이미 테이블이 있는데 다시 생성을 요청하면 어떻게 되나요? 어떻게 방지할 수 있을까요? psycopg2 를 사용해 iris 데이터 하나를 삽입해 봅시다. psql 등을 이용해 생성한 테이블과 삽입한 데이터를 확인합니다. 1. Requirements 이번 목표를 진행하기 앞서 다음 패키지들을 설치합니다.\n1 pip install pandas psycopg2-binary scikit-learn 2. psycopg2 를 사용해 iris_data 이름을 가진 table을 만들어 봅시다. 2.1 DB Connect psycopg2 를 이용하기 위해서는 connect 함수를 이용해 database에 접근해야 합니다. database에 접근할 수 있는 connector를 db_connect에 선언합니다.\n1 2 3 import psycopg2 db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) 2.2 Create Table Query 삽입할 iris 데이터는 scikit-learn 패키지의 load_iris을 이용합니다.\n1 2 3 4 5 6 import pandas as pd from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True, as_frame=True) df = pd.concat([X, y], axis=\u0026#34;columns\u0026#34;) iris 데이터를 확인하면 다음과 같습니다.\n1 2 3 4 5 6 7 \u0026gt;\u0026gt;\u0026gt; df.dtypes sepal length (cm) float64 sepal width (cm) float64 petal length (cm) float64 petal width (cm) float64 target int64 dtype: object X 의 data type은 float64로 target은 int64로 표기됩니다. 그런데 이 데이터 타입들은 postgres에서 사용할 수 없기에 각각 float8, int로 선언해야 합니다. 또한 column 이름은 sepal length (cm) 에 포함되어 있는 ( 때문에 이용할 수 없기 때문에 해당 부분을 제거합니다.\n위의 내용을 반영한 query문은 다음과 같이 작성할 수 있습니다.\n1 2 3 4 5 6 7 8 9 create_table_query = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE iris_data ( id SERIAL PRIMARY KEY, sepal_width float8, sepal_length float8, petal_width float8, petal_length float8 target int );\u0026#34;\u0026#34;\u0026#34; 2.3 Execute Query 이제 선언한 query문을 db에 전달합니다.\n1 2 3 with db_connect.cursor() as cur: cur.execute(create_table_query) db_connect.commit() excute 함수를 통해 query를 전달할 수 있습니다. 작성 후 commit 까지 해야 정상적으로 query문이 수행됩니다.\n2.4 Full Code 아래 코드는 위의 내용을 정리한 전체 코드입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # create_table.py import psycopg2 def create_table(db_connect): create_table_query = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE iris_data ( id SERIAL PRIMARY KEY, sepal_width float8, sepal_length float8, petal_width float8, petal_length float8, target int );\u0026#34;\u0026#34;\u0026#34; print(create_table_query) with db_connect.cursor() as cur: cur.execute(create_table_query) db_connect.commit() if __name__ == \u0026#34;__main__\u0026#34;: db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) create_table(db_connect) 위 코드를 최초 실행하면 다음과 같이 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 \u0026gt; python create_table.py CREATE TABLE iris_data ( id SERIAL PRIMARY KEY, sepal_width float8, sepal_length float8, petal_width float8, petal_length float8, target int ); 하지만 두 번째 실행하면 다음과 같이 에러가 납니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; python create_table_insert_row.py CREATE TABLE iris_data ( id SERIAL PRIMARY KEY, sepal_width float8, sepal_length float8, petal_width float8, petal_length float8, target int ); Traceback (most recent call last): ... psycopg2.errors.DuplicateTable: relation \u0026#34;iris_data\u0026#34; already exists 이를 해결하기 위해서는 query에 IF NOT EXISTS 옵션을 추가해야 합니다. 추가한 전체 코드는 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # create_table.py import psycopg2 def create_table(db_connect): create_table_query = \u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS iris_data ( id SERIAL PRIMARY KEY, sepal_width float8, sepal_length float8, petal_width float8, petal_length float8, target int );\u0026#34;\u0026#34;\u0026#34; print(create_table_query) with db_connect.cursor() as cur: cur.execute(create_table_query) db_connect.commit() if __name__ == \u0026#34;__main__\u0026#34;: db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) create_table(db_connect) 2.5 psql psql cli 에 접속해서 생성된 테이블을 확인합니다.\n1 2 3 4 postgres=# SELECT * FROM iris_data; id | sepal_width | sepal_length | petal_width | petal_length | target ----+-------------+--------------+-------------+--------------+-------- (0 rows) 3. psycopg2 를 사용해 iris 데이터 하나를 삽입해 봅시다. 3.1 Load iris data iris 데이터를 생성하는 코드와 column name을 앞서 생성한 db table의 column name이 일치하도록 rename 함수를 이용해 수정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 import pandas as pd from sklearn.datasets import load_iris X, y = load_iris(return_X_y=True, as_frame=True) df = pd.concat([X, y], axis=\u0026#34;columns\u0026#34;) rename_rule = { \u0026#34;sepal length (cm)\u0026#34;: \u0026#34;sepal_width\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;: \u0026#34;sepal_length\u0026#34;, \u0026#34;petal length (cm)\u0026#34;: \u0026#34;petal_width\u0026#34;, \u0026#34;petal width (cm)\u0026#34;: \u0026#34;petal_length\u0026#34;, } df = df.rename(columns=rename_rule) 3.2 Insert Query 다음으로 data를 삽입할 수 있는 query 문을 작성합니다. 데이터를 insert 하는 query문의 포맷은 아래와 같습니다.\n1 INSRT INTO {table_name} (col_1, col_2, ...) VALUES (val_1, val_2, ...) 우선 sample 함수를 이용해 데이터 하나를 추출합니다.\n1 data = df.sample(1) 추출된 데이터를 이용해 query문을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 insert_row_query = f\u0026#34;\u0026#34;\u0026#34; INSERT INTO iris_data (sepal_width, sepal_length, petal_width, petal_length, target) VALUES ( {data.sepal_width.values[0]}, {data.sepal_length.values[0]}, {data.petal_width.values[0]}, {data.petal_length.values[0]}, {data.target.values[0]} ); \u0026#34;\u0026#34;\u0026#34; 3.3 Full Code 위의 내용을 종합해 전체 코드를 작성하면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # insert_row.py import pandas as pd import psycopg2 from sklearn.datasets import load_iris def get_data(): X, y = load_iris(return_X_y=True, as_frame=True) df = pd.concat([X, y], axis=\u0026#34;columns\u0026#34;) rename_rule = { \u0026#34;sepal length (cm)\u0026#34;: \u0026#34;sepal_width\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;: \u0026#34;sepal_length\u0026#34;, \u0026#34;petal length (cm)\u0026#34;: \u0026#34;petal_width\u0026#34;, \u0026#34;petal width (cm)\u0026#34;: \u0026#34;petal_length\u0026#34;, } df = df.rename(columns=rename_rule) return df def insert_row(db_connect, data): insert_row_query = f\u0026#34;\u0026#34;\u0026#34; INSERT INTO iris_data (sepal_width, sepal_length, petal_width, petal_length, target) VALUES ( {data.sepal_width.values[0]}, {data.sepal_length.values[0]}, {data.petal_width.values[0]}, {data.petal_length.values[0]}, {data.target.values[0]} ); \u0026#34;\u0026#34;\u0026#34; print(insert_row_query) with db_connect.cursor() as cur: cur.execute(insert_row_query) db_connect.commit() if __name__ == \u0026#34;__main__\u0026#34;: db_connect = psycopg2.connect(host=\u0026#34;localhost\u0026#34;, database=\u0026#34;postgres\u0026#34;, user=\u0026#34;postgres\u0026#34;, password=\u0026#34;mypassword\u0026#34;) df = get_data() insert_row(db_connect, df.sample(1)) 위 스크립트를 실행하면 아래와 같이 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; python insert_row.py INSERT INTO iris_data (sepal_width, sepal_length, petal_width, petal_length, target) VALUES ( 5.9, 3.0, 5.1, 1.8, 2 ); 3.4 psql psql을 이용해 삽입한 데이터를 확인합니다.\n1 2 3 4 5 postgres=# SELECT * FROM iris_data; id | sepal_width | sepal_length | petal_width | petal_length | target ----+-------------+--------------+-------------+--------------+-------- 1 | 5.9 | 3 | 5.1 | 1.8 | 2 (1 row) ","date":"2022-09-16T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-1.-database-2-create-table-and-insert-row/","title":"[ Chapter 1. Database ] 2) Create Table and Insert Row"},{"content":"Github 에서 해당 내용에 대해서 확인할 수 있습니다.\nOverview 목표 도커를 이용해 postgres 서버를 띄워봅시다. psql 을 이용해 생성된 username과 그 역할을 확인합니다. 요구사항 Postgresql User Requirements Username: postgres Role: Superuser Password: mypassword Docker Run Option port forwarding: 5432:5432 psql cli tool 을 통해 생성한 데이터 베이스에 접속합니다. 다음 option을 사용해 접속합니다. -h, --host=HOSTNAME database server host or socket directory (default: \u0026quot;local socket\u0026quot;) -U, --username=USERNAME database user name (default: \u0026quot;...\u0026quot;) psql 내부에서 다음 내용을 확인합니다. 생성한 Role name 과 role Docker 다음 명령어를 통해 기본적인 postgresql server를 실행시킬 수 있습니다.\n-d 옵션을 통해 실행된 도커와의 interaction을 주지 않습니다.\n1 docker run -d postgres 1. Postgresql User Requirements Username \u0026amp; Role Postgresql 서버를 최초 실행할 경우 생성되는 username은 postgres 이며 그 role이 Superuser 입니다.\n그렇기 때문에 해당 요구사항은 자동적으로 만족됩니다.\nPassword 컨테이너를 run할 때 -e 옵션을 통해 필요한 environment를 제공할 수 있습니다.\n최초 postgres 유저의 비밀번호는 POSTGRES_PASSWORD 를 통해 설정할 수 있습니다.\n이제 위의 명령어에 다음 내용을 추가합니다.\n1 docker run -e POSTGRES_PASSWORD=mypassword -d postgres 2. Docker Run Option 컨테이너를 run할 때 -p 옵션을 통해 지정한 port를 localhost 포트로 맵핑할 수 있습니다.\n1 docker run -p 5432:5432 -e POSTGRES_PASSWORD=mypassword -d postgres 3. psql cli tool 을 통해 생성한 데이터 베이스에 접속합니다. psql -h \u0026lt;host\u0026gt; -p \u0026lt;port\u0026gt; -U \u0026lt;username\u0026gt; \u0026lt;database\u0026gt;\n-h 를 통해 host를 지정하고 -p를 통해 포트를 지정합니다.이 때 host는 docker 에서 포트 포워딩을 했기 때문에 localhost 를 입력합니다. -U 옵션을 통해 접속해 시도할 유저 이름을 입력할 수 있습니다. 여기서는 postgres를 통해 접속합니다. database 이름 또한 따로 지정하지 않았기 때문에 기본값인 postgres를 입력합니다. 위의 내용을 바탕으로 명령어를 작성하면 다음과 같습니다.\n1 psql -h localhost -p 5432 -U postgres postgres 위 명령어를 실행하면 다음과 같이 비밀번호를 요구합니다.\n1 2 \u0026gt; psql -h localhost -p 5432 -U postgres postgres Password for user postgres: 비밀번호를 입력하고 접속하면 다음과 같이 출력됩니다.\n1 2 3 4 5 6 \u0026gt; psql -h localhost -p 5432 -U postgres postgres Password for user postgres: psql (14.3, server 14.5 (Debian 14.5-1.pgdg110+1)) Type \u0026#34;help\u0026#34; for help. postgres=# 4. psql 내부에서 다음 내용을 확인합니다. \\du 를 통해 현재 데이터베이스 내의 Role name과 role을 확인할 수 있습니다.\n1 2 3 4 5 postgres=# \\du List of roles Role name | Attributes | Member of -----------+------------------------------------------------------------+----------- postgres | Superuser, Create role, Create DB, Replication, Bypass RLS | {} Docker Compose docker-compose.yaml docker-compose.yaml 파일을 생성하고 아래 항목을 작성합니다.\n1 2 3 4 5 6 7 services: db: image: postgres:latest ports: - \u0026#34;5432:5432\u0026#34; environment: POSTGRES_PASSWORD: \u0026#34;mypassword\u0026#34; image는 postgres 의 latest tag를 사용합니다.\nports 을 통해 port-forwarding 을 설정합니다.\n앞서 docker 에서 사용한 -e 옵션은 environment에 설정할 수 있습니다. docker에서와 같이 POSTGRES_PASSWORD: \u0026quot;mypassword\u0026quot; 를 입력합니다.\ndocker-compose up 아래 명령어를 이용해 컨테이너들을 실행할 수 있습니다.\n1 docker-compose up 실행하면 아래와 같은 메세지가 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 \u0026gt; docker-compose up [+] Running 1/0 ⠿ Container db-db-1 Created 0.0s Attaching to db-db-1 db-db-1 | The files belonging to this database system will be owned by user \u0026#34;postgres\u0026#34;. db-db-1 | This user must also own the server process. db-db-1 | db-db-1 | The database cluster will be initialized with locale \u0026#34;en_US.utf8\u0026#34;. db-db-1 | The default database encoding has accordingly been set to \u0026#34;UTF8\u0026#34;. db-db-1 | The default text search configuration will be set to \u0026#34;english\u0026#34;. db-db-1 | db-db-1 | Data page checksums are disabled. db-db-1 | db-db-1 | fixing permissions on existing directory /var/lib/postgresql/data ... ok db-db-1 | creating subdirectories ... ok db-db-1 | selecting dynamic shared memory implementation ... posix db-db-1 | selecting default max_connections ... 100 db-db-1 | selecting default shared_buffers ... 128MB db-db-1 | selecting default time zone ... Etc/UTC db-db-1 | creating configuration files ... ok db-db-1 | running bootstrap script ... ok db-db-1 | performing post-bootstrap initialization ... ok db-db-1 | syncing data to disk ... initdb: warning: enabling \u0026#34;trust\u0026#34; authentication for local connections db-db-1 | You can change this by editing pg_hba.conf or using the option -A, or db-db-1 | --auth-local and --auth-host, the next time you run initdb. db-db-1 | ok db-db-1 | db-db-1 | db-db-1 | Success. You can now start the database server using: db-db-1 | db-db-1 | pg_ctl -D /var/lib/postgresql/data -l logfile start db-db-1 | db-db-1 | waiting for server to start....2022-09-16 04:16:32.249 UTC [47] LOG: starting PostgreSQL 14.5 (Debian 14.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit db-db-1 | 2022-09-16 04:16:32.251 UTC [47] LOG: listening on Unix socket \u0026#34;/var/run/postgresql/.s.PGSQL.5432\u0026#34; db-db-1 | 2022-09-16 04:16:32.254 UTC [48] LOG: database system was shut down at 2022-09-16 04:16:32 UTC db-db-1 | 2022-09-16 04:16:32.258 UTC [47] LOG: database system is ready to accept connections db-db-1 | done db-db-1 | server started db-db-1 | db-db-1 | /usr/local/bin/docker-entrypoint.sh: ignoring /docker-entrypoint-initdb.d/* db-db-1 | db-db-1 | waiting for server to shut down...2022-09-16 04:16:32.362 UTC [47] LOG: received fast shutdown request db-db-1 | .2022-09-16 04:16:32.364 UTC [47] LOG: aborting any active transactions db-db-1 | 2022-09-16 04:16:32.365 UTC [47] LOG: background worker \u0026#34;logical replication launcher\u0026#34; (PID 54) exited with exit code 1 db-db-1 | 2022-09-16 04:16:32.366 UTC [49] LOG: shutting down db-db-1 | 2022-09-16 04:16:32.381 UTC [47] LOG: database system is shut down db-db-1 | done db-db-1 | server stopped db-db-1 | db-db-1 | PostgreSQL init process complete; ready for start up. db-db-1 | db-db-1 | 2022-09-16 04:16:32.478 UTC [1] LOG: starting PostgreSQL 14.5 (Debian 14.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit db-db-1 | 2022-09-16 04:16:32.478 UTC [1] LOG: listening on IPv4 address \u0026#34;0.0.0.0\u0026#34;, port 5432 db-db-1 | 2022-09-16 04:16:32.478 UTC [1] LOG: listening on IPv6 address \u0026#34;::\u0026#34;, port 5432 db-db-1 | 2022-09-16 04:16:32.481 UTC [1] LOG: listening on Unix socket \u0026#34;/var/run/postgresql/.s.PGSQL.5432\u0026#34; db-db-1 | 2022-09-16 04:16:32.484 UTC [59] LOG: database system was shut down at 2022-09-16 04:16:32 UTC db-db-1 | 2022-09-16 04:16:32.488 UTC [1] LOG: database system is ready to accept connections 다만 이렇게 실행할 경우 커널을 종료할 수 있는 방법이 없습니다.\n그렇기 때문에 -d 옵션을 통해 컨테이너만 실행되게 합니다.\n1 docker-compose up -d ","date":"2022-09-15T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/chapter-1.-database-1-run-postgresql-server/","title":"[ Chapter 1. Database ] 1) Run Postgresql Server"},{"content":"Kafka CLI 카프카 명령어를 사용할 수 있는 CLI를 설정하는 과정에 대해서 설명합니다.\nkafka_2.13-2.8.1 를 다운로드 받습니다. (버전은 작성일 기준 도커로 이용할 수 있는 최신 버전입니다.)\n1 wget https://dlcdn.apache.org/kafka/2.8.1/kafka_2.13-2.8.1.tgz 압축을 풉니다.\n1 tar -xzf kafka_2.13-2.8.1.tgz 압축이 해제된 폴더를 홈 디렉토리로 이동합니다.\n1 2 mv kafka_2.13-2.8.1 ~/ cd ~/ 아래 항목이 실행되는지 확인합니다.\n1 2 cd kafka_2.13-2.8.1 bin/kafka-topics.sh kafka는 백엔드로 java를 사용하는데 자바가 설치되어 있지 않다면 다음 에러 메세지가 나옵니다.\n1 2 The operation couldn’t be completed. Unable to locate a Java Runtime. Please visit http://www.java.com for information on installing Java. 또한 카프카를 구동하기 위해 필요한 자바 버전은 8 로 본인의 자바 버전이 맞는지 확인합니다.\n1 java -version 만약 호환되는 자바또는 자바 버전이 없을 경우 각 OS별로 자바를 설치합니다.\nMac OS의 경우 brew를 통해 쉽게 설치할 수 있습니다.\n1 2 brew tap homebrew/cask-versions brew install --cask adoptopenjdk8 설치 후 자바 버전을 확인합니다.\n1 java -version 다음과 같이 출력 되면 정상적으로 설치되었습니다.\n1 2 3 openjdk version \u0026#34;1.8.0_292\u0026#34; OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_292-b10) OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.292-b10, mixed mode 다시 아래 명령어를 실행합니다.\n1 bin/kafka-topics.sh 정상적으로 실해오디면 다음과 같이 나옵니다.\n1 2 3 4 5 6 7 8 9 10 11 12 Create, delete, describe, or change a topic. Option Description ------ ----------- --alter Alter the number of partitions, replica assignment, and/or configuration for the topic. --at-min-isr-partitions if set when describing topics, only show partitions whose isr count is equal to the configured minimum. Not supported with the --zookeeper option. ...(생략) 이제 bin 파일에 있는 명령어를 CLI로 사용할 수 있도록 설정합니다. 사용하고 있는 shell profile 파일에 경로를 추가합니다.\n1 2 cd ~/kafka_2.13-2.8.1/bin echo $(pwd) 위 명령어를 통해 출력되는 경로를 아래 명령어의 \u0026lt;YOUR-KAFKA-BIN-PATH\u0026gt;를 수정하여서 실행합니다.\n1 2 echo \u0026#39;export PATH=\u0026#34;$PATH:\u0026lt;YOUR-KAFKA-BIN-PATH\u0026gt;\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc source ~/.zshrc 다음 명령어가 정상적으로 수행되는지 확인합니다.\n1 kafka-topics.sh Docker-compose 다음과 같은 docker-compose.yaml 파일을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 version: \u0026#39;3\u0026#39; services: zookeeper: container_name: zookeeper image: wurstmeister/zookeeper ports: - \u0026#34;2181:2181\u0026#34; kafka: image: wurstmeister/kafka:2.13-2.8.1 ports: - \u0026#34;9092:9092\u0026#34; environment: KAFKA_ADVERTISED_HOST_NAME: localhost KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181 volumes: - /var/run/docker.sock:/var/run/docker.sock docker-compose를 실행합니다.\n1 docker-compose up -d 정상적으로 실행되면 다음과 같이 출력됩니다.\n1 2 3 Creating network \u0026#34;kafka_default\u0026#34; with the default driver Creating kafka_kafka_1 ... done Creating zookeeper ... done Kafka Topic Create 테스트 토픽을 생성합니다.\n1 kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test 정상적으로 수행되면 다음과 같이 출력됩니다.\n1 Created topic test. List 생성된 토픽을 확인해 보겠습니다.\n1 kafka-topics.sh --list --bootstrap-server localhost:9092 다음과 같이 출력됩니다.\n1 test Message 전송 1 kafka-console-producer.sh --broker-list localhost:9092 --topic test 위 명령어를 입력하면 console창이 실행됩니다. 실행된 콘솔창에서 다음과 같이 입력합니다.\n1 \u0026gt; This is test message. 입력 후 단축키를 이용해 콘솔창을 종료합니다.\n읽기 이제 위에서 작성한 메세지를 확인해 보겠습니다.\n1 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 정상적으로 실행되면 다음과 같이 출력됩니다.\n1 This is test message. 단축키를 이용해 콘솔창을 종료합니다. 콘솔창이 종료되면 다음과 같은 메세지가 출력됩니다.\n1 Processed a total of 1 messages ","date":"2022-01-02T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/kafka-setup/","title":"Kafka Setup"},{"content":"알고리즘 문제 해결 전략을 읽고 요약했습니다.\n6. 무식하게 풀기 6.1 도입 무식하게 풀기 Brute Force 컴퓨터의 빠른 계산 능력을 이용해 가능한 경우의 수를 일일이 나열하면서 답을 찾는 방법 완전 탐색 Exhaustive search 컴퓨터의 장점을 이용한 방법 6.2 재귀호출과 완전탐색 재귀 함수 Recursive Function 자신이 수행한 작업을 유사한 형태의 여러 조각으로 쪼갠 뒤 그 중 한 조각을 수행하고, 나머지를 자기 자신을 호출해 실행하는 함수 eg) 1부터 n까지의 합을 구하는 함수\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 필수 조건: n\u0026gt;=1 # 결과: 1부터 n까지의 합을 반환한다. def n_sum(n): ret = 0 for i in range(1, n+1): ret += i return ret # 필수 조건: n\u0026gt;=1 # 결과: 1부터 n까지의 합을 반환한다. def recursive_sum(n): if n == 1: return n return n + recursive_sum(n-1) n개의 숫자의 합을 구하는 작업을 n개의 조각으로 쪼개, 더할 각 숫자가 하나의 조각이 되도록 한다. 재귀 호출을 이용하기 위해서는 이 조각 중 하나를 떼어내어 자신이 해결하고, 나머지 조각들을 자기 자신을 호출해서 해결한다. 모든 재귀 함수는 \u0026lsquo;더 이상 쪼개지지 않는\u0026rsquo; 최소한의 작업에 도달 했을 때 답을 곧장 반환하는 조건문을 포함한다. 쪼개기지 않는 가장 작은 작업들 : 기저 사례 (base case) 예제: 중첩 반복문 대체하기 eg) 0번 부터 차례대로 번호 매겨진 n개의 원소 중 네 개를 고르는 모든 경우를 출력하라\n1 2 3 4 5 for i in range(n): for j in range(i+1, n): for k in range(j+1, n): for l in range(k+1, n): print(i, j, k, l) if) 5개? -\u0026gt; 5중 for 문 if) 6개? -\u0026gt; 6중 for 문 중첩 for문 골라야 할 원소의 수가 늘어날수록 코드가 길고 복잡해진다. 골라야 할 원소의 수가 입력에 따라 달라질 수 있는 경우 사용할 수 없다. -\u0026gt; 재귀 호출\n위 코드 조각이 하는 일을 네 개의 조각으로 나눌 수 있다. 각 조각에서 하나를 고르고 남은 원소들을 고르는 작업을 자기 자신을 호출해 떠넘기는 재귀함수 남은 원소를 고르는 \u0026lsquo;작업\u0026rsquo;을 다음과 같은 입력들의 집합으로 정의 원소들의 총 개수 더 골라야 할 원소의 개수 지금까지 고른 원소의 번호 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # n: 전체 원소의 수 # picked: 지금까지 고른 원소들의 번호 # to_pick: 더 고를 원소의 수 def pcik(n, picked, to_pick): # 기저 사례: 더 고를 원소가 없을 때 고른 원소들을 출력한다. if to_pick == 0: print_picked(pick) return # 고를 수 있는 가장 작은 번호를 계산한다. smallest = 0 if len(pick) == 0 else pcik[-1] + 1 # 이 단계에서는 원소 하나를 고른다. for next_n in range(smallest, n): picked.append(next_n) pick(n, picked, to_pick - 1) picked.pop() 예제: 보글 게임 Q)\n보글(Boggle)은 그림 6.2(a)와 같은 5x5 크기의 알파벳 격자를 가지고 하는 게임\n게임의 목적은 상하좌우/ 대각선으로 인접한 글자들을 이어서 단어를 찾아내는 것\neg) PRETTY, GIRL, REPEAT\n각 글자들을 대각선으로도 이어질 수 있으며, 한 글자가 두 번 사용될 수도 있다.\n-\u0026gt; has_word(y, x, word)\n문제의 분할 각 글자를 하나의 조각으로 만드는 것 기자 사례의 선택 더 이상의 탐색 없이 간단히 답을 낼 수 있는 다음의 경우를 기저 사례로 선택 위치 (y, x)에 있는 글자가 원하는 단어의 첫 글자가 아닌 경우 항상 실패 (1번에 해당되지 않을 경우) 원하는 단어가 한 글자인 경우 성공 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 dx = [-1, -1, -1, 1, 1, 1, 0, 0] dy = [-1, 0, 1, -1, 0, 1, -1, 1] def has_word(y, x, word): if not in_range(y, x): return False if board[y][x] != word[0]: return False if len(word) == 1: return True for direction in range(8): new_y = y + dy[direction] new_x = x + dx[direction] if has_word(new_y, new_x, word[1:]): return True return False 시간 복잡도 분석 완전 탐색 알고리즘의 시간 복잡도 -\u0026gt; 가능한 모든 경우의 수를 전부 세보기 if) 전부 A인 격자에서 AAAAAH 찾기 답이 없는 경우 $8^{N-1}=O(8^N)$ 완전 탐색 레시피 어떤 문제를 완전 탐색으로 해결하기 위해 필요한 과정\n단계 완전 탐색은 존재하는 모든 답을 하나씩 검사하므로, 걸리는 시간은 가능한 답의 수에 정확히 비례 최대 크기의 입력을 가정했을 때 답의 개수를 계산하고 이들을 모두 제한 시간 안에 생성할 수 있을 지를 가늠 만약 시간안에 계산할 수 없다면 설계 패러다임(추후 설명)을 적용 단계 가능한 모든 답의 후보를 만드는 과정을 여거래의 선택으로 나눈다. 각 선택은 답의 호부를 만드는 과정의 한 조각 단계 그 중 하나의 조각을 선택해 답의 일부를 만들고, 나머지 답을 재귀 호출을 통해 완성 단계 조각이 하나 밖에 남지 않은 경우, 혹은 하나도 남지 않은 경우에는 답을 생성 했으므로, 이것을 기저 사례로 선택해 처리 이론적 배경: 재귀 호출과 부분문제 문제와 부문문제 예시)\n문제: 주어진 자연수를 정렬하라 문제: ${16, 7, 9, 1, 31}$을 정렬하라 차이점: 전자는 입력을 지정하지 않고 후자는 특별한 입력 지정\n보글 게임 예시)\n현재 위치 (y, x)에 단어의 첫글자가 있는가? 윗 칸에서 시작해서, 단어의 나머지 글자를 찾을 수 있는가? \u0026hellip; \u0026hellip; 2번 이후 원래 문제에서 한 조각을 떼어 냈을 뿐, 형식이 같은 또 다른 문제를 푼 결과 문제를 구성하는 조각들 중 한 조각을 뺏기 때문에, 이 문제들은 원래 문제의 일부 -\u0026gt; 이런 문제들을 원래의 부분문제 6.7 최적화 문제 문제의 답이 하나가 아니라 여러 개이고, 그 중에서 어떤 기준에 따라 가장 좋은 답을 찾아내는 문제 eg) $n$개의 원소 중 $r$개를 순서없이 골라내는 문제 최적화 문제 x 우리가 원하는 답은 딱 하나 밖에 없다 -\u0026gt; 더 좋은 답이나 덜 좋은 답이 없음 eg) $n$개 사과 중 $r$개 골라서 무게의 합을 최대화하는 문제 최적화 문제 최적화 문제 해결 방법 완전 탐색 동적 계호기법 조합 탐색 최적화 문제 -\u0026gt; 결정 문제로 변환 예제: 여행하는 예판원 문제 (Traveling Salssman Problem, TSP) 어떤 나라에 $n(2 \\le n \\le 10)$개의 큰 도시가 있다. 한 영업사원이 한 도시에서 출발해 다른 도시들을 전부 한 번씩 방문한 뒤 시작 도시로 돌아오려고 한다. 각 도시들은 모두 직선도로로 연결되어 있다. 영업사원이 여행해야 할 거리 중 가장 짧은 경로는 ?\n무식하게 풀 수 있을까? 시간안에 답을 구할 수 있을까?\n-\u0026gt; $(n-1)!$ -\u0026gt; $9!$ -\u0026gt; $362,880$ -\u0026gt; 1초안에 처리할 수 있는 숫자\n재귀 호출을 통한 답안 생성 $n$개의 도시로 구성된 경로를 $n$개의 조각으로 나눠 앞에서부터 도시를 하나씩 추가해 경로를 만들기 shortest_path(path)=path -\u0026gt; 지금까지 만든 경로, 나머지 도시를 모두 방문하는 경로들 중 가장 짧은 것을 반환한다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 def shortest_path(path, visited, current_length): # 기저 사례: 모든 도시를 다 방문했을 때는 시작도시로 돌아가고 종료 if len(path) == n: return current_length + dist[path[0]][path[-1]] ret = 987654321 for next_visit in range(n): if visited[next_visit]: continue here =path[-1] path.append(next_visit) visited[next_visit] = True cand = shorted_path(path, visited, current_length + dist[here][next_visit]) ret = min(ret, cand) visited[next_visit] = False path.pop() return ret ","date":"2021-10-26T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/6.-%EB%AC%B4%EC%8B%9D%ED%95%98%EA%B2%8C-%ED%92%80%EA%B8%B0/","title":"6. 무식하게 풀기"},{"content":"알고리즘 문제 해결 전략을 읽고 요약했습니다.\n5. 알고리즘의 정당성 증명 5.1 도입 알고리즘의 정당성 증명\n단위 테스트 알고리즘에 문제가 있음을 증명할 수는 있어도 문제가 없음을 증명할 수는 없음 다른 방법이 필요 5.2 수학적 귀납법과 반복문 불변식 eg) 100개의 도미노가 있고 다음 두가지 사실을 안다고 가정\n첫 번째 도미노는 직접 손으로 밀어서 쓰러트인다. 한 도미노가 쓰러지면 다음 도미노 역시 반드시 쓰러진다. -\u0026gt; 마지막 도미노 또한 당연히 쓰러진다.\n-\u0026gt; 직관적으로 알 수 있음\n수학적 귀납법 Mathmetical Induction 반복적인 구조를 갖는 명제들을 증명하는데 유용하게 사용되는 증명 기법 귀납법 단계 단계 나누기 증명하고 싶은 사실을 여러 단계로 나눈다. 100개의 도미노를 하나씩 나눔 첫 단계 증명 첫 단계에서 증명하고 싶은 내용이 성립함을 보인다. 첫 번째 도미노가 넘어짐을 증명 귀납 증명 다음 단계에서도 성립함을 보인다. 한 도미노가 쓰러지면 다음 도미노는 반드시 쓰러짐 사다리 게임 맨 위와 맨 아래가 1:1 대응이다. 단계 나누기 텅 빈 $N$개의 세로줄에서부터 시작해서 원하는 사다리가 될 때까지 하나씩 가로줄을 그어 간다. 이때, 가로즐을 하나 긋는 것을 한 단계라고 정의 첫 단계 증명 텅 빈 $N$개의 세로줄에서는 맨 위 선택지와 맨 아래 선택지가 1:1 대응 첫 번째 도미노가 넘어짐을 증명 귀납 증명 가로 줄을 그어서 두 개의 세로줄을 연결, 이 때 두 세로줄의 결과는 서로 뒤바뀐다. 두 세로줄의 결과가 바뀌어도 1:1 대응은 변하지 않는다 -\u0026gt; 다음 단계에서도 1:1 대응 특성 유지 -\u0026gt; 따라서 귀납법에 의해 가로줄만을 사용하는 사다리들은 항상 1:1 대응된다.\n반복문 불변식 귀납법은 알고리즘의 정당성을 증명할 때 가장 유용하게 사용되는 기법이다.\n대부분의 알고리즘은 어떠한 형태로든 반복적인 요소를 가지고 있기 때문이다. 귀납법은 이런 알고리즘들이 옳은 답을 계산함을 보이기 위해서 알고리즘의 각 단계가 정답으로 가는 길 위에 있음을 보이고 결과적으로는 알고리즘의 답이 옳음을 보인다. 반복분 불변식 반복문의 내용이 한 번 실행될 때 마다 중간 결과가 우리가 원하는 답으로 가는 길 위에 잘 있는지를 명시하는 조건 반복문이 마지막에 정답을 계산하기 위해서는 항상 이 식이 변하지 않고 성립해야 한다. 불변식을 이용한 반복문의 정당성 증명 반복문 진입시에 분변식이 성립함을 보인다. 반복문 내용이 불변식을 깨뜨리지 않음을 보인다. 반복문 내용이 시작할 때 분변식이 성립했다면 내용이 끝날때도 불변식이 항상 성립함을 보인다. 반복문 종료시에 불변식이 성립하면 항상 우리가 정답을 구했음을 보인다. eg) while문\n1 2 3 4 5 6 # 불변식은 여기에서 성립해야 한다. while 어떤 조건: # 반복문 내용의 시작 ... # 반복문 내용의 끝 # 불변식은 여기에서도 성립해야 한다. 이진 탐색과 반복분 불변식 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 필수 조건: A는 오름차순으로 정렬되어 있다. # 결과: A[i-1] \u0026lt; x \u0026lt;= A[i]인 i를 반환한다. # 이 때 A[-1]=음의 무한대, A[N]=양의 무한대라고 가정한다. def binsearch(A, x): n = len(A) lo = -1 hi = n # 반복문 불변식 1: lo \u0026lt; hi # 반복문 불변식 2: A[lo] \u0026lt; x \u0026lt;= A[hi] while lo+1 \u0026lt; hi: mid = (lo+hi)//2 if A[mid] \u0026lt; x: lo = mid else: hi = mid return hi $lo+1=hi$ while 문이 종료된다. $lo+1 \\ge hi$ 불변식에 의하면 $lo \u0026lt; hi$이니, $lo+1 = hi$ $A[lo] \u0026lt; x \\le A[hi]$: 불변식 성립 초기 조건 while 문이 시작할 때 lo, hi는 초기값 -1, n으로 초기화된 상태 만약 n=0이라면 while문을 건너 뛰게 됨 -\u0026gt; 불변식 1 성립 $A[-1] = -\\infty, A[N]=\\infty$라고 가정 -\u0026gt; 불변식 2 성립 유지 조건 while 문 내부가 불변식을 깨뜨리지 않음 불변식 1 while문 내부로 들어옴 -\u0026gt; hi와 lo의 차이가 2 이상 -\u0026gt; mid는 항상 두 값 사이에 위치 -\u0026gt; mid를 lo,hi 어디에 대입해도 항상 불변식 1은 성립 불변식 2 $A[mid] \u0026lt; x$인 경우 반복문을 시작할 때 $x \\le A[hi]$임을 알고 있음 -\u0026gt; $A[mid] \u0026lt; x \\le A[hi]$이므로, lo에 mid를 대입해도 불변식 2는 성립 $x \\le A[mid]$인 경우 $A[lo] \u0026lt; x$와 합치면 -\u0026gt; $A[lo] \u0026lt; x \\le A[mid]$ -\u0026gt; hi에 mid를 대입해도 불변식 2는 성립 삽입 정렬과 반복문 불변식 구현 코드 1 2 3 4 5 6 7 8 9 10 def insertion_sort(A): for i in range(len(A)): # 불변식 a: A[0, ..., i-1]은 이미 정렬되어 있다. # A[0, ..., i-1]에 A[i]를 끼워 넣는다. j = i while j \u0026gt; 0 and A[j-1] \u0026gt; A[j]: # 불변식 b: A[j+1, ..., i]의 모든 원소는 A[j]보다 크다. # 불변식 c: A[0, ..., i]구간은 A[j]를 제외하면 정렬되어 있다. A[j], A[j-1] = A[j-1], A[j] j -= 1 초기 조건: 반복문이 시작할 때 i=0이면 해당 구간은 비어 있으니 항상 정렬되어 있다고 가정한다. 불변식 유지: for문의 내용이 종료할 때 이 불변식이 깨지지 않고 유지됨을 보이기 위해서는 while문의 정당성을 증명 eg) 이해를 위한 example\ni = 0\n불변식 a A[0, ... -1]은 정렬되어 있다. A[0, ... -1]에 A[0]를 끼어 넣는다. j = 0 while문 스킵 i = 1\n불변식 a A[0, ... 0]은 정렬되어 있다. A[0, ... 0]에 A[1]를 끼어 넣는다. j = 1 j \u0026gt; 0 and A[0] \u0026gt; A[1] A[0, 1] \u0026lt;- sorted i = 2\n불변식 a A[0, ... 1]은 정렬되어 있다. A[0, ... 1]에 A[2]를 끼어 넣는다. \u0026hellip; 초기 조건 $j=0$ $j=0$ 이라면 불변식 (b)에 의해 $A[j]$가 $A[0, \u0026hellip;, i]$ 구간 중 가장 작은 수가 된다. 불변식 (c)와 합쳐 보면 $A[0, \u0026hellip;, i]$ 구간 전체가 정렬되어 있다. $j\u0026gt;0$ $j\u0026gt;0$이고 $A[j-1] \\le A[j]$라면, 불변식 (b)와 합쳐 $A[j-1] \\le A[j] \u0026lt; A[j+1]$ 가 된다. 불변식 (c)와 합쳐 보면 $A[0, \u0026hellip;, i]$ 구간 전체가 정렬되어 있다. 불변식 유지 (b)와 (c)가 항상 성립함을 증명\n(b) 초기 조건: while 문 진입시에 $A[j+1, \u0026hellip;, i]$ 구간은 (빈 구간)(j+1==i)이므로 (b)는 참 (b) 유지 조건: while 문 내용이 실행되었다는 말은 $A[j-1]\u0026gt;A[j]$ 라는 의미. 이 둘을 교체하고 $j$를 1 줄이면 (b)는 여전히 참 (c) 초기 조건: 불변식 (a)에 의해 구간 $A[0, \u0026hellip;, i-1]]$은 항상 정렬되어 있으니 while문 진입 초기시에는 (c)는 항상 참 (c) 유지 조건: 그림 (c)에 $A[j]$ 와 이전 원소를 교체한다고 해도 회색 원소들 간의 상대적 순서는 변하지 않기에 (c)는 항상 참\n단정문을 이용해 반복문 불변식 강제하기 불변식을 주석으로만 달아두는 것이 아니라 단정문으로 강제 -\u0026gt; 불변식이 깨지면 프로그램이 종료되게해서 불변식이 깨졌음을 쉽게 알 수 있음\n5.3 귀류법 우리가 원하는 바와 반대되는 상황을 가정하고 논리를 전개해서 결론이 잘못 됏음을 찾아내는 증명 기법 어떤 선택이 항상 최선임을 증명하고자 할 때 많이 이용됨 -\u0026gt; 우리가 선택한 답보다 좋은 답이 있다고 가정한 후에, 사실 그럴일이 있을 수 없음을 보이면 우리가 최선의 답을 선택했음을 보임 책상 쌓기 $Q)$ 상장 형태로 된 책장을 여러 개 쌓아 올릴려고 한다.\n각 책장마다 버틸수 있는 최대 무게 $M_i$와 자신의 무게 $W_i$가 주어진다.\n이 때 책장을 가장 높이 쌓는다면 몇 개나 쌓을 수 있을까?\n단, $above(i)$가 $i$번 책상 위에 쌓인 모든 책장의 집합이라고 할 때, 다음이 성립해야 한다. $$\\sum_{j\\in above(i)}{w_j \\le M_i}$$\n-\u0026gt; 책상위에 올라간 다른 책장들의 무게의 합이 견딜 수 있는 최대 무게를 초과하면 안 된다.\n$A)$ what if? 항상 무거운 책장을 아래 쪽에 쌓는 것이 좋다는 사실을 알고 있다.\n-\u0026gt; 주어진 책상들을 정렬 후 순서에 신경 쓰지 않고 어느 책장을 고를지만 집중\n-\u0026gt; $M_i$로 정렬? $W_i$로 정렬?\n-\u0026gt; 정답은 $M_i + W_i$가 큰 것부터 아래에\n$Proof)$\n$M_i + W_i$가 더 큰 책장 A가 더 작은 책장 B에 올라간 형태\n-\u0026gt; A와 B의 위치를 항상 바꿀 수 있음을 증명\n$$M_A \u0026gt; M_B + W_B - W_A$$ A 위에 올라가 있는 상자들의 무게의 합 = $X$ $$M_B \u0026gt; M_A + X$$ =\u0026gt; $$M_A \u0026gt; M_B + W_B - W_A \\ge + X + W_B$$\nA도 B와 나머지 모든 상자를 지탱할 수 있음.\n따라서, 우리가 원하는 순서대로 쌓을 때 가장 높은 탑을 알지 못할 경우의 수는 존재하지 않는다.\n5.4 다른 기술들 비둘기 집의 원리 Pigeonhole Principla 10 마리의 비둘기가 9개의 비둘기 집에 들어 깟다면, 2 마리 이상이 들어간 비둘기 집은 반드시 하나 있기 마련이다. 동전 뒤집기 100개의 동전이 바닥에 깔려 있는데 이 중 $F$개는 앞면, $100-F$개는 뒷면이다.\n이 동전들이 모두 앞면으로 오게 하고 싶은데, 한 번 뒤집을 때 반드시 $X$개를 뒤집어야 한다.\n이 때 뒤집는 횟수를 최소화 하고 싶을 때 답의 상한은?\n=\u0026gt; 100\n순환 소수 찾기 분수 $\\frac{a}{b}$가 주어졌을 때 실수 연산을 사용하지 않고 이 분수를 소수 형태로 출력하려고 한다.\neg) $\\frac{3}{8}=0.375$, $\\frac{4912}{400}=11.78$\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 분수 a/b의 소수 표현을 출력한다. # a\u0026gt;=0, b\u0026gt;0 이라고 가정함 def print_demical(a, b): ret = \u0026#34;\u0026#34; i = 0 while a \u0026gt; 0: # 첫 번째와 두 번째 사이에 소수점을 찍는다. i += 1 if i == 1: ret += \u0026#34;.\u0026#34; ret += str(a//b) a = (a % b) * 10 print(ret) if) $\\frac{1}{11}$이 입력 -\u0026gt; $0.090909\u0026hellip;$ 무한 소수\na%b의 결과는 언제나 [0, b-1] 범위의 값을 가정한다. while문이 b+1번 반복될 때까지 함수가 종료되지 않음 -\u0026gt; a%b의 결과는 b가지의 결과를 가질 수 있음 -\u0026gt; 결과가 중복되는 경우가 반드시 있음 구성적 증명 Constructive Prrof 우리가 원하는 어떤 답이 존재한다는 사실을 증명하기 위해 사용 답이 존재한다는 사실을 노증하는 것(귀납법, 귀류법) \u0026lt;=\u0026gt; 답의 실제 예를 들거나 만드는 방법을 제시하는 증명(구성적 증명) 안정적 결혼 문제 문제 풀이 알고리즘\n처음에는 여성들이 모두 자신이 가장 선호하는 남성의 앞에 가서 프러포즈를 한다. 남성이 그 중 제일 마음에 드는 여성을 고르면 나머지는 제자리로 돌아간다. 제자리도 돌아간 여성들이 (상대에게 짝이 있던 없던 관계없이) 다음으로 마음에 드는 남성에게 프러포즈한다. 남성은 현재 자기 짝보다 마음에 드는 여성이 다가오면, 현재 짝을 돌려 보낸다. 더 프러포즈를 할 여성이 없을 때까지 2를 반복한다. 증명\n종료 증명 각 여성은 돌아갈 때 마다 지금까지 프러포즈했던 남성들보다 우선 순위가 낮은 남성에게 프러포즈한다. 따라서 각 여성이 최대 $n$명의 남성들에게 순서대로 프러포즈한 이후에는 더 이상 프러포즈를 할 남성이 없으므로, 이 과정을 언젠간 반드시 종료한다. 모든 사람이 짝을 찾는지 증명 프러포즈를 받은 남성은 그 중 한 사람을 반드시 선택하고, 더 우선순위가 높은 여성이 프러포즈해야만 짝을 바꾸므로 한 번이라도 프러포즈를 받은 남성은 항상 짝이 있다. 귀류법을 적용 남녀 한 사람씩 짝을 못 찾음 여성은 우선순위가 높은 순서대로 모두에게 프러포즈하기 때문에 이 남성에게 프러포즈 남성은 프러포즈를 받아 들여야 함 짝을 찾지 못하는 사람은 있을 수 없음 짝들의 안정성 귀류법 짝이 아닌 두 남녀가 서로 자신의 짝보다 상대방을 더 선호한다고 가정 여성은 지금 자신의 짝 이전에 그 남성에게 반드시 프러포즈 했어야 함 그런데도 이 남성이 이 여성과 짝 지어지지 않았따는 것은 더 마음에 드는 여성에게 프러포즈 받아서 수락함 프러포즈 받았떤 여성보다 맘에 들지 않은 여성과 최종적으로 짝이 되는일은 없음 ","date":"2021-10-22T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/5.-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%98-%EC%A0%95%EB%8B%B9%EC%84%B1-%EC%A6%9D%EB%AA%85/","title":"5. 알고리즘의 정당성 증명"},{"content":"알고리즘 문제 해결 전략을 읽고 요약했습니다.\n4. 알고리즘의 시간 복잡도 분석 4.1 도입 알고리즘의 속도는 어떻게 측정할 수 있을까? 프로그램을 구현한 뒤 같은 입력에 대해 프로그래밍의 수행시간을 비교\n-\u0026gt; 부적합\n사용 프로그래밍 언엉, 하드웨어, 운영체제, 컴파일러등 수 많은 요소에 의해 변할 수 있음 프로그램의 실제 수행 시간이 다양한 입력에 대한 실험 시간을 반영하지 못함 입력의 크기나 특성에 따라 수행 시간이 달라질 수 있다. 반복문이 지배한다 지배한다 (Dominate) 한가지 항목이 전체의 대소를 좌지우지 하는 것 알고리즘의 시간을 지배하는 것 -\u0026gt; \u0026ldquo;반복문\u0026rdquo; 입력의 크기가 작을 때는 반복 외의 다른 부분들이 갖는 비중이 클 수 있지만, 입력의 크기가 커지면 커질수록 반복문이 알고리즘의 수행시간을 지배한다. 반복문의 수행 횟수는 입력의 크기에 대한 함수로 표현 eg) 주어진 배열에서 가장 많이 등장하는 숫자를 반환하기 1 2 3 4 5 6 7 8 9 10 11 12 13 def majority_1(A): n = len(A) majority = -1 majority_count = 0 for i in A: count = 0 for j in A: if j == i: count += 1 if count \u0026gt; majority_count: majority_count = count majority = i return majority 알고리즘의 수행시간은 입력 크기 N에 따라 변한다.\nN 번 수행되는 반복문이 두 개 겹쳐 있으므로, 반복문의 가장 안쪽은 항상 $N^2$번 실행\n-\u0026gt; 알고리즘의 수행시간은 $N^2$\nif 입력으로 주어지는 숫자들이 100점 만점의 수학 점수 1 2 3 4 5 6 7 8 9 10 def majority_2(A): n = len(A) count = [0] * 101 for i in A: count[i] += 1 majority = 0 for i in range(1, 101): if count[i] \u0026gt; count[majority]: majority = i return majority 하나는 N번, 하나는 100번\n-\u0026gt; 전체 반복문 수행 횟수 N + 100\nN이 커질수록 후자의 방복문의 비중은 감소한다.\n-\u0026gt; 알고리즘 수행시간: $N$\n4.2 선형 시간 알고리듬 다이어트 현황 파악: 이동 평균 계산하기 이동 평균(Moving Average) 시간에 따라 변화하는 값들을 관찰할 때 유용하게 사용할 수 있는 통계적 기준 eg) $N$개의 측정치가 주어질 때, M 간의 이동평균 계산 1 2 3 4 5 6 7 8 9 def moving_average(A, M): ret = [] N = len(A) for i in range(M-1, N): partial_sum = 0 for j in range(M): partial_sum += A[i-j] ret.append(partial_sum / M) return ret 2개의 for loop\n$j$를 사용하는 반복문: $N$ $i$를 사용하는 반복문: $N-M+1$ -\u0026gt; $N * (N-M+1)=N^2-NM+N$\n중복된 계산을 없애기\n$M-1$일과 $M$일을 비교 0과 M일만 차이나고 모두 동일하다 $M-1$일에서 구한 몸무게 합에서 0일을 버리고 M일을 더하면? 1 2 3 4 5 6 7 8 9 10 11 def moving_average(A, M): ret = [] N = len(A) partial_sum = 0 for i in range(M-1, N): parital_sum += A[i] for j in range(M): partial_sum += A[i] ret.append(partial_sum / M) partial_sum -= A[i-M+1] return ret 하나로 묶여 있던 두 개의 반복문이 분리\n-\u0026gt; $M-1 +(N-M+1) = N$ 코드의 수행시간은 $N$에 정비례 N이 2배가 되면 실행도 2배 걸리고, 반으로 줄어들면 수행시간도 반으로 줄어든다.\n입력의 크기에 대비해 걸리는 시간을 그래프로 그러벼몬 정확히 직선이다\n-\u0026gt; 선형 시간(Linear Time) 알고리즘\n4.3 선형 이하 시간 알고리듬 어떤 문제건 입력된 자료를 모두 한 번 훑어 보는 데에는 입력의 크기에 비례하는 시간, 즉 선형시간이 걸린다. 선형 시간보다 빠르게 동작하는 알고리즘은 입력된 값도 다 보지 않는다는 뜻. 입력으로 주어진 자료에 대해 우리가 알고 있는 지식을 활용하면 가능하다! 이진 탐색 $binsearch(A[],x)$ 오름차순으로 정려된 $A[]$와 찾고 싶은 값 $x$가 주어질 때 $A[i-1] \u0026lt; x \\le A[i]$인 $i$를 반환 이 때 $A[-1] = - \\infty, A[N] = \\infty$로 가정한다. 배열 $A[]$에서 $x$를 삽입할 수 있는 위치 중 가장 앞에 있는 것을 반환한다. 4.4 지수 시간 알고리듬 다항시간 알고리듬 반복문의 수행 횟수를 입력 크기의 다항식으로 표현할 수 있는 알고리즘 eg) 알러지가 심한 친구들 집들이에 $N$명의 친구를 초대 / 할 줄 아는 $M$가지 음식 중 무엇을 대접해야 할까? 모든 답 후보 평가하기 이 문제는 여러 개의 답이 있을 수 있다. 만들 수 있는 모든 음식을 다 만들면 된다 -\u0026gt; 더 적은 종류의 음식만을 준비하고 싶다. 여러개의 답이 있고 그 중 가장 좋은 답을 찾을 때, 가장 간단한 방법은 모든 답을 일일이 고려해 보는 것 1 2 3 4 5 6 7 8 9 10 11 12 INF = 987654321 def can_everybody_eat(menu): def select_menu(menu, food): if food == M: if can_everybody_eat(menu): return len(menu) return INF ret = select_menu(menu, food+1) menu.append(food) ret = min(ret, select_menu(menu, food+1)) menu.pop() return ret 지수시간 알고리듬 N이 하나 증가할 때 마다 걸리는 시간이 배로 증가하는 알고리즘은 지수시간(exponential time)에 동작한다고 말한다.\n4.5 시간 복잡도 시간 복잡도 시간 복잡도(Tiem Complexity)란 가장 널리 사용되는 알고리즘의 수행시간 기준\n알고리즘에 실행되는 동안 수행하는 기본적인 연산의 수를 입력의 크기에 대한 함수로 표현한 것\n기본적인 연산이란?\n더 적게 쪼개질 없는 최소 크기의 연산 두 부호있는 32비트 정수의 사칙연산 두 실수형 변수으 ㅣ대소 비교 한 변수에 다른 변수 대입하기 쪼갤수 있는 연산 정수 배열 정리하기 두 문자열에 서로 같은지 확인하기 입력된 두 소인수 분해하기 시간 복잡도가 높다 = 입력의 크기가 증가할 때 알고리즘의 수행시간이 더 빠르게 증가\n-\u0026gt; 시간 복잡도가 낮다고 해서 언제나 더 빠르게 증가하는 것은 아님!\n입력의 종류에 따른 수행시간의 변화 입력이 어떤 형태로 구성되어 있는지도 수행시간에 영향 eg) 선형 탐색 운 좋게 처음에 찾을 수 있고/ 운 나쁘면 마지막에 찾을 수 도 있음 최선의 수행시간: 찾으려는 원소가 맨 앞에 있음 반목문의 수행 횟수: $1$ 최악의 수행시간: 배열에 해당 원소가 없을 때 알고리즘은 $N$번 반복하고 종료 반목문의 수행 횟수: $N$ 평균적인 경우의 수행시간: 평균적인 경우의 수행시간을 분석하기 위해서는 존재할 수 있는 모든 입력의 등장 확률이 모두 같다고 가정 만약 주어진 배열이 항상 원소를 포함한다고 가정하면 반환 값의 기대값은 $2 \\over N$ 평균적인 수행 횟수: $2 \\over N$ 대게 사용하는 것은 최악의 수행시간 혹은 수행시간의 기대치 많은 경우 이 두 기준은 따로 구분하지 않고 쓰인다 여러 알고리즘에서 이 두 기준은 거의 같음 점근적 시간 표기: $O$ 표기 대문자 $O$ 표기 (Big-O Notation) 주어진 함수에서 가장 빨리 증가하는 항만을 남긴채 나머지를 다 버리는 표기법 $O$ 표기법 의미 $O$ 표기법은 대략적으로 함수의 상한을 나타낸다. $N$에 대한 함수 $f(N)$에 주어질 때, $f(N)=O(G(N))$이라고 쓰는 것은 아주 큰 $N_0$와 $C(N_0,C\u0026gt;0)$를 적절히 선택하면 $N_0 \\lt N)인 모든 $N$에 대해\n$\\left| f(N) \\right| \\le C*\\left| g(N) \\right| $ 이 참이 되도록 할 수 있다. ","date":"2021-10-19T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/4.-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%98-%EC%8B%9C%EA%B0%84-%EB%B3%B5%EC%9E%A1%EB%8F%84-%EB%B6%84%EC%84%9D/","title":"4. 알고리즘의 시간 복잡도 분석"},{"content":"On Orcale Cloud 이제 네트워크에 ingress 를 추가해주어야 합니다.\nInstance 메뉴를 선택합니다.\nInstance를 선택합니다.\nSubnet을 선택합니다.\nSecurity를 선택합니다. Add Ingress Rules를 눌러 WireGuard를 위한 포트를 추가하겠습니다. 다음 빨간 칸의 내용들을 입력한 후 추가합니다. Prxoy-Manager Proxy Hosts를 추가합니다. On Ubuntu Server Wireguard와 이를 쉽게 사용할 수 있는 Subspace를 실행합니다.\n우선 아래 과정을 통해 Wireguard를 설치합니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 sudo su apt-get update apt-get install -y wireguard # Remove dnsmasq because it will run inside the container. apt-get remove -y dnsmasq # Disable systemd-resolved listener if it blocks port 53. echo \u0026#34;DNSStubListener=no\u0026#34; \u0026gt;\u0026gt; /etc/systemd/resolved.conf systemctl restart systemd-resolved # Set Cloudfare DNS server. echo nameserver 1.1.1.1 \u0026gt; /etc/resolv.conf echo nameserver 1.0.0.1 \u0026gt;\u0026gt; /etc/resolv.conf # Load modules. modprobe wireguard modprobe iptable_nat modprobe ip6table_nat # Enable modules when rebooting. echo \u0026#34;wireguard\u0026#34; \u0026gt; /etc/modules-load.d/wireguard.conf echo \u0026#34;iptable_nat\u0026#34; \u0026gt; /etc/modules-load.d/iptable_nat.conf echo \u0026#34;ip6table_nat\u0026#34; \u0026gt; /etc/modules-load.d/ip6table_nat.conf # Check if systemd-modules-load service is active. systemctl status systemd-modules-load.service # Enable IP forwarding. sysctl -w net.ipv4.ip_forward=1 sysctl -w net.ipv6.conf.all.forwarding=1 exit docker-compose를 위한 폴더를 생성합니다. 1 mkdir subspace; cd subspace 다음과 같은 docker-compose 파일을 작성합니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 cat \u0026gt; docker-compose.yml \u0026lt;\u0026lt; EOF version: \u0026#34;3.3\u0026#34; services: subspace: image: subspacecommunity/subspace:latest container_name: subspace volumes: - /opt/docker/subspace:/data restart: always environment: - SUBSPACE_HTTP_HOST=vpn.example.duckdns.org # 설정한 proxy-host의 이름과 동일해야 합니다. - SUBSPACE_LETSENCRYPT=false - SUBSPACE_HTTP_INSECURE=true - SUBSPACE_HTTP_ADDR=\u0026#34;:5000\u0026#34; - SUBSPACE_NAMESERVERS=1.1.1.1,8.8.8.8 - SUBSPACE_LISTENPORT=51820 - SUBSPACE_IPV4_POOL=10.99.97.0/24 - SUBSPACE_IPV6_POOL=fd00::10:97:0/64 - SUBSPACE_IPV4_GW=10.99.97.1 - SUBSPACE_IPV6_GW=fd00::10:97:1 - SUBSPACE_IPV6_NAT_ENABLED=1 - SUBSPACE_DNSMASQ_DISABLED=1 cap_add: - NET_ADMIN network_mode: \u0026#34;host\u0026#34; EOF docker-compose를 실행합니다 1 docker-compose up -d VPN Configure 추가 설정한 vpn domain으로 접속후 계정을 생성합니다. configure를 발급받습니다. config 파일을 다운로드 받습니다. config 파일의 내용은 다음과 같습니다. 1 2 3 4 5 6 7 8 9 10 [Interface] PrivateKey = ~~~ DNS = 10.99.97.1, fd00::10:97:1 Address = 10.99.97.2/24,fd00::10:97:2/64 [Peer] PublicKey = ~~~ Endpoint = vpn.aiden-jeon.duckdns.org:51820 AllowedIPs = 0.0.0.0 이 때 AllowedIPs를 VPN IP의 0번으로 수정합니다. 제 경우 10.99.97.x 이기 때문에 10.99.97.0/24를 입력합니다. 수정된 파일은 다음과 같습니다. 1 2 3 4 5 6 7 8 9 10 [Interface] PrivateKey = ~~~ DNS = 10.99.97.1, fd00::10:97:1 Address = 10.99.97.2/24,fd00::10:97:2/64 [Peer] PublicKey = ~~~ Endpoint = vpn.aiden-jeon.duckdns.org:51820 AllowedIPs = 10.99.97.0/24 vpn에 추가할 서버에 접속후 wireguard를 설치합니다. 1 sudo apt-get install -y resolvconf wireguard 발급 받은 config 파일을 다음 위치에 적어줍니다. 1 sudo cp wg0.conf /etc/wireguard/wg0.conf systemctl을 설정합니다. 1 2 3 sudo systemctl enable wg-quick@wg0 sudo systemctl start wg-quick@wg0 sudo systemctl status wg-quick@wg0 인터페이스를 확인하고 서버로 ping을 날려봅니다. 1 2 ifconfig w0 ping 10.99.97.1 ","date":"2021-10-15T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/wireguard-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/","title":"Wireguard 설치하기"},{"content":"이번 포스트에서는 생성한 인스턴스를 nginx proxy로 관리하는 방법에 대해서 설명합니다.\nUbuntu Instance 기본 설정 1. 기본 패키지 업데이트 1 sudo apt update \u0026amp;\u0026amp; sudo apt -y upgrade 2. Swap Memory 생성 Free tier의 메모리는 1G 이므로 스왑 메모리를 생성하도록 하겠습니다.\n1 2 3 sudo fallocate -l 2G /swapfile sudo chmod 600 /swapfile sudo mkswap /swapfile 생성된 스왑 메모리를 확인합니다.\n1 free -h 재부팅해도 스왑 메모리가 유지되도록 수정합니다.\n1 sudo vim /etc/fstab 아래 내용을 추가합니다.\n1 /swapfile swap swap defaults 0 0 추가한 후의 모습은 다음과 같습니다.\n1 2 3 LABEL=cloudimg-rootfs / ext4 defaults 0 0 LABEL=UEFI /boot/efi vfat defaults 0 0 /swapfile swap swap defaults 0 0 3. 시간 설정 1 sudo timedatectl set-timezone Asia/Seoul Nginx Proxy Manager 1. iptable 초기화 oracle에서 기본적으로 설치한 iptable 규칙을 초기화합니다.\n1 2 3 4 sudo iptables -F sudo iptables -X sudo netfilter-persistent save sudo netfilter-persistent reload 2. docker 설치 공식 홈페이지를 참고해 docker를 설치합니다.\nSetup repository 1 2 3 4 5 6 7 sudo apt-get update sudo apt-get install \\ apt-transport-https \\ ca-certificates \\ curl \\ gnupg \\ lsb-release Add Docker\u0026rsquo;s official GPG key 1 curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg 1 2 3 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null Install docker 1 2 sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io 실행 확인 1 docker ps 실행이 안될 경우 포스트를 참고하세요. 3. docker-compose 설치 공식 홈페이지를 참고해 docker-compose를 설치합니다.\nDownload Docker Compose 1 sudo curl -L \u0026#34;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose Apply permission 1 sudo chmod +x /usr/local/bin/docker-compose Add Path 1 sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose Test 1 docker-compose --version 4. docker-compose.yaml 작성 폴더 생성 1 mkdir ~/npm; cd ~/npm docker-compose.yaml 작성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 cat \u0026gt; docker-compose.yml \u0026lt;\u0026lt; EOF version: \u0026#34;3\u0026#34; services: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; restart: always network_mode: \u0026#34;host\u0026#34; environment: # These are the settings to access your db DB_MYSQL_HOST: localhost DB_MYSQL_PORT: 3306 DB_MYSQL_USER: \u0026#34;npm\u0026#34; DB_MYSQL_PASSWORD: \u0026#34;npm\u0026#34; DB_MYSQL_NAME: \u0026#34;npm\u0026#34; # If you would rather use Sqlite uncomment this # and remove all DB_MYSQL_* lines above # DB_SQLITE_FILE: \u0026#34;/data/database.sqlite\u0026#34; # Uncomment this if IPv6 is not enabled on your host # DISABLE_IPV6: \u0026#39;true\u0026#39; volumes: - /home/ubuntu/npm/data:/data - /home/ubuntu/npm/letsencrypt:/etc/letsencrypt depends_on: - db db: image: \u0026#39;jc21/mariadb-aria:latest\u0026#39; restart: always network_mode: \u0026#34;host\u0026#34; environment: MYSQL_ROOT_PASSWORD: \u0026#39;npm\u0026#39; MYSQL_DATABASE: \u0026#39;npm\u0026#39; MYSQL_USER: \u0026#39;npm\u0026#39; MYSQL_PASSWORD: \u0026#39;npm\u0026#39; volumes: - /home/ubuntu/npm/mysql:/var/lib/mysql EOF docker-compose 실행 1 docker-compose up -d 실행 확인 1 docker-compose logs 로그의 제일 마지막에 다음과 같은 출력이 있다면 정상적으로 실행된 것입니다. 1 2 3 4 5 app_1 | [10/13/2021] [9:53:02 AM] [SSL ] › ℹ info Renewing SSL certs close to expiry... app_1 | [10/13/2021] [9:53:02 AM] [IP Ranges] › ℹ info IP Ranges Renewal Timer initialized app_1 | [10/13/2021] [9:53:02 AM] [Global ] › ℹ info Backend PID 246 listening on port 3000 ... app_1 | [10/13/2021] [9:53:03 AM] [Nginx ] › ℹ info Reloading Nginx app_1 | [10/13/2021] [9:53:03 AM] [SSL ] › ℹ info Renew Complete 5. DNS 설정 무료로 이용할 수 있는 duckdns를 사용해 DNS를 연결하겠습니다.\n로그인 한 뒤 사용할 dns를 추가합니다. oracle에서 받은 public ip를 입력합니다. 6. Nginx Proxy 연결 http://\u0026lt;Public IP\u0026gt;:81로 접속합니다.\n다음과 같이 화면이 출력됩니다. 기본 아이디와 비밀번호는 다음과 같습니다.\nid: admin@example.com pw: changeme 접속 후 아이디와 비밀번호를 바꿔줍니다.\n7. Nginx DNS 연결 Proxy Hosts를 선택합니다. Add Proxy Host를 선택합니다. Nginx Proxy Host를 접속한 dns를 입력해줍니다. 저는 npm을 subdomain으로 생성했습니다. 설정한 dns로 접속되는지 확인합니다. https 접속을 위한 ssl를 설정해주어야 합니다. 다음과 같은 내용으로 작성합니다.\noracle cloud에서 81번 포트를 삭제합니다. ","date":"2021-10-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/nginx-proxy-manager-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/","title":"Nginx Proxy Manager 설정하기"},{"content":"이번 포스트에서는 할당 받은 Public IP를 사용하는 방법에 대해서 설명합니다.\n1. 인스턴스 생성하기 Instances 메뉴를 선택합니다.\nCreate Instance를 선택합니다. Instances 이름을 입력합니다. 이미지를 변경합니다. 사용할 이미지는 Canonical Ubuntu 입니다. 저는 ssh key를 자체적으로 발급받고 사용하려고 합니다. 이를 위해서 Private Key를 저장합니다. 볼륨은 따로 건들지 않겠습니다. 2. Public IP 할당하기 생성된 인스턴스 화면에서 Attached VNCs 메뉴를 선택합니다. 생성한 instance 이름을 선택합니다. IPv4 Addrresses 를 선택합니다. \u0026hellip;을 선택합니다. Edit을 선택합니다. No Pulic IP를 선택하고 Update합니다. 다시 edit을 누릅니다. Reserved public IP를 선택하고 생성된 IP를 선택한뒤 Update합니다. 다음과 같이 IP가 할당된 것을 확인합니다. 3. 접속하기 이미지를 생성할 때 다운로드 받은 key를 ~/.ssh로 옮깁니다. 1 2 mv ssh-key-2021-10-13.key ~/.ssh/oracle.key cd ~/.ssh 400 권한을 줍니다. 1 chmod 400 oracle.key ssh를 이용해 접속합니다. 1 ssh ubuntu@\u0026lt;PUBLIC IP\u0026gt; -i ~/.ssh/oracle.key ","date":"2021-10-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/oracle-cloud-public-ip-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/","title":"Oracle Cloud Public IP 사용하기"},{"content":"이번 포스트에서는 Oracle Cloud 무료 버전을 이용해 Public IP를 할당 받는 방법에 대해서 설명합니다.\n1. 구획 생성하기 Compartments 메뉴를 선택합니다. Create Compartment를 누른 후 사용할 이름과 설명을 적어줍니다 2. Networking Virtual Cloud Networks 메뉴를 선택합니다. Compartment에서 생성한 구획을 선택하고 Start VCN Wizard 버튼을 눌러줍니다. 다음 항목들을 클릭하고 넘어갑니다. vcn 이름을 작성하고 다음으로 넘어갑니다. 생성을 누릅니다.$$ 생성이 완료되면 다음과 같이 나옵니다. View Virtual Cloud Network를 선택합니다. 구획 이름과 vcn 이름을 확인합니다. 3. Security 3.1 http, https security list를 선택하고 Default Security List를 선택합니다. Add Ingress Rules를 선택합니다. 다음과 같이 http와 https를 위한 포트를 추가합니다. 다음과 같이 룰이 추가된 것을 확인합니다. 3.2 nginx proxy 다음과 같이 nginx proxy를 위한 81 포트를 추가합니다. ngix설정 후 해당 포트는 삭제합니다. 다음과 같이 룰이 추가된 것을 확인합니다. 4. Public IP IP Management 메뉴를 선택합니다. Public 구획에서 Reserve Public IP Address를 선택합니다. ip 이름을 설정한 후 Reserve Public IP Address를 선택합니다.\n다음과 같이 IP가 추가된 것을 확인합니다. ","date":"2021-10-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/oracle-cloud-public-ip-%ED%95%A0%EB%8B%B9-%EB%B0%9B%EA%B8%B0/","title":"Oracle Cloud Public IP 할당 받기"},{"content":"알고리즘 문제 해결 전략을 읽고 요약했습니다.\n3. 코딩과 디버깅에 관하여 3.1 도입: 코딩의 중요성을 간과하지 말라 좋은 성적을 올리기 위한 비결은 당장 빨리 코드를 작성하기 보다 읽기 쉬운 코드를 작성하는 것\n복잡하고 읽기 어려운 코드 디버깅이 어렵다. 한 번에 정확하게 작성하기 어렵다. 3.2 좋은 코드를 짜기 위한 원칙 1) 간결한 코드를 작성하기 코드가 짧으면 짧을 수록 오타나 단순한 버그가 사라질 우려가 줄어든다. 디버깅이 쉬워진다. 프로그래밍과 다른 환경을 이용한 Trick 전역 변수 사용하기 2) 적극적으로 코드 재사용하기 간결한 코드를 작성하기 위한 가장 직접적인 방법이다. 같은 코드가 3번 이상 등장한다면 해당 코드를 함수로 분리해 재사용 실무에서는 1 함수 1 기능의 원칙이 있지만 대회에서는\u0026hellip; 시간안에 문제를 풀 수가 없다. 대회에서 작성한 코드는 재사용되지 않는다. 3) 표준 라이브러리 공부하기 기본적으로 제공하는 알고리즘, 자료 구조는 직접 구현하지 않는다. 큐, 스택과 같은 자료구조 정렬등의 기초 알고리즘 4) 항상 같은 형태로 프로그램을 작성하기 자주 작성하는 알고리즘이나 코드에 대해서는 한 번 검증된 코드를 작성, 이것만 꾸준히 사용한다. 도구가 아닌 문제에 집중할 수 있다. 5) 일관적이고 명료한 명명법 사용하기 네이밍 컨벤션 잘 지키기 함수의 반환값이 들어나게 이름 짓기 judge(...) -\u0026gt; isInsideCircle(...) 6) 모든 자료를 정규화해서 저장하기 같은 자료를 두 가지 형태로 저장하지 않는다. eg 1) 기약 분수 $\\frac{9}{6}$ -\u0026gt; $\\frac{3}{2}$ eg 2) 두 점사이의 각도 $-30$, $330$, $690$ 정규화는 프로그램이 자료를 입력 받거나 계산하자 마자 곧바로 이루어져야 한다. 자료를 표현하는 클래스의 생성자에서 수행 외부에서 자료를 입력 받자마자 수행 7) 코드와 데이터를 분리하기 코드의 논리와 상관없는 코드는 부리하기 eg) 월을 출력하는 함수 1 2 3 4 5 6 7 8 def get_month_name(month: int): if month == 1: return \u0026#34;January\u0026#34; elif month == 3: return \u0026#34;Feburary\u0026#34; ... else: return \u0026#34;December\u0026#34; =\u0026gt; 다음과 같이 수정하기 1 month_name = [\u0026#34;January\u0026#34;, \u0026#34;Feburary\u0026#34;, ... , \u0026#34;Decebmer\u0026#34;] 체스 게임에서 말들이 움직일 수 있는 상대적인 위치를 저장 1 2 knight_dx = [2, 2, -2, -2, 1, 1, -1, -1] knight_dx = [1, -1, 1, -1, 2, -2, 2, -2] 3.3 자주 하는 실수 1) 산술 오버플로 계산 과정에서 변수의 표현 범위를 벗어나는 값을 사용하는 산술 오버플로\n2) 배열 범위 밖 원소 접근 0을 시작으로하는 범위와 1로 시작하는 범위를 혼동하는 것\neg) 1년에 포함된 달의 날짜 수를 저장하는 상수 배열\n1년 12달 -\u0026gt; 배열의 크기: 12 입력 받은 값을 곧장 배열 인덱스로 사용할 경우 3) 일관되지 않은 범위 표현 방식 사용하기 프로그램 내에서 여러 가지의 범위 표현 방식을 섞어서 사용할 경우 eg) $[2, 3, 4, \u0026hellip; , 12]$ 닫힌 구간: $[2, 12]$ 열린 구간: $(1, 13)$ 반 열린 구간: $[2, 13)$ 반 열린 구간의 장점 첫 번째 값과 마지막 값이 같은 구간을 이용하면 텅 빈 구간을 쉽게 표현할 수 있다. eg) $[2, 2)$ -\u0026gt; $2 \\leq i \u0026lt; 2$ -\u0026gt; 공집합 두 구간이 연속해 있는지 쉽게 알 수 있다. eg) $[a, b)$ , $[c, d)$가 연속인지 보려면?\n-\u0026gt; $b = c$, $a = d$ 인지만 확인하면 된다. 구간의 크기를 알기 쉽다. eg) $[a, b)$ -\u0026gt; 자연수의 수는 $b-a$ 자연어에서 사용하는 범위와 다르기 때문에 문제가 생길 수 도 있다. average(A[], i, j): 배열 A[]의 부분 구간의 평균을 구하는 함수 반 열린 구간: average(a, 0, n) 닫힌 구간: average(a, 0, n-1)\n-\u0026gt; 함수 내에서 사용하는 표현 방법과 함수 밖에서 사용하는 표현 방법이 다르면 혼란이 발생. 4) Off-by-one 오류 계산의 큰 줄기는 맞지만 하나가 모자르거나 하나가 많아서 틀리는 오류들 eg 1) 100미터 길이의 담장에 10미터 간격으로 울타리 기둥을 세운다. 기둥이 몇 개 필요할까? -\u0026gt; 정답은 10개가 아니라 11개 eg 2) 정수 배열 A[]가 주어질 때 A[i] 부터 A[j] 까지의 평균을 계산한다. 이 때 합을 얼마로 나누어야 할까? -\u0026gt; $j-i$가 아니라 $j-i+1$ 언제 일어날까? 반복문에서 $\u0026lt;, \u0026gt;$ 연산자와 $\\leq, \\geq$ 연산자를 혼동하여 원서를 하나 더 적게, 혹은 하나 더 많이 분리하는 경우 반 열린 구간과 닫힌 구간을 서로 혼용해 쓸 경우 오류를 방지하는 방법 최소 입력이 주어졌을 때 코드가 어떻게 동작할지 되새겨 보기 eg 1) 담장의 길이가 0m 여도 기둥은 하나 박아야 한다. eg 2) A[1] 부터 A[1] 까지의 평균을 구할 때 0이 아니라 1로 나누어야 한다. 5) 컴파일러가 잡아주지 못하는 상수 오타 6) 스택 오버 플로 프로그램 실행 중 콜 스택이 오버플로해서 프로그램이 강제 종료되는 경우\n재귀 호출의 깊이가 너무 깊어져서 온다. 7) 다차원 배열 인덱스 순서 바꿔 쓰기 8) 잘못된 비교 함수 작성 9) 최소, 최대 예외 잘못 다루기 가능한 입력 중 최솟 값과 최대 값이 예외가 되는 문제는 생각보다 많다.\neg) 소수 판정 함수\n1 2 3 4 5 6 7 def is_prime(n: int): if n % 2 == 0: # 짝수면 소수다. return False for i in range(2, n): # 다른 수로 나눠지면 소수다. if n % i == 0: return False return True -\u0026gt; 2는 짝수이면서 소수\n1 2 3 4 5 6 7 8 9 def is_prime(n: int): if n == 2: return True if n % 2 == 0: # 짝수면 소수다. return False for i in range(2, n): # 다른 수로 나눠지면 소수다. if n % i == 0: return False return True -\u0026gt; n=1은 두 조건문을 통과해 소수로 판정됨\n10) 연산자 우선 순위 잘못 쓰기 연산자의 우선순위 잘 기억하기 헷갈릴 경우 괄호로 적절히 감싸기 11) 너무 느린 입출력 방식 선택 12) 변수 초기화 문제 이전 입력에서 사용한 전역 변수 값을 초기화 하지 않고 그대로 사용하는 경우 Tip) 예제 입력 파일을 두 번 반복해서 쓰기 eg) 1 2 3 2 1234 321 -\u0026gt; 1 2 3 4 5 4 1234 321 1234 321 예제간의 의존 관계 때문에 우연히 답이 나오는 경우를 방지할 수 있다. 3.4 디버깅과 테스팅 1) 디버깅에 관하여 작은 입력에 대해 제대로 실행되나 확인하기 단정문(assertion)을 쓴다. 프로그램의 계산 중간 결과를 출력한다. 2) 테스팅에 관하여 스캐폴딩 (Scaffolding) 코드의 정당성을 확인하거나 반례를 찾을 때 유용하다. 임의의 작은 입력을 자동으로 생성하는 프로그램 -\u0026gt; 검증하는 프로그램 eg) 직접 짠 정렬 알고리즘 난수 생성 라이브러리 결과 == 직접 짠 결과 라이브러리가 없는 경우 작은 입력에서만 동작하는 더 느리지만 단순한 알고리즘을 사용해 검증 3.5 변수 범위의 이해 1) 산술 오버플로 2) 너무 큰 결과 3) 너무 큰 중간값 4) 너무 큰 \u0026lsquo;무한대\u0026rsquo; 값 무한대에 해당하는 큰 값을 이용하는 것이 편리할 때가 있다.\neg) 두 위치를 잇는 가장 짧은 길의 길이를 구하기\n$s$ 에서 $t$ 까지 가는데 세 중간 지점 $a, b, c$ 중 항상 한 군데를 거쳐서 가야 한다.\n$(s,t)$ 구간의 최단 경로 길이 $dist(s,t)$ 계산 방법\n\\( dist(s,t)=min \\begin{cases} dist(s,a) + dist(a,t) \\\\ dist(s,b) + dist(b,t) \\\\ dist(s,c) + dist(c,t) \\\\ \\end{cases} \\) 만약 두 구간을 잇는 경우가 없을 경우에는?\n길이 존재하지 않음을 나타내는 특수한 값을 사용 이 값에 대한 예외 처리를 또 해주어야 함 실제로 나타날 리 없는 아주 큰 값을 반환 $min$ 에서 자동으로 필터링 Tip) 987,654,321 같이 디버깅이 쉬운 큰 값을 추천 $2^30$에 가까운 매우 큰 값 5) 오버플로 피해가기 더 큰 자료형 사용하기 오버플로가 일어나지 않도록 연산의 순서 바꾸기 계산 방법을 다르게 해 오버플로 피해가기 6) 자료형의 프로모션 ","date":"2021-10-09T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/3.-%EC%BD%94%EB%94%A9%EA%B3%BC-%EB%94%94%EB%B2%84%EA%B9%85%EC%97%90-%EA%B4%80%ED%95%98%EC%97%AC/","title":"3. 코딩과 디버깅에 관하여"},{"content":"Kubeflow 설치 중 auth의 dex pod이 동작하지 않는 것을 확인했습니다. 우선, auth namespace를 확인해봤습니다.\n1 kubectl get all -n auth 다음과 같은 결과를 얻을 수 있었습니다.\n1 2 3 4 5 6 7 8 9 10 11 NAME READY STATUS RESTARTS AGE pod/dex-6f4f4fd769-wdfhp 0/1 Error 197 16h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dex NodePort 10.43.69.43 \u0026lt;none\u0026gt; 5556:32000/TCP 16h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dex 0/1 1 0 16h NAME DESIRED CURRENT READY AGE replicaset.apps/dex-6f4f4fd769 1 1 0 16h dex pod을 확인해 보니 다음과 같았습니다.\n1 kubectl describe -n auth po dex-6f4f4fd769-wdfhp CrashLoopBackOff 상태임을 확인했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 Name: dex-6f4f4fd769-wdfhp Namespace: auth Priority: 0 Node: mrx-desktop3/10.160.121.43 Start Time: Thu, 07 Oct 2021 17:56:55 +0900 Labels: app=dex pod-template-hash=6f4f4fd769 Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.42.0.36 IPs: IP: 10.42.0.36 Controlled By: ReplicaSet/dex-6f4f4fd769 Containers: dex: Container ID: docker://5d1ec3cba7b0630e7fd254fbd5c5372a5459235c696884e209606a0e0facc856 Image: quay.io/dexidp/dex:v2.24.0 Image ID: docker-pullable://quay.io/dexidp/dex@sha256:c9b7f6d0d9539bc648e278d64de46eb45f8d1e984139f934ae26694fe7de6077 Port: 5556/TCP Host Port: 0/TCP Command: dex serve /etc/dex/cfg/config.yaml State: Waiting Reason: CrashLoopBackOff Last State: Terminated Reason: Error Exit Code: 2 Started: Fri, 08 Oct 2021 10:20:28 +0900 Finished: Fri, 08 Oct 2021 10:20:28 +0900 Ready: False Restart Count: 197 Environment Variables from: dex-oidc-client Secret Optional: false Environment: \u0026lt;none\u0026gt; Mounts: /etc/dex/cfg from config (rw) /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-r69m8 (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: config: Type: ConfigMap (a volume populated by a ConfigMap) Name: dex Optional: false kube-api-access-r69m8: Type: Projected (a volume that contains injected data from multiple sources) TokenExpirationSeconds: 3607 좀 더 자세한 내용을 확인하기 위해서 로그도 확인해봤습니다.\n1 kubectl log -n auth po dex-6f4f4fd769-wdfhp 로그 결과는 아래와 같았습니다.\n1 2 3 time=\u0026#34;2021-10-08T01:20:28Z\u0026#34; level=info msg=\u0026#34;config using log level: debug\u0026#34; time=\u0026#34;2021-10-08T01:20:28Z\u0026#34; level=info msg=\u0026#34;config issuer: http://dex.auth.svc.cluster.local:5556/dex\u0026#34; failed to initialize storage: failed to inspect service account token: jwt claim \u0026#34;kubernetes.io/serviceaccount/namespace\u0026#34; not found 제일 마지막에 있는 이슈를 기준으로 찾아보니 쿠버네티스 v1.21 이후 dex에서 jwt 파싱에 관한 문제였습니다.\nhttps://github.com/dexidp/dex/issues/2082\n해결 방법은 dex container에 KUBERNETES_POD_NAMESPACE env을 직접 추가해주면 됩니다.\n저는 helm차트를 이용하지 않았기 때문에 직접 deployment를 수정하였습니다.\n1 kubectl edit deployments -n auth dex 1 2 3 4 5 6 7 8 spec: template: spec: containers: - name:dex env: - name: KUBERNETES_POD_NAMESPACE value: \u0026lt;your-namespace-name\u0026gt; 추가 후 확인하면 다음과 같이 됩니다.\n1 k get all -n auth 1 2 3 4 5 6 7 8 9 10 11 12 13 NAME READY STATUS RESTARTS AGE pod/dex-9bfcdd65c-9pt7c 1/1 Running 0 70s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/dex NodePort 10.43.69.43 \u0026lt;none\u0026gt; 5556:32000/TCP 16h NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/dex 1/1 1 1 16h NAME DESIRED CURRENT READY AGE replicaset.apps/dex-6f4f4fd769 0 0 0 16h replicaset.apps/dex-9bfcdd65c 1 1 1 70s replicaset.apps/dex-58c689dffb 0 0 0 8m32s ","date":"2021-10-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/kubeflow-%EC%84%A4%EC%B9%98-%EC%A4%91-auth-dex-%EC%9D%B4%EC%8A%88-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0/","title":"kubeflow 설치 중 auth dex 이슈 해결하기"},{"content":"Kubeflow 설치 시리즈\nPre-Requisites for k3s Setup k3s Installation Kubeflow Installation 이번 포스트에서는 서버에 k3s를 설치하는 과정에 대해서 설명합니다.\n필요한 준비 과정을 확인 후 진행해 주세요.\nk3s Installation k3s 설치\nk3s는 gpu를 사용하기 위해서 도커를 백엔드로 사용하겠습니다.\ndocker 백엔드는 --docker를 추가하면 됩니다. 1 curl -sfL https://get.k3s.io | sh -s - server --disable traefik --disable servicelb --disable local-storage --docker k3s config 확인 k3s를 설치후 k3s config를 확인합니다 1 cat /etc/rancher/k3s/k3s.yaml k3s config를 kubeconfig로 복사 1 2 3 mkdir .kube sudo cp /etc/rancher/k3s/k3s.yaml .kube/config sudo chown mrx:mrx .kube/config local로 kube config 옮기기\nlocal에서 ~/.kube/config로 설정합니다.\nmulti-context인 경우 multi-context에 맞게 설정합니다.\nmulti-context는 kubectl ctx 를 통해 쉽게 변경할 수 있습니다. local에서 서버 확인하기 1 kubectl get nodes Kubernetes Setup k3s를 설치 후에는 서버로 사용할 수 있도록 써드파티들을 설치해주어야 합니다.\n위의 k3s installation 과정이 끝났다면 로컬에서 서버를 kubectl 을 통해 관리할 수 있습니다.\n아래 과정은 모두 local에서 이루어집니다. (kubectl의 context를 잘 확인 후 진행하시기 바랍니다.)\n1. Nvidia Device Plugin 1 2 3 4 5 6 helm repo add nvdp https://nvidia.github.io/k8s-device-plugin helm repo update helm install \\ --version=0.9.0 \\ --generate-name \\ nvdp/nvidia-device-plugin 2. Ingress 1 2 3 helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx helm repo update helm install ingress-nginx/ingress-nginx -g -n ingress-nginx --create-namespace --set controller.service.type=\u0026#39;NodePort\u0026#39; --set controller.service.nodePorts.http=32080 --set controller.service.nodePorts.https=32443 3. Longhorn Longhorn은 k3s에서 StorageClass를 생성을 해줍니다.\nOn Local 설치 1 2 3 helm repo add longhorn https://charts.longhorn.io helm repo update helm install longhorn longhorn/longhorn --namespace longhorn-system --set csi.kubeletRootDir=/var/lib/kubelet --create-namespace VPN IP 설정하기 1 sudo sh -c \u0026#34;echo \u0026#39;\u0026lt;VPN_IP\u0026gt; longhorn.k3s.cluster.local\u0026#39; \u0026gt;\u0026gt; /etc/hosts\u0026#34; auth \u0026amp; ingress 생성 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 USER=admin; PASSWORD=adminadmin; echo \u0026#34;${USER}:$(openssl passwd -stdin -apr1 \u0026lt;\u0026lt;\u0026lt; ${PASSWORD})\u0026#34; \u0026gt;\u0026gt; auth kubectl -n longhorn-system create secret generic basic-auth --from-file=auth cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: longhorn-ingress namespace: longhorn-system annotations: ingress.kubernetes.io/rewrite-target: / kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; # type of authentication nginx.ingress.kubernetes.io/auth-type: basic # prevent the controller from redirecting (308) to HTTPS nginx.ingress.kubernetes.io/ssl-redirect: \u0026#39;false\u0026#39; # name of the secret that contains the user/password definitions nginx.ingress.kubernetes.io/auth-secret: basic-auth # message to display with an appropriate context why the authentication is required nginx.ingress.kubernetes.io/auth-realm: \u0026#39;Authentication Required \u0026#39; # custom max body size for file uploading like backing image uploading nginx.ingress.kubernetes.io/proxy-body-size: 10000m spec: rules: - host: longhorn.k3s.cluster.local http: paths: - pathType: Prefix path: \u0026#34;/\u0026#34; backend: service: name: longhorn-frontend port: number: 80 EOF On Server 로컬에서 확인할 수 있는 reverse-proxy를 설정해주는 작업입니다. 이 작업은 서버에서 이루어집니다.\nreverse-proxy 폴더 생성 1 2 mkdir reverse-proxy cd reverse-proxy reverse-proxy 폴더 아래 다음 파일들 생성 합니다. 생성할 파일은 총 3개 입니다. Dockerfile 1 2 3 FROM nginx COPY ./longhorn.conf /etc/nginx/conf.d/longhorn.conf COPY ./kubeflow.conf /etc/nginx/conf.d/kubeflow.conf longhorn.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 server { listen 80; server_name longhorn.k3s.cluster.local; location / { proxy_redirect off; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-NginX-Proxy true; proxy_set_header X-Scheme $scheme; proxy_pass http://localhost:32080; } } kubeflow.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 server { listen 80; server_name kubeflow.k3s.cluster.local; location / { proxy_redirect off; proxy_pass_header Server; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; proxy_set_header X-NginX-Proxy true; proxy_set_header X-Scheme $scheme; proxy_pass http://localhost:32455; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; add_header X-Frame-Options SAMEORIGIN; } } reverse proxy docker를 빌드합니다. 1 docker build -t reverse-proxy . 빌드한 docker image를 실행합니다. 1 docker run -d --net host reverse-proxy On local 다음 주소로 접속이 되는지 확인합니다.\nhttp://longhorn.k3s.cluster.local/\n이 때 기본 ID/PWD는 다음과 같습니다.\nid: admin pw: adminadmin 로그인이 되면 다음과 같은 화면이 나옵니다. ","date":"2021-10-01T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/k3s-installation/","title":"k3s Installation"},{"content":"Kubeflow 설치 시리즈\nPre-Requisites for k3s Setup k3s Installation Kubeflow Installation 이번 포스트에서는 서버에 kubeflow를 설치하는 과정에 대해서 설명합니다.\n필요한 준비 과정을 확인 후 진행해 주세요.\nPre-Requisites for k3s Setup k3s Installation 이번 포스트에서 사용하는 kubeflow의 버전은 1.3.0 입니다.\n버전이 바뀔 경우 해당 설치 과정과 달라질 수 있습니다.\n자세한 내용은 링크를 확인해 주세요.\nKustomize Kubeflow를 설치하기 위해서는 kustmoize와 kubeflow manifest가 필요합니다.\nClone kubeflow/manifest\nClone 받는 위치는 편한 곳에 하면 됩니다.\n1 git clone https://github.com/kubeflow/manifests kustomize 다운로드\nkubelfow manifest를 지원하는 kustomize를 다운 받습니다. 포스트를 작성하는 기준으로는 3.2.0을 지원합니다.\nkustomize (version 3.2.0)\n⚠️ Kubeflow 1.3.0 is not compatible with the latest versions of of kustomize 4.x. This is due to changes in the order resources are sorted and printed. Please see kubernetes-sigs/kustomize#3794 and kubeflow/manifests#1797. We know this is not ideal and are working with the upstream kustomize team to add support for the latest versions of kustomize as soon as we can.\nhttps://github.com/kubernetes-sigs/kustomize/releases/tag/v3.2.0\nkubeflow/manifest 경로로 이동 다운받은 kustomize를 kubeflow/manifest로 옮겨줍니다.\n이 때 cli에서 편하게 사용하기 위해서 이름을 kustomize 로 수정하면 좋습니다.\n1 mv ~/Downloads/kustomize_3.2.0_darwin_amd64 manifests/kustomize 실행 권한 주기\n옮긴 kustomize에 실행 권한을 줍니다.\n1 2 cd manifests chmod a+x kustomize Checkout to Version 1.3.0\nmanifest를 1.3.0 버전으로 맞춰 줍니다.\n1 git checkout v1.3.0 Kubeflow Manifest 설치하기 Kubeflow를 설치하기 위한 Manifest를 설치합니다.\nCert-Manager\n1 2 3 kustomize build common/cert-manager/cert-manager-kube-system-resources/base | kubectl apply -f - kustomize build common/cert-manager/cert-manager-crds/base | kubectl apply -f - kustomize build common/cert-manager/cert-manager/overlays/self-signed | kubectl apply -f - Istio\n1 2 3 kustomize build common/istio-1-9-0/istio-crds/base | kubectl apply -f - kustomize build common/istio-1-9-0/istio-namespace/base | kubectl apply -f - kustomize build common/istio-1-9-0/istio-install/base | kubectl apply -f - Dex\n1 kustomize build common/dex/overlays/istio | kubectl apply -f - OIDC AuthService authoservice의 경우 spec의 변경이 필요합니다.\nauthservice spec 다운 받기 1 kustomize build common/oidc-authservice/base \u0026gt; oidc-authservice.yaml spec 수정\nauthservice 의 spec을 변경합니다. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: apps/v1 kind: StatefulSet metadata: name: authservice namespace: istio-system spec: replicas: 1 selector: matchLabels: app: authservice serviceName: authservice template: metadata: annotations: sidecar.istio.io/inject: \u0026#34;false\u0026#34; labels: app: authservice spec: containers: - envFrom: - secretRef: name: oidc-authservice-client - configMapRef: name: oidc-authservice-parameters image: gcr.io/arrikto/kubeflow/oidc-authservice:28c59ef imagePullPolicy: Always name: authservice ports: - containerPort: 8080 name: http-api readinessProbe: httpGet: path: / port: 8081 volumeMounts: - mountPath: /var/lib/authservice name: data securityContext: runAsUser: 0 # 추가 하기 # fsGroup: 111 # 지우기 volumes: - name: data persistentVolumeClaim: claimName: authservice-pvc 설치 1 kubectl apply -f oidc-authservice.yaml Knative\n1 2 3 kustomize build common/knative/knative-serving-crds/base | kubectl apply -f - kustomize build common/knative/knative-serving-install/base | kubectl apply -f - kustomize build common/istio-1-9-0/cluster-local-gateway/base | kubectl apply -f - Optionally, you can install Knative Eventing which can be used for inference request logging:\n1 2 kustomize build common/knative/knative-eventing-crds/base | kubectl apply -f - kustomize build common/knative/knative-eventing-install/base | kubectl apply -f - Kubeflow Namespace\n1 kustomize build common/kubeflow-namespace/base | kubectl apply -f - Kubeflow Roles\n1 kustomize build common/kubeflow-roles/base | kubectl apply -f - Kubeflow Istio Resources\n1 kustomize build common/istio-1-9-0/kubeflow-istio-resources/base | kubectl apply -f - Kubeflow Pipelines using Docker\n1 kustomize build apps/pipeline/upstream/env/platform-agnostic-multi-user | kubectl apply -f - Microservice Kubeflow에서 사용하는 Microservice들을 설치합니다.\nKFServing\n1 kustomize build apps/kfserving/upstream/overlays/kubeflow | kubectl apply -f - Katib\n1 kustomize build apps/katib/upstream/installs/katib-with-kubeflow | kubectl apply -f - Central Dashboard\n1 kustomize build apps/centraldashboard/upstream/overlays/istio | kubectl apply -f - Admission Webhook\n1 kustomize build apps/admission-webhook/upstream/overlays/cert-manager | kubectl apply -f - Notebooks\n1 2 kustomize build apps/jupyter/notebook-controller/upstream/overlays/kubeflow | kubectl apply -f - kustomize build apps/jupyter/jupyter-web-app/upstream/overlays/istio | kubectl apply -f - Profiles + KFAM\n1 kustomize build apps/profiles/upstream/overlays/kubeflow | kubectl apply -f - Volumes Web App\n1 kustomize build apps/volumes-web-app/upstream/overlays/istio | kubectl apply -f - Tensorboard\n1 2 kustomize build apps/tensorboard/tensorboards-web-app/upstream/overlays/istio | kubectl apply -f - kustomize build apps/tensorboard/tensorboard-controller/upstream/overlays/kubeflow | kubectl apply -f - User Namespace\n1 kustomize build common/user-namespace/base | kubectl apply -f - 설치 확인하기 다음 명령어를 통해 모든 pod들이 정상적으로 생성되고 있는지 확인합니다.\n1 2 3 4 5 6 7 kubectl get pods -n cert-manager kubectl get pods -n istio-system kubectl get pods -n auth kubectl get pods -n knative-eventing kubectl get pods -n knative-serving kubectl get pods -n kubeflow kubectl get pods -n kubeflow-user-example-com Port-forwarding port-forwarding을 통해 kubeflow 에 접속합니다.\n1 kubectl port-forward svc/istio-ingressgateway -n istio-system 8080:80 http://localhost:8080 로 접속합니다. 기본 ID/PW는 다음과 같습니다. ID: user@example.com PW: 12341234 포트 포워딩후 연결이 되지 않을 경우 포스트를 참고해보시기 바랍니다.\nReverse-proxy 다음 글에서 설정한 reverse-proxy 도커를 이용해 접속할 수 있습니다.\n우선 ingress-gateway의 연결된 포트를 확인합니다.\n1 kubectl -n istio-system get service istio-ingressgateway 다음과 같이 포트들이 포워딩된 것을 확인할 수 있습니다.\n1 2 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway NodePort 10.43.244.7 \u0026lt;none\u0026gt; 15021:31160/TCP,80:32455/TCP,443:32293/TCP,31400:32132/TCP,15443:32123/TCP 3d21h 이중 80번 포트의 포워딩 포트를 확인합니다. 저는 32455 였습니다.\n만약 다를 경우 다음 글 에서 kubeflow.conf의 값을 수정하면 됩니다.\nlocal에 host를 추가해줍니다.\n1 sudo sh -c \u0026#34;echo \u0026#39;\u0026lt;VPN_IP\u0026gt; kubeflow.k3s.cluster.local\u0026#39; \u0026gt;\u0026gt; /etc/hosts\u0026#34; http://kubeflow.k3s.cluster.local/ 에 접속합니다.\n마무리 설치가 정상적으로 되었다면 다음과 같은 화면이 출력됩니다. ","date":"2021-10-01T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/kubeflow-installation/","title":"Kubeflow Installation"},{"content":"Kubeflow 설치 시리즈\nPre-Requisites for k3s Setup k3s Installation Kubeflow Installation 이번 포스트에서는 k3s를 설정하기 전에 필요한 것들을 세팅하는 과정에 대해서 설명합니다.\n이번 포스트에서 진행하는 과정은 서버 데스크탑, 로컬 노트북 두 대의 장비가 있다고 가정하고 있습니다.\nOn Server 사용하는 서버 스s펙은 다음과 같습니다.\n1 2 3 4 OS: Ubuntu 20.04 LST GPU: RTX 2060 x2 CPU: Intel(R) Core(TM) i7-9700K CPU @ 3.60GHz MEM: 64G 서버에 설치할 것 들은 3가지 입니다.\nDocker Nvidia Driver WireGuard VPN (사용할 경우) 1. Docker 1.1 Docker 설치 k3s는 snap으로 설치된 docker를 사용할 수 없습니다.\n아래 링크를 통해 직접 빌드를 해야 합니다.\nhttps://docs.docker.com/engine/install/ubuntu/\n1.2 설치 확인 링크를 통해 설치가 끝나면 sudo 권한이 있는지 확인해야 합니다.\n우선 아래 명령어로 권한이 있는지 확인합니다.\n1 docker ps 권한이 없는 경우 아래와 같은 에러를 출력합니다.\n1 docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.35/containers/create: dial unix /var/run/docker.sock: connect: permission denied. 1.3 권한 문제 해결 권한이 없는 경우 그룹을 생성해 추가를 해주어야 합니다.\n자세한 내용은 링크를 통해 확인할 수 있습니다.\n우선 docker 그룹을 생성합니다. 1 sudo groupadd docker 유저를 docker 그룹에 추가합니다. 1 sudo usermod -aG docker $USER 권한을 확인합니다 1 docker ps 다만 이 과정을 통해서도 권한이 없다는 에러가 나올 수 있습니다. 이 경우에는 위 과정이 끝난 후 reboot을 하면 해결되는 경우도 있습니다.\n1 sudo reboot 2. Nvidia Driver 제가 사용하는 서버는 gpu를 이용하기 때문에 nvidia driver의 설치가 필요합니다.\nubuntu-drivers-common를 이용하면 쉽게 설치할 수 있습니다.\n2.1 ubuntu-drivers-common 설치 ubuntu-drivers-common 설치 1 sudo apt install ubuntu-drivers-common auto install 실행 1 sudo ubuntu-drivers autoinstall 2.2 Nvidia Docker 세팅 Docker가 gpu를 사용할 수 있도록 설정해주어야 합니다. 자세한 내용은 링크를 통해 확인할 수 있습니다.\nNvidia Docker 설치 1 2 3 4 5 6 distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add - curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y nvidia-docker2 sudo systemctl restart docker Docker Config 설정\nsudo 권한으로 /etc/docker/daemon.json 파일을 열어줍니다. 1 sudo vim /etc/docker/daemon.json 원본의 내용을 지우고 아래 내용으로 수정합니다. 1 2 3 4 5 6 7 8 9 { \u0026#34;default-runtime\u0026#34;: \u0026#34;nvidia\u0026#34;, \u0026#34;runtimes\u0026#34;: { \u0026#34;nvidia\u0026#34;: { \u0026#34;path\u0026#34;: \u0026#34;/usr/bin/nvidia-container-runtime\u0026#34;, \u0026#34;runtimeArgs\u0026#34;: [] } } } Reboot 1 sudo reboot nvidia-smi 확인\n설치가 되었는지 확인합니다. 1 nvidia-smi 3. WireGuard VPN (사용할 경우) 저는 WireGuard VPN을 이용하고 있기 때문에 서버에 추가적인 VPN 작업을 해주었습니다.\nWireGuard Config 발급\nWireGuard에서 Ubuntu OS의 config를 생성합니다. WireGuard 설치 1 sudo apt-get install -y resolvconf wireguard Config 복사\n발급받은 config를 서버의 /etc/wireguard/wg0.conf에 옮겨 줍니다. 저는 ssh를 이용해 전송 후 복사하였습니다. 1 sudo cp wg0.conf /etc/wireguard/wg0.conf ssh가 불가능할 경우 sudo vim /etc/wireguard/wg0.conf을 이용해 직접 수정해도 됩니다. Systemctl 설정 1 2 3 sudo systemctl enable wg-quick@wg0 sudo systemctl start wg-quick@wg0 sudo systemctl status wg-quick@wg0 인터페이스 확인 1 ifconfig wg0 Ping 확인 1 ping 172.25.0.1 WireGuard config를 수정할 경우 restart를 해줍니다.\n1 sudo systemctl restart wg-quick@wg0 On Local 로컬에서 설치 할 것 들은 2가지 입니다.\nkubectl krew 1. kubectl 각 OS에 맞는 kubectl을 설치합니다.\nhttps://kubernetes.io/docs/tasks/tools/\n2. krew krew설치\n각 OS에 맞는 krew를 설치합니다.\nhttps://krew.sigs.k8s.io/docs/user-guide/setup/install/ ctx설치 1 kubectl krew install ctx ","date":"2021-10-01T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/pre-requisites-for-k3s-setup/","title":"Pre-Requisites for k3s Setup"},{"content":"알고리즘 문제 해결 전략을 읽고 요약했습니다.\n2.2 문제 해결 과정 1.문제를 읽고 이해하기 그림과 입출력 예제로 문제가 원하는 것을 유추하지 않기 2.재정의와 추상화 자신이 다루기 쉬운 개념을 이용해, 문제를 자신의 언어로 풀어 쓰는 것 문제의 추상화 3.계획 세우기 문제를 어떻게 해결할지 계획 세우기 문제를 어떻게 해결할지 결정 사용할 알고리즘과 자료구조 선택 4.계획 검증하기 5.계획 수정하기 6.회고하기 자신이 문제를 해결한 과정을 돌이켜 보고 개선하는 과정\n코드와 함께 자신의 경험을 기록으로 남기는 것\n문제의 간단한 해법 어떤 방식으로 접근했는지? 문제의 해법을 찾는데 결정적인 깨달음을 무엇이 있는지? 오답의 원인 오답노트를 통해 자주 틀리는 부분 확인하기 다른 사람의 코드 보기 문제를 풀지 못했을 때 일정 시간이 지나도록 고민해도 답을 못 찾을 때, 다른 사람의 풀이를 참조 다른사람의 풀이를 참고했을 대 자신이 문제를 해결할 때 취했던 접근들을 되새김 왜 이 풀이를 떠올리지 못 했는지 살펴 보기 2.3 문제 해결 전략 가장 좋은 방법 Best) 직관과 체계적인 접근\n안되면? =\u0026gt; 체계적인 접근 방법\n백지에서부터 차근차근 쌓아 올리는 방법\n체계적인 적근을 위한 질문들 1. 비슷한 문제를 풀어본 적이 있던가? 2. 단순한 방법에서 시작할 수 있을까? (무식하게 풀 수 있을까?) 시간과 공간의 제약을 생각하지 않고 문제를 해결할 수 있는 가장 단순한 알고리즘\n목표) 간단하게 풀 수 있는 문제를 너무 복잡하게 생각해서 어렵게 푸는 실수를 방지하기 위함\n-\u0026gt; 만약 이 방법으로 안 풀린다면?\n최적화 과정 필요 좀 더 효율적인 자료 구조를 사용 중복 계산 방지 [예제 문제]\nN(N\u0026lt;=30)개 이하의 사탕을 세 명의 어린이에게 공평하게 나눠 주려고 한다.\n이 때 사탕의 무게는 20 이하의 정수이다.\n공평하다 -\u0026gt; 어린이들 중 가장 무거운 무게와 가장 가벼운 무게의 차이가 적을 수록 공평하다.\n[문제 풀이]\n문제를 푸는 가장 단순한 방법\n어린이에게 나눠 주는 모든 방법을 하나씩 만들어 보기 경우의 수) $3^N$, 최대 205조 개 사탕의 총량을 상태 공간으로 하는 너비 우선 탐색\n상태 공간: (0,0,0) 경우의 수) $(20*N+1)^3$, 최대 2억개 \u0026hellip;\n3. 내가 문제를 푸는 과정을 수식화 할 수 있을까? 4. 문제를 단순화 할 수 있을까? 2차원 -\u0026gt; 1차원\n왼쪽에 점이 더 많으면 왼쪽으로 오른쪽에 점이 더 많으면 오른쪽으로 5. 그림으로 그려볼 수 있을까? 6. 수식으로 표현할 수 있을까? 7. 문제를 분해할 수 있을까? 더 다루기 쉬운 형태로 문제를 변형하는 것\n문제의 제약 조건 분해\n8. 뒤에서 부터 생각해 문제를 풀 수 있을까? 문제에 내재되어 있는 순서를 바꿔 보는 것\nex) 사다리 게임에서 위로 올라가는 과정 9. 순서를 강제할 수 있을까? [예제 문제]\n어떤 순서로 버튼이 눌리던 상관 없다. 같은 칸을 2번 누를 일은 없다. [문제 풀이]\n5x5 -\u0026gt; $2^{25}$ 가지의 경우의 수 모든 칸을 왼쪽 위에서부터 순서대로 누르게 강제 $2^5$ -\u0026gt; \u0026hellip; 누를 수 있는 경우의 수를 줄여 나가기 10. 특정 형태의 답변을 고려할 수 있을까? 정규화 기법(Canonicalization)\n우리가 고려해야할 답들 중 형태가 다르지만 결과적으로 똑같은 것들을 묶은 후 그들의 대표만 고려하는 방식 ","date":"2021-09-30T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/2.-%EB%AC%B8%EC%A0%9C-%ED%95%B4%EA%B2%B0-%EA%B0%9C%EA%B4%80/","title":"2. 문제 해결 개관"},{"content":"Properties of Explanations 딥러닝 모델을 해석하려는 목적은 모델의 의사 결정 과정을 사람이 이해할 수 있게 하는데 있습니다. 이 목적이 달성되었다고 보려면 다음 두 가지 특성이 만족해야 됩니다.\nexplanations should be easy for humans to interpret explanations should be faithfully describing the decision-making process of the target model Easiness to interpret 모델 예측의 설명이 어렵다면 사람들이 사용하기 어렵습니다. 예를 들어서 뉴럴넷 모델의 해석이라고 모델의 모든 함수들의 chain 값을 준다면 이는 모델을 이해하는데 아무런 도움이 되지 않습니다.\n각 모델 설명 방법 별로 해석 방법은 다양합니다. 예를 들어서 minimal suffictient subsets는 일부의 feature만 있어도 모델 예측을 할 수 있는 모든 정보는 들어 있다고 해석할 수 있습니다. 그런데 Shapley value를 이용한 설명 방법은 사람이 이해하기 어렵습니다. Shapley value는 가능한 모든 경우의 수에서 각 marginal contribution을 계산한 값입니다. 이렇게 정량화된 값은 사람마다 다를 수 있지만 직관적이진 않습니다. 주어진 Shapley value의 값과 방향만 보고 feature별 중요도를 비교하는 등 잘못된 해석을 할 수 있습니다.\nFaithfulness 모델 설명의 충실함은 모델이 예측하는 의사 결정 과정을 얼마나 잘 맞추었는지로 평가할 수 있습니다. 여기서 주의해야 할 것은 이 개념이 모델 설명이 얼마나 정확했는지와는 다른 개념입니다.\n모델의 설명은 사용자들에게 모델에 대한 인식과 신뢰에 영향을 줍니다. 그렇기 때문에 충실하지 못한 모델의 설명은 잘못된 모델을 믿게 만들 수 있습니다.\n","date":"2021-07-19T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/properties-of-explanations/","title":"Properties of Explanations"},{"content":"Feature-Based Explanations $m$: model $x=(x_1, x_2, \u0026hellip;, x_n)$: an instance with variable $n$ eg) $x_i$ 는 문장 $x$의 $i$번 째 토큰 Importance weights explanations는 각 feature $x_i$별로 중요도를 숫자로 표현\nSubsets explanations는 $x$가 예측에 영향을 미친 subset $x$를 제공\n1. Importance Weights 1.1) Feature-Additive Weights Feature-additivity란 모든 feature의 중요도의 합이 모델의 예측에서 모델의 편향을 뺀 값과 동일해야 하는 속성을 말합니다.\n분류 문제에서는 모델의 예측은 각 클래스별 확률이고, 보통 이 중 가장 높은 확률값으로 반환합니다. 모델의 편향이란 reference, baseline 입력과 같이 아무런 정보(no information)도 없는 예측입니다. 예를 들어서 이미지 처리에서는 검은색 이미지, 자연어 처리에서는 zero-vector embedding을 baseline으로 사용합니다.\n더할 수 있는 성질과 유사하게 예측에 대한 설명에 사용된 모든 피쳐들은 occlusion, omission 방법이 가능합니다. Occlusion이란 특정 feature를 baseline feature로 대체하는 것을, Omission은 특정 feature를 완전히 제거하는 것을 의미합니다. 예를 들어 자연어 처리에서 Omission이란 문장의 길이를 변수로 사용하는 방법 입니다. Feature에 변형을 가하는 occlusion과 omission은 out-of-distribution 입력으로 신뢰할 수 없는 중요치를 제공하게 됩니다.\nFeature-additivitiy 제약 조건 아래, feature ${\\{ x_{i} \\}}{i}$ 는 $\\{w{i}(m,x)\\}_i$ 의 가중치를 갖게 됩니다.\n$$\\sum_{i=1}^{\\left| \\mathbf{x} \\right|}{w_{i}(m,\\mathbf{x})=m(\\mathbf{x})-m(\\mathbf{b})}\\tag{1}$$\n수식에서 $m$은 모델, $\\mathbf{x}$는 instance, $\\mathbf{b}$는 베이스라인 input을 의미합니다. 회귀 모델에서 $m(\\mathbf{x})$는 모델이 예측한 실제 값이며, 분류 모델에서는 클래로 예측한 확률입니다.\nShapley values Lundberg and Lee (2017) 연구는 feature-additive 설명 방법을 하나로 통합하기 위해 진행된 연구입니다. 연구에서는 게임이론의 Shapley values만이 연구에서 정의한 세 가지의 특성을 만족하는 유일한 방법임을 증명했습니다.\nLocal Accuracy (completeness)\n이 특성을 만족하기 위해서는 위에서 제시한 (1)식을 만족해야 합니다. 모든 feature-additive 방법들이 제공하는 중요도가 이 식을 만족하지는 않습니다. 예를 들으서 LIME은 $\\mathbf{x}$의 주변에서 선형 회귀 모델을 학습합기 때문에 같은 값을 재현할 수 가 없습니다. Missingness\n이 특성을 만족하기 위해서는 $\\mathbf{x}$에 존재하지 않는 feature는 중요도가 0이어야 합니다. 예를 들어서 모델 $m$과 문장 $\\mathbf{x}$이 있을 때, $\\mathbf{x}$에 존재하는 토큰들만 0 또는 다른 값의 중요도를 가져야합니다. 문장 $\\mathbf{x}$에 존재하지 않는 다른 단어들은 0의 중요도여야 합니다. Consistency\n이 특성을 만족하기 위해서는 두 개의 모델 $m$, $m\u0026rsquo;$ 있을 때 어떤 feature $x_i$가 모델 $m\u0026rsquo;$ 보다 $m$ 에서 더 큰 기여(marginal contribution)를 했다면 모델 $m$에서의 feature $x_i$의 기여도가 $m\u0026rsquo;$의 $x_i$의 기여도 보다 더 크게 나와야 합니다. 1.2) Non-Feature-Additive Weights 대부분의 importance weights explainer들은 feature-additivity 특성을 기본으로 합니다. 다만 일부 연구에서는 이러한 특성을 신경쓰지 않습니다. 예를 들어 LS-Tree는 구분 분석 트리(parse tree)를 언어 데이터에 사용해 문장 내 토큰 간의 상호작용을 감지하고 이를 정량화하는데 가중치를 사용 할 수 있도록 합니다.\n2. Minimal Sufficient Subsets 다른 유명한 예측을 설명하는 방법으로는 minimal sufficient subset(mss) 가 있습니다. Minimal sufficient subset란 입력으로 받은 feature중 일부분만 사용했을 때 전체 feature를 사용한 값과 같은 예측을 할 수 있는 feature subset을 말합니다.\nMinimal sufficient subset를 구하기 위해서는 우선 feature의 부분 집합 $\\mathbf{x}_\\mathbf{s}$ 를 모델 $m$으로 예측해야 합니다.\n이 때 위에서 설명한 omission과 deletion을 이용해 $\\mathbf{x}$를 $\\mathbf{x}_\\mathbf{s}$를 만들 수 있습니다.\n그런데 만약 모델이 feature의 부분 집합으로는 예측할 수 없고 feature 전체를 필요로 하는 경우에는 이 방법을 적용할 수 없습니다. 전체 feature를 필요하는 경우 이 방법으로 이용한 설명에는 아무런 정보가 없습니다(uninformative). 그럼에도 일부 feature만 사용하는 모델에 대해서는 좋은 설명 방법이 됩니다. 예를 들어서 computer vision에서는 모든 픽셀이 문제를 푸는데 필요하지 않습니다. 비슷하게 감정분석에서는 일부 구문만 있어도 충분히 감정을 분류할 수 있습니다.\n또한 여러 개의 minimal sufficient subset이 존재할 수 있는데, 이 때 각 minimal sufficient subset들은 인스턴스의 예측을 설명하는 하나의 독립적인 잠재적 설명(potential explanation)이 됩니다. 이런 경우에는 모델의 전체적인 면(complete view) 를 보여주기 위해서는 가능한 모든 minimal sufficient subset를 보여주어야 합니다.\nMinimal Sufficient Subsets를 이용하는 설명 방법으로는 L2X, SIS, Anchors, INVASE 등이 있습니다. L2X는 전체 feature를 이용한 예측과 subset feature를 이용한 예측의 상호 정보(mutual information)을 극대화 시키는 subset를 학습합니다. 다만, L2X를 계산하기 위해서는 최소의 원소 개수(cardinality of a minimal sufficient subset)를 모수로서 정해야 합니다. 이러한 요소는 L2X를 실제 문제에 적용을 어렵게 합니다. 이를 극복한 알고리즘이 INVASE인데, 이 알고리즘은 각 인스턴스 별로 최소의 원소 개수를 다르게 지정할 수 있습니다. L2X와 INVASE 알고리즘은 한 개의 subset만을 제공합니다. 반대로 SIS는 서로 겹치지(overlap) 않는 minimal sufficient subset를 제공합니다.\n","date":"2021-07-18T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/feature-based-explanations/","title":"Feature-Based Explanations"},{"content":"들어가기 앞서..\n본 포스트는 Explaining Deep Neural Networks 논문을 읽고 요약한 내용입니다. 더 자세한 내용은 해당 논문을 참고해주세요.\nTypes of Explanatory Methods 1. Post-Hoc versus Self-Explanatory Post-hoc explanatory methods\n사후 설명 방법은 이미 학습이 끝나서 고정된 모델을 대상으로 합니다.\n이러한 방법의 대표적인 예시로는 LIME이 있습니다. LIME은 설명하고자 하는 모델의 예측을 그 주변의 값들을 이용해 선형 회귀와 같은 해석 가능한 모델을 학습시켜서 설명합니다.\n- 장점) 고정된 모델을 대상으로 하기 때문에 모델에 영향을 미치지 않음\n- 단점) 모델 학습 외에 추가적인 학습이 필요하며 이를 위한 설명이 있는 데이터셋이 필요\nSelf-explanatory models\n자기 설명(Self-explanatory)이 가능한 모델들은 아키텍쳐 속에 모델의 예측의 해석을 제공할 수 있는 방법을 내장하고 있습니다. 큰 그림으로 보자면 self-explanatory model들은 두 개의 모듈, predictor 와 explanation generator를 갖고 있습니다.\n- 장점) 모델의 학습과 explanation generator를 동시에 함으로 추가적인 데이터가 필요 없음\n- 단점) predictor와 explanation generator가 묶여서(jointly) 학습되는데 이 때 explanation generator 가 predictor의 학습에 영향을 미치고 이는 모델의 성능의 저하로 이어짐\n2. Black-Box versus White-Box 모델을 설명하기 위해서 다른 분류 방법으로는 모델의 구조의 이해가 필요한지에 따라 나눌 수 있습니다. 이 분류 방법은 Post-hoc explanatory methods의 세분화한 것입니다.\nBlack-box/model-agnostic explainers\n블랙 박스 explainer들은 대상 모델에 input에 대한 결과만을 요청할 수 있습니다. 대표적으로 LIME, KernalSHAP, L2X 그리고 LS-Tree 와 같은 방법들이 있습니다.\n- 장점) 모델에 대한 이해가 필요 없기 때문에 어떤 형태의 모델에도 빠르게 적용할 수 있음\n- 단점) 모델에 대한 이해가 없기 때문에 input과 prediction 사이의 correlation에 영향을 받아모델의 설명력이 부정확해질 수 있음\nWhite-box/model-dependant explainers\n화이트 박스 explainer들은 대상 모델의 구조(Architecture)에 접근이 가능해야 합니다. 대표적으로 LRP, DeepLIFT, saliency maps, integrated gradients, Grad-CAM, DeepSHAP 그리고 MaxSHAP와 같은 방법이 있습니다.\n3. Instance-Wise versus Global 모델을 설명하는 또 다른 분류 방법은 설명의 범위에 따라 나눌 수 있습니다. 이 분류 방법 또한 주로 Post-hoc explanatory methods을 세분화한 것 입니다. Self-explanatory model들은 기본적으로 Instance-wise explainers 입니다.\nInstance-wise explainers\n모델의 단일 예측(individual instance) 결과에 대해서 설명을 제공합니다. 예를 들어 LIME은 instance 별로 linear regression을 학습해 설명합니다.\n- 장점) End user가 요구하는 모델의 결정에 대한 설명을 제공할 수 있음\nGlobal explainers\n조금 더 high-level에서 모델의 내부 동작 방식에 대해서 설명합니다. 예를 들어서 뉴럴 네트워크를 soft decision tree로 distilling 하여서 전체 설명력을 제공합니다.\n- 장점) 모델에 발생할 수 있는 biases를 빠르게 진단하거나 지식의 발견에 유용함\nGlobal explainers은 주로 모델을 해석할 수 있는 모델로 distiling 하기 때문에 instance-wise에 대한 설명도 제공 할 수 있습니다. 하지만 Instance-wise explainer로 모델의 전체 동작 방식을 설명하기에는 어려움이 있습니다. 비록 Instance-wise explainer 방법이 모델의 전체 동작 방식을 설명하기에는 어렵지만, 전체 모델의 설명력을 얻기 위한 시작점으로 사용할 수 있습니다.\n4. Forms of Explanations 모델을 설명하기 위한 방법에 따라서 나눌 수 도 있습니다. 각 방법에 따른 방법론들은 논문을 참고해주세요. 이 챕터에서는 간단하게 개념만 설명하고 넘어갑니다.\nFeature-based explanations Feature에 기반한 설명은 현재 가장 많이 사용되는 방법입니다. 이 방법은 모델의 개별 예측을 중요도(importance)또는 기여도(contribution)로 평가하는 방식입니다. 텍스트의 토큰과 이미지의 super-pixel 도 feature에 포함합니다.\nimportance weights\nImportance weights explanation은 입력 feature의 모델이 수행한 예측에 대한 기여 정도를 숫자로 나타냅니다.\nsubsets of features\nSubset explanation은 각 instance 별로 예측에 중요한 feature의 subset을 제공합니다.\n예를 들어서문장 감정 분류 모델이 “The movie was very good.\u0026quot;를, \u0026quot;4/5점, 긍정\u0026quot;이라고 분류했습니다. 이 때 subset explanation은 {“very”, “good”}을 중요한 feature로 제공합니다. 반면, importance weights explanation는 {\u0026quot;good\u0026quot;: 3, \u0026quot;very\u0026quot;:1}(이 때 두 점수의 합은 분류 모델의 예측인 4점 입니다)을 제공합니다.\nNatural language explanations 자연어에 대한 설명은 예측에 대한 결과를 사람과 같은 방식으로 전달합니다. 예를 들어서 “A woman is walking her dog in the park.”라는 문장에 “A person is in the park.”를 포함시켜서 설명한다면, “A woman is a person, and walking in the park implies being in the park.”과 같이 설명할 수 있습니다.\nConcept-based explanations Concept-based explainers는 사용자가 정의한 high-level의 컨셉의 중요도를 정령화 하는 것을 목표로 합니다.\nExample-based explanations Example-based는 모델이 예측한 결과에 영향을 미친 training set의 instance를 제시합니다.\nSurrogate explanations Surrogate explainers는 설명이 가능한 대체 모델을 제공하고자 합니다.\nCombinations of forms of explanations 단일 explainer는 여러 개의 설명을 제공할 수 있습니다.\n","date":"2021-07-14T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/types-of-explanatory-methods/","title":"Types of Explanatory Methods"},{"content":" 업데이트 내역: 2021.07.16 Brier Score 수식 추가 TL;DR MLOps를 성공적으로 수행하기 위해서는 아래와 같은 상황들이 일어나지 않도록 해야 합니다.\n데이터 유출에 의해 과도하게 최적화(overly optimistic)된 모델이 배포된 상황 모델이 배포된 후 아무런 관리없이 계속해서 사용 되는 상황 Hyper-parameter Optimization이 이루어지지 않는 상황 실제로 성능이 검증되지 않는 기법이 파이프라인에 바로 적용되는 상황 모델 성능 향상의 요인을 정확히 파악할 수 없는 상황 Test 데이터가 독립적이지 못한 상황 Concept drift에 대응할 수 없는 상황 잘 보정되지 않은 모델을 배포함으로서 머신러닝 파이프라인 전체의 신뢰도 하락이 일어나는 상황 모델을 개발하는 과정에서 데이터가 수정되는 과정이 기록되지 않는 상황 해당 포스트는 Using AntiPatterns to avoid MLOps Mistakes를 읽고 요약한 내용입니다.\n원문이 더 자연스럽게 이해되는 경우에는 따로 번역하지 않고 원문 그대로를 담았습니다.\nAntiPatterns Anti Pattern 이란? AntiPatterns provide real-world experience in recognizing recurring problems in the software industry and provide a detailed remedy for the most common predicaments. AntiPatterns highlight the most common problems that face the software industry and provide the tools to enable you to recognize these problems and to determine their underlying causes.\n출처: https://sourcemaking.com/antipatterns\nMLOps를 개발 및 배포할 때 발생할 수 있는 문제들 Does the data processing pipeline have unintended side effects on modeling due to data leakage or HARKing? What happens when models ‘misbehave’ in production? How is this misbehavior measured? Are there compensatory or remedial pipelines? How often are models re-trained and what is the process necessary to tune models? Is the training and model tuning reproducible? How is model performance assessed and tracked to ensure compliance with performance requirements? What constitutes a material change in the MLOps pipeline? How are changes handled? Where does the input data reside and how is it prepared on a regular basis for input to an ML model? Types of AntiPatterns Production 단계에서는 모델들은 다음과 같은 패턴으로 계속해서 바뀌게 됩니다.\nreplaced by a newer (e.g., more accurate) model retrained with more recent data 2.1) with keeping existing hyper-parameters or ranges constant or fixed 2.2) with new search for hyper-parameters 모델이 바뀌는 과정에서 여러가지 AntiPattern들을 마주하게 되는데 논문에서는 다음과 같은 9개의 상황을 제시합니다.\nData Leakage AntiPattern \u0026lsquo;Act Now, Reflect Never\u0026rsquo; AntiPattern Tuning-under-the-Carpet AntiPattern \u0026lsquo;PEST\u0026rsquo; AntiPattern Bad Credit Assignment AntiPattern Grade-your-own-Exam AntiPattern Set\u0026amp;Forget AntiPattern \u0026lsquo;Communicate with Ambivalence\u0026rsquo; AntiPattern \u0026lsquo;Data Crisis as a Service\u0026rsquo; AntiPattern 1. Data Leakage AntiPattern 데이터 유출에 의해 과도하게 최적화(overly optimistic)된 모델이 배포된 상황입니다.\nData leakage란 직역하면 데이터 유출이라는 뜻입니다. Data leakage가 된 데이터로 모델을 학습한다면 모델이 과도하게 최적화가 됩니다. 과도하게 최적화된 모델을 배포할 경우 모델 성능의 하락으로 인한 문제가 발생할 수 있습니다. 일반적으로 데이터는 Row와 Column으로 이루어져 있는데 양 방향에서 데이터 유출이 생길 수 있습니다\nFeature Leakage (Column-wise leakage) caused by a duplicate label, a proxy for the label, or the label itself For example, including a \u0026ldquo;MonthlySalary\u0026rdquo; column when predicting \u0026ldquo;YearlySalary\u0026rdquo; Training Example Leakage (Row-wise leakage) caused by improper sharing of information between rows of data. For example, suppose a model is developed to predict an individual\u0026rsquo;s risk for being diagnosed with a particular disease within the next year. 출처: https://en.wikipedia.org/wiki/Leakage_(machine_learning)\n논문에서는 이러한 데이터 유출의 상황 4가지를 제시합니다.\nPeek-a-Boo AntiPattern\n시계열 데이터의 경우 실제 발생 시간과 측정 시간의 차이(lag)가 있을 수 있습니다. 그런데 이러한 시계열 데이터를 여러 소스에서 받고 이를 한 데이터셋으로 만든 후 모델을 학습한다면 각 데이터의 lag들이 문제가 될 수 있습니다. Temporal Leakage AntiPattern\n예측 문제에서 데이터를 train, test 을 sequential하게 나누지 않을 경우 독립이어야 하는 두 데이터가 높은 상관관계를 가질 수 있습니다. Oversampling Leakage AntiPattern\nOversampling을 상황에서도 데이터 유출이 일어날 수 있습니다. SMOTE와 같은 oversampling 기법은 tarin, test 가 이미 구분된 뒤에 실행되어야 합니다. 그런데 이러한 구분 전에 oversampling을 실행한다면 정보 유출의 가능성이 생깁니다. Metrics-from-Beyond AntiPattern\n이 패턴은 pre-processing leakage, hyper-parameter leakage 에서 볼 수 있습니다. 예를 들어서 전처리를 진행할 때 train 과 test를 묶어서 진행하는 경우입니다. 2. \u0026lsquo;Act Now, Reflect Never\u0026rsquo; AntiPattern 모델이 배포된 후 아무런 관리없이 계속해서 사용 되는 상황입니다. 이러한 상황에서는 다음과 같은 문제들에 대처할 수 없게 됩니다.\nConcept drift Irrelevant or easily recognisable erroneous predictions Adversarial attacks 이러한 문제들에 대처하기 위해서는 배포된 모델에 대한 monitoring, track, debug가 가능해야 합니다. 논문에서는 이를 위해 Meta-model을 제안합니다. Meta-model 이란 배포된 모델의 모든 예측값을 평가하고 신뢰할 수 있는지 판단하는 모델을 뜻 합니다. 논문에서는 Meta-model을 사용하는 2가지 방법을 제시합니다.\nPerforming duplicate detection, filling in missing values, and is also used to fine-tune precision / recall by suppressing alerts deemed to be of low quality. Inspect model decisions further by employing explanation frameworks like LIME. 3. Tuning-under-the-Carpet AntiPattern 머신러닝과 딥러닝을 학습할 때 가장 중요한 부분은 바로 hyper-parameter 입니다. Baseline 모델과 hyper-parameter tuning이 된 모델의 성능 차이는 크다고 여러 연구에서 증명되었습니다. 배포하는 모델의 성능을 높이려면 이러한 hyper-parameter tuning을 쉽게 이용할 수 있어야 합니다. 또한 재현이 가능해야 합니다.\n4. \u0026lsquo;PEST\u0026rsquo; AntiPattern 실제로 성능이 검증되지 않는 기법이 파이프라인에 바로 적용되는 상황입니다.\n지금도 많은 학회에서 모델의 성능을 높이기 위한 여러 기법들이 발표되고 있습니다. 그런데 연구 환경과 실제 환경의 차이로 인해서 새로운 기법이 현재 배포되고 있는 모델에서는 같은 효과를 보이지 않는 경우도 많습니다. 그래서 실제 데이터에 검증되지 않은 새로운 기법을 바로 적용할 수 없습니다.\n이러한 새로운 기법을 실험을 실제 모델과 동일한 파이프라인에서 진행할 수 있어야 합니다.\n5. Bad Credit Assignment AntiPattern 모델 성능 향상의 요인을 정확히 파악할 수 없는 상황입니다.\n머신러닝 파이프라인에서 모델의 성능이 향상되는 부분은 각 과정마다 일어날 수 있습니다. 논문에서는 다음과 같은 과정들을 제시합니다.\nA function of clever problem formulations Data preprocessing Hyperparameter tuning application of existing well-established methods to interesting new tasks as detailed by paper. 정확한 성능 향상의 요인을 알기 위해서는 수정이 되는 부분을 제외하고 다른 부분은 계속해서 동일한 환경에서 실험이 이루어져야 합니다.\n6. Grade-your-own-Exam AntiPattern Test 데이터가 독립적이지 못한 상황입니다. 일반적으로 머신러닝 실험을 할 경우 평가와 test 데이터가 독립적으로 샘플링되어야 합니다. 또한 test 데이터는 최종적인 모델 평가 전에는 사용되어서는 안됩니다. 만약 test 데이터가 모델을 평가할 때 계속 사용된다면 HARKing (Hypothesizing After Results are Known)에 빠지게 됩니다. HARKing이란 모델을 만드는 사람이 성능을 test 데이터에 대해서 확인하고 모델의 성능을 올리는 과정을 뜻합니다. 이 과정은 잠재적인 data leakage를 야기하게 됩니다.\n이를 위해서 모델링을 담당하는 팀은 독립적인 \u0026lsquo;‘Ground Truth system\u0026rsquo;을 구성해서 API를 통해 예측 데이터를 얻어와 예측을 할 수 있어야 합니다.\n7. Set\u0026amp;Forget AntiPattern Concept drift에 대응할 수 없는 상황입니다. 일반적으로 머신러닝 파이프라인을 만들 때 데이터 샘플링이 i.i.d 한 상황에서 이루어진다고 가정합니다. 그래서 한번 모델 학습과 예측을 설정하고 나면 잊어 버리는 경우(Set \u0026amp; Forget)가 많습니다. 그런데 실제 상황에서는 예측해야 하는 값의 통계적 특성이 변화하는 경우(Concept Drift)가 발생합니다.\nIn predictive analytics and machine learning, concept drift means that the statistical properties of the target variable, which the model is trying to predict, change over time in unforeseen ways. This causes problems because the predictions become less accurate as time passes.\n출처: https://en.wikipedia.org/wiki/Concept_drift\n이러한 concept drift에 대처하기 위해서는 의사결정을 도와주는 보조 시스템(Decision support system)이 필요합니다. Concept drift를 대처하기 위해 의사결정나무와 같은 해석이 가능한 tree-based 를 이용할 수 있습니다.\n자세한 방법은 논문에서 인용한 논문을 참고하시기 바랍니다.\n8.\u0026lsquo;Communicate with Ambivalence\u0026rsquo; AntiPattern 잘 보정되지 않은 모델을 배포함으로서 머신러닝 파이프라인 전체의 신뢰도 하락이 일어나는 상황입니다. 잘 보정된 모델은 Brier score가 잘 보정 되었다는 뜻입니다.\n만약 보정되지 않는 모델을 사용한다면 파이프라인의 어디가 문제인지 정확히 진단할 수 없게 됩니다. 그래서 최근 연구에서는 모델의 설명 가능성 뿐만 아니라 모델의 불확실성을 전달하는 것도 중요하다고 합니다.\nBrier Score란? The Brier Score is a strictly proper score function or strictly proper scoring rule that measures the accuracy of probabilistic predictions. For uni dimensional predictions, it is strictly equivalent to the mean squared error as applied to predicted probabilities.\n출처: https://en.wikipedia.org/wiki/Brier_score\nBrier Score 수식은 다음과 같습니다.\n$$BS=\\frac{1}{N}\\sum_{t=1}^N (f_{t}-o_{t})^2$$\n$N$: the number of items you’re calculating a Brier score for $f_{t}$: is the forecast probability (i.e. 25% chance) $o_{t}$: is the outcome (1 if it happened, 0 if it didn’t) 출처: https://www.statisticshowto.com/brier-score/\nBrier Score가 0에 가까울 수록 모델이 잘 보정되었다고 해석할 수 있습니다.\n9.\u0026lsquo;Data Crisis as a Service\u0026rsquo; AntiPattern 모델을 개발하는 과정에서 데이터가 수정되는 과정이 기록되지 않는 상황입니다. 이 과정이 반복된다면 모델 학습에 사용한 데이터가 어떤 과정을 거쳤는지 확인할 수 없게 됩니다.\n","date":"2021-07-06T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/using-antipatterns-to-avoid-mlops-mistakes/","title":"Using AntiPatterns to avoid MLOps Mistakes"},{"content":"준비 과정 앞선 포스팅에도 나와 있지만 저는 Udemy의 CKA 강의를 들으며 준비했습니다. 강의 링크\n준비 기간은 2달이 걸렸습니다. 강의 듣고 요약 + Practice Test를 총 한 바퀴 돌렸습니다. 시험 1주전에는 실제 시험에서 사용할 수 있도록 Kuberenetes Document만 보면서 다시 Practice Test를 한 바퀴 더 돌렸습니다. 시험 과정에서 공식 문서의 즐겨찾기는 사용 가능합니다. 그래서 저는 두 번째 볼 때 제가 바로 찾아 볼 수 있도록 제목으로 저장했습니다.\n아래는 제가 사용했었던 즐겨찾기 입니다.\n시험 내용 CKA 시험은 v1.19 를 기준으로 2시간 17문제로 개편되었으며, 개편된 합격 기준은 66점입니다. 다른 후기를 찾아 보실 때 참고하시면 좋을 것 같습니다.\n시험이 시작하면 감독이 입장합니다. 이 때 감독관과 나누는 모든 대화는 채팅으로 이루어집니다. 감독관이 들어오면 제일 처음 신분증을 요구하는데 저는 여권을 보여주었습니다. 다른 후기들을 보니 여권이 없다면 신용카드 + 주민등록증으로도 가능한 것 같습니다.\n신분증 확인 후에는 시험보는 곳에 대해서 보여달라고 합니다. 4방향의 벽, 책상과 책상 밑을 확인합니다. 저는 노트북 스탠드, 키보드와 마우스를 따로 챙겨가서 아래 부분도 다 보여주었습니다.\n이 과정은 실제 시험시간에 포함되지 않으며 이 과정이 끝나면 시험 시작 버튼이 활성화되는데 이 때 부터 2시간입니다. 다만 정확히 몇 분이 남았는지 보여주는 것이 아니라 Progress bar로 표시되서 남은 시간을 정확히 알 수 없었습니다. 시험시작 할 때의 시간을 알아둔다면 시간 조절에 더 좋을 것 같습니다. 저는 약 30분 정도 남기고 마무리를 지었습니다.\n시험 문제 유형 시험은 Udemy Practice Test와 거의 유사하게 나옵니다. 다른 후기들과 비교해보면 아래 유형들은 꼭 나오는 것 같습니다.\nETCD Save \u0026amp; Restore Upgrade Kubeadm Fix Node 그 외에는 Pod 생성 문제들이 많았습니다.\nMulti pod Sidecar Mount PVC Expose 제일 어려웠던 부분은 Networking관련 문제들이였습니다. Practice Test와 다르게 맞았는지 틀렸는지 바로 확인할 수 없기 때문에 확신이 없었습니다. Udemy Solution에서 다른 팟을 이용해 네트워킹의 동작 여부를 확인하는 방법이 있었는데 이 부분을 집중해서 보지 않았던게 아쉬웠습니다.\n그 외에 당황스러운건 ServiceAccount 문제였습니다. 강의에선 그냥 넘어 가고 Mock Exam 3에서만 나왔던 것 같은데 다행히 시험 전에 손 풀 겸 다시 봐서 풀 수 있었습니다.\n마치며 저는 89점으로 시험에 합격했습니다. 준비를 하면서 Kubernetes에 대해서 많이 알게 된 것 같습니다. 다만 개념을 아는 것과 시험문제를 푸는 것은 다르다는 것을 또다시 느낄 수 있었습니다. 자격증이 목표라면 개념보다는 Practice Test를 위주로 본다면 한 달 정도면 충분 할 것 같습니다.\n","date":"2021-06-28T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-cka-%ED%95%A9%EA%B2%A9-%ED%9B%84%EA%B8%B0/","title":"(CKA) CKA 합격 후기"},{"content":"Pre-requisites 이번 포스트에서는 작성하는 Custom Server를 사용하기 위해서는 아래와 같은 Secret이 필요합니다.\nGHCR secrets aws secrets 1. GHCR Secrets 작성한 Custom Server의 Docker는 GHCR에서 관리합니다.\nk8s에서 image를 pull 하기 위해서는 ghcr에 접근할 수 있는 권한이 필요합니다.\n1 2 3 4 5 kubectl -n seldon create secret docker-registry mrx-ghcr \\\\ --docker-server=ghcr.io \\\\ --docker-username=\u0026lt;github id\u0026gt; \\\\ --docker-password=\u0026lt;secret key\u0026gt; \\\\ --docker-email=\u0026lt;github mail\u0026gt; 2. aws secrets 이번 포스트에서는 모델을 S3에 저장한다고 가정합니다.\nk8s에서 S3에 있는 모델을 가져오기 위한 aws에 접근할 수 있는 권한이 필요합니다.\n이를 위해서 아래와 같은 aws-secret.yaml을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Secret metadata: name: seldon-init-container-secret namespace: seldon type: Opaque data: AWS_ACCESS_KEY_ID: \u0026lt;base64 | key_id\u0026gt; AWS_SECRET_ACCESS_KEY: \u0026lt;base64 | access_key\u0026gt; AWS_ENDPOINT_URL: \u0026lt;base64 | endpoint\u0026gt; USE_SSL: \u0026lt;base64 | true\u0026gt; 여기서 주의해야할 점은 data에 value 값은 모두 base64 인코딩 되어 있어야 합니다.\n이는 bash에서 base64 command로 쉽게 변환할 수 있습니다.\n1 echo \u0026#39;hello, world!\u0026#39; | base64 변환이 되면 다음과 같이 출력됩니다.\n1 aGVsbG8sIHdvcmxkIQo= apply로 secret을 생성합니다.\n1 kubectl apply -f aws-secret.yaml Custom Server Seldon-core에서 제공하는 pre-packaged inference server 다음과 같습니다.\nSKLearn Server MLflow Server Triton Inference Server Tensorflow Serving XGBoost Server 대부분의 경우는 제공하는 서버를 사용하면 됩니다. 하지만 필요한 경우 Custom 서버를 직접 만들어서 사용할 수 도 있습니다.\n이번 포스트에서는 MLFlowServer를 기본으로 하지만 Conda 환경이 아닌 Pip 환경을 이용하는 방법에 대해서 설명합니다.\nModel Docker Content seldon-core 에서 제공하는 서버들의 디렉토리 구성은 다음과 같습니다.\n1 2 3 4 5 6 mlflowserver ├── MLFlowServer.py ├── before-run ├── conda_env_create.py ├── image_metadata.json └── requirements.txt 파일 중 before-run 과 conda_env_create.py 가 inference server의 환경을 구성하는 파일 입니다.\nconda_env_create.py 에서 conda 환경을 구성하는 부분을 빼고 pip 설치만 하도록 수정 후 pip_env_create.py 를 생성하겠습니다.\npip_env_create.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 import argparse import json import logging import os import shutil import tempfile from typing import Any import yaml from pip._internal.operations import freeze from seldon_core import Storage from seldon_core.microservice import PARAMETERS_ENV_NAME, parse_parameters log = logging.getLogger() log.setLevel(\u0026#34;INFO\u0026#34;) parser = argparse.ArgumentParser() parser.add_argument( \u0026#34;--parameters\u0026#34;, type=str, default=os.environ.get(PARAMETERS_ENV_NAME, \u0026#34;[]\u0026#34;), ) # This is already set on the environment_rest and environment_grpc files, but # we\u0026#39;ll define a default just in case. DEFAULT_CONDA_ENV_NAME = \u0026#34;mlflow\u0026#34; BASE_REQS_PATH = os.path.join( os.path.dirname(os.path.abspath(__file__)), \u0026#34;requirements.txt\u0026#34;, ) def setup_env(model_folder: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Sets up a pip environment. This methods creates the pip environment described by the `MLmodel` file. Parameters -------- model_folder : str Folder where the MLmodel files are stored. \u0026#34;\u0026#34;\u0026#34; mlmodel = read_mlmodel(model_folder) flavours = mlmodel[\u0026#34;flavors\u0026#34;] pyfunc_flavour = flavours[\u0026#34;python_function\u0026#34;] env_file_name = pyfunc_flavour[\u0026#34;env\u0026#34;] env_file_path = os.path.join(model_folder, env_file_name) env_file_path = copy_env(env_file_path) copy_pip(env_file_path) install_base_reqs() def read_mlmodel(model_folder: str) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Reads an MLmodel file. Parameters --------- model_folder : str Folder where the MLmodel files are stored. Returns -------- obj Dictionary with MLmodel contents. \u0026#34;\u0026#34;\u0026#34; log.info(\u0026#34;Reading MLmodel file\u0026#34;) mlmodel_path = os.path.join(model_folder, \u0026#34;MLmodel\u0026#34;) return _read_yaml(mlmodel_path) def _read_yaml(file_path: str) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Reads a YAML file. Parameters --------- file_path Path to the YAML file. Returns ------- dict Dictionary with YAML file contents. \u0026#34;\u0026#34;\u0026#34; with open(file_path, \u0026#34;r\u0026#34;) as file_reader: return yaml.safe_load(file_reader) def copy_env(env_file_path: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Copy conda.yaml to temp dir to prevent the case where the existing file is on Read-only file system. Parameters ---------- env_file_path : str env file path to copy. Returns ------- str tmp file directory. \u0026#34;\u0026#34;\u0026#34; temp_dir = tempfile.mkdtemp() new_env_path = os.path.join(temp_dir, \u0026#34;conda.yaml\u0026#34;) shutil.copy2(env_file_path, new_env_path) return new_env_path def copy_pip(new_env_path: str) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Copy pip packages from conda.yaml to requirements.txt Parameters ---------- new_env_path : str requirements.txt path. \u0026#34;\u0026#34;\u0026#34; conda = _read_yaml(new_env_path) pip_packages = conda[\u0026#34;dependencies\u0026#34;][-1][\u0026#34;pip\u0026#34;] freezed = freeze.freeze() freezed = map(lambda x: x.split(\u0026#34;==\u0026#34;)[0], freezed) package_to_install = [] for package in pip_packages: name = package.split(\u0026#34;==\u0026#34;)[0] if name not in freezed: package_to_install += [package] with open(BASE_REQS_PATH, \u0026#34;a\u0026#34;) as file_writer: file_writer.write(\u0026#34;\\\\n\u0026#34;.join(package_to_install)) def install_base_reqs() -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;Install additional requirements from requirements.txt. If the variable is not defined, it falls back to `mlflow`. \u0026#34;\u0026#34;\u0026#34; log.info(\u0026#34;Install additional package from requirements.txt\u0026#34;) cmd = f\u0026#34;pip install -r {BASE_REQS_PATH}\u0026#34; os.system(cmd) def main(arguments: argparse.Namespace) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;main algorithm. Parameters ---------- arguments : argparse.Namespace \u0026#34;\u0026#34;\u0026#34; parameters = parse_parameters(json.loads(arguments.parameters)) model_uri = parameters.get(\u0026#34;model_uri\u0026#34;, \u0026#34;/mnt/model/\u0026#34;) log.info(\u0026#34;Downloading model from %s\u0026#34;, model_uri) model_folder = Storage.download(model_uri) setup_env(model_folder) if __name__ == \u0026#34;__main__\u0026#34;: args = parser.parse_args() main(args) before-run\n1 2 3 4 #!/bin/bash -e echo \u0026#34;---\u0026gt; Creating environment with pip...\u0026#34; python ./pip_env_create.py MLFlowServer.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 import logging import os from typing import Dict, List, Union import numpy as np import pandas as pd import requests import yaml from mlflow import pyfunc from seldon_core import Storage from seldon_core.user_model import SeldonComponent logger = logging.getLogger() MLFLOW_SERVER = \u0026#34;model\u0026#34; class MLFlowServer(SeldonComponent): def __init__(self, model_uri: str, xtype: str = \u0026#34;ndarray\u0026#34;): super().__init__() logger.info(f\u0026#34;Creating MLFLow server with URI {model_uri}\u0026#34;) logger.info(f\u0026#34;xtype: {xtype}\u0026#34;) self.model_uri = model_uri self.xtype = xtype self.ready = False def load(self): logger.info(f\u0026#34;Downloading model from {self.model_uri}\u0026#34;) model_folder = Storage.download(self.model_uri) self._model = pyfunc.load_model(model_folder) self.ready = True def predict( self, X: np.ndarray, feature_names: List[str] = [], meta: Dict = None ) -\u0026gt; Union[np.ndarray, List, Dict, str, bytes]: logger.debug(f\u0026#34;Requesting prediction with: {X}\u0026#34;) if not self.ready: raise requests.HTTPError(\u0026#34;Model not loaded yet\u0026#34;) if self.xtype == \u0026#34;ndarray\u0026#34;: result = self._model.predict(X) else: if feature_names is not None and len(feature_names) \u0026gt; 0: df = pd.DataFrame(data=X, columns=feature_names) else: df = pd.DataFrame(data=X) result = self._model.predict(df) logger.debug(f\u0026#34;Prediction result: {result}\u0026#34;) return result def init_metadata(self): file_path = os.path.join(self.model_uri, \u0026#34;metadata.yaml\u0026#34;) try: with open(file_path, \u0026#34;r\u0026#34;) as f: return yaml.safe_load(f.read()) except FileNotFoundError: logger.debug(f\u0026#34;metadata file {file_path} does not exist\u0026#34;) return {} except yaml.YAMLError: logger.error( f\u0026#34;metadata file {file_path} present but does not contain valid yaml\u0026#34; ) return {} def class_names(self) -\u0026gt; Iterable[str]: output_schema = self._model.metadata.get_output_schema() if output_schema is not None: columns = [schema[\u0026#34;name\u0026#34;] for schema in output_schema.to_dict()] return columns image_metadata.json\n1 {\u0026#34;labels\u0026#34;: [{\u0026#34;name\u0026#34;: \u0026#34;Seldon MLFlow Server\u0026#34;}, {\u0026#34;vendor\u0026#34;: \u0026#34;Seldon Technologies\u0026#34;}, {\u0026#34;version\u0026#34;: \u0026#34;1.7.0\u0026#34;}, {\u0026#34;release\u0026#34;: \u0026#34;1\u0026#34;}, {\u0026#34;summary\u0026#34;: \u0026#34;An MLFlow Model Server for Seldon Core\u0026#34;}, {\u0026#34;description\u0026#34;: \u0026#34;The model server for MLFlow models\u0026#34;}]} requirements.txt 빈 파일을 생성합니다.\n위와 같이 수정하면 디렉토리 구성은 다음과 같이 됩니다.\n1 2 3 4 5 6 7 8 mlflowserver ├── Dockerfile └── mrxflowserver ├── MLFlowServer.py ├── before-run ├── image_metadata.json ├── pip_env_create.py └── requirements.txt Dockerfile Dockerfile 을 작성합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ARG BASE_CONTAINER=ghcr.io/\u0026lt;github-owner\u0026gt;/python:3.8-minimal FROM $BASE_CONTAINER RUN pip3 install seldon_core ENV LANG C.UTF-8 COPY mlflowserver/ /app/ RUN chmod +r /app/MLFlowServer.py RUN chmod +x /app/before-run WORKDIR /app EXPOSE 5000 EXPOSE 9000 ENV MODEL_NAME MLFlowServer ENV SERVICE_TYPE MODEL ENV PERSISTENCE 0 CMD bash before-run \u0026amp;\u0026amp;\\\\ exec seldon-core-microservice $MODEL_NAME --service-type $SERVICE_TYPE --persistence $PERSISTENCE Docker를 build하고 push 하겠습니다.\n1 2 docker build . -t ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest docker push ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest Seldon-deploy 다음과 같은 model.yaml deploy 파일을 작성합니다.\ncontainer의 image가 build 될 때 requirements.txt 에 접근 권한이 있어야 하므로 securityContext 에 privileged , runsAsUser, runAsGroup 을 작성합니다.\nmodel.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: graph-iris-model namespace: seldon spec: name: model predictors: - name: model componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} containers: - image: ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest name: model imagePullPolicy: Always securityContext: privileged: true runAsUser: 0 runAsGroup: 0 volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true imagePullSecrets: - name: mrx-ghcr initContainers: - name: model-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 imagePullPolicy: IfNotPresent args: - s3://mlflow/mlflow/artifacts/17/b84160df245441fa8d5ad7c5b62a424d/artifacts/iris_svm_model/ - /mnt/models envFrom: - secretRef: name: seldon-init-container-secret volumeMounts: - mountPath: /mnt/models name: model-provision-location graph: name: model type: MODEL children: [] parameters: - name: xtype type: STRING value: DataFrame - name: model_uri type: STRING value: /mnt/models - name: predict_method type: STRING value: decision_function replicas: 1 apply 합니다.\n1 kubectl apply -f model.yaml predict를 요청합니다.\n1 2 3 4 5 6 7 8 9 10 11 curl -X POST \u0026lt;http://10.102.231.216/seldon/seldon/graph-iris-model/api/v1.0/predictions\u0026gt; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ], \u0026#34;names\u0026#34;:[\u0026#34;sepal length (cm)\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;petal length (cm)\u0026#34;, \u0026#34;petal width (cm)\u0026#34;] } }\u0026#39; 다음과 같은 결과를 받을 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [ \u0026#34;Class_0\u0026#34;, \u0026#34;Class_1\u0026#34;, \u0026#34;Class_2\u0026#34; ], \u0026#34;ndarray\u0026#34;: [ [ 0.9663949459143529, 1.0137844346775422, 1.0215099592994648 ], [ 0.9660782736674745, 1.0133498258843965, 1.0222645232705032 ] ] }, \u0026#34;meta\u0026#34;: { \u0026#34;requestPath\u0026#34;: { \u0026#34;model\u0026#34;: \u0026#34;ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest\u0026#34; } } } Preprocessor 이번에는 데이터 전처리를 할 수 있는 preprocessor를 작성해보겠습니다.\nDocker 앞서 사용했던 파일에서 MLFLowServer.py 를 수정하겠습니다. sklearn의 StandardScaler는 predict 함수를 갖고 있지 않습니다. 그래서 pyfunc을 이용해서 load할 수 없습니다. 이를 위해서 pyfunc을 새로 생성하고 transform을 할 수 있는 transform_input 를 작성합니다.\npyfunc.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 import importlib import os from typing import Any, Dict, List, Union import mlflow import numpy as np import pandas import yaml from mlflow.exceptions import MlflowException from mlflow.models import Model from mlflow.models.model import MLMODEL_FILE_NAME from mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST from mlflow.pyfunc import ( _enforce_schema, _warn_potentially_incompatible_py_version_if_necessary, ) from mlflow.tracking.artifact_utils import _download_artifact_from_uri FLAVOR_NAME = \u0026#34;python_function\u0026#34; MAIN = \u0026#34;loader_module\u0026#34; CODE = \u0026#34;code\u0026#34; DATA = \u0026#34;data\u0026#34; ENV = \u0026#34;env\u0026#34; PY_VERSION = \u0026#34;python_version\u0026#34; PyFuncInput = Union[pandas.DataFrame, np.ndarray, List[Any], Dict[str, Any]] PyFuncOutput = Union[pandas.DataFrame, pandas.Series, np.ndarray, list] class PyFuncModel: \u0026#34;\u0026#34;\u0026#34; MLflow \u0026#39;python function\u0026#39; model. Wrapper around model implementation and metadata. This class is not meant to be constructed directly. Instead, instances of this class are constructed and returned from :py:func:`load_model() \u0026lt;mlflow.pyfunc.load_model\u0026gt;`. ``model_impl`` can be any Python object that implements the `Pyfunc interface \u0026lt;https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#pyfunc-inference-api\u0026gt;`_, and is returned by invoking the model\u0026#39;s ``loader_module``. ``model_meta`` contains model metadata loaded from the MLmodel file. \u0026#34;\u0026#34;\u0026#34; # Customized from mlflow==1.16.0 def __init__(self, model_meta: Model, model_impl: Any): if not model_meta: raise MlflowException(\u0026#34;Model is missing metadata.\u0026#34;) self._model_meta = model_meta self._model_impl = model_impl def predict(self, data: PyFuncInput) -\u0026gt; PyFuncOutput: \u0026#34;\u0026#34;\u0026#34; Generate model predictions. If the model contains signature, enforce the input schema first before calling the model implementation with the sanitized input. If the pyfunc model does not include model schema, the input is passed to the model implementation as is. See `Model Signature Enforcement \u0026lt;https://www.mlflow.org/docs/latest/models.html#signature-enforcement\u0026gt;`_ for more details.\u0026#34; :param data: Model input as one of pandas.DataFrame, numpy.ndarray, or Dict[str, numpy.ndarray] :return: Model predictions as one of pandas.DataFrame, pandas.Series, numpy.ndarray or list. \u0026#34;\u0026#34;\u0026#34; input_schema = self.metadata.get_input_schema() if input_schema is not None: data = _enforce_schema(data, input_schema) return self._model_impl.predict(data) # Customized from mlflow==1.16.0 def transform(self, data: PyFuncInput) -\u0026gt; PyFuncOutput: \u0026#34;\u0026#34;\u0026#34;transform method\u0026#34;\u0026#34;\u0026#34; input_schema = self.metadata.get_input_schema() if input_schema is not None: data = _enforce_schema(data, input_schema) return self._model_impl.transform(data) @property def metadata(self) -\u0026gt; Model: \u0026#34;\u0026#34;\u0026#34;Model metadata.\u0026#34;\u0026#34;\u0026#34; if self._model_meta is None: raise MlflowException(\u0026#34;Model is missing metadata.\u0026#34;) return self._model_meta def __repr__(self) -\u0026gt; Any: info = {} if self._model_meta is not None: if ( hasattr(self._model_meta, \u0026#34;run_id\u0026#34;) and self._model_meta.run_id is not None ): info[\u0026#34;run_id\u0026#34;] = self._model_meta.run_id if ( hasattr(self._model_meta, \u0026#34;artifact_path\u0026#34;) and self._model_meta.artifact_path is not None ): info[\u0026#34;artifact_path\u0026#34;] = self._model_meta.artifact_path info[\u0026#34;flavor\u0026#34;] = self._model_meta.flavors[FLAVOR_NAME][\u0026#34;loader_module\u0026#34;] return yaml.safe_dump( {\u0026#34;mlflow.pyfunc.loaded_model\u0026#34;: info}, default_flow_style=False, ) def load_model(model_uri: str, suppress_warnings: bool = True) -\u0026gt; PyFuncModel: local_path = _download_artifact_from_uri(artifact_uri=model_uri) model_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME)) conf = model_meta.flavors.get(FLAVOR_NAME) if conf is None: raise MlflowException( f\u0026#39;Model does not have the \u0026#34;{FLAVOR_NAME}\u0026#34; flavor\u0026#39;, RESOURCE_DOES_NOT_EXIST, ) model_py_version = conf.get(PY_VERSION) if not suppress_warnings: _warn_potentially_incompatible_py_version_if_necessary( model_py_version=model_py_version, ) if CODE in conf and conf[CODE]: code_path = os.path.join(local_path, conf[CODE]) mlflow.pyfunc.utils._add_code_to_system_path(code_path=code_path) data_path = os.path.join(local_path, conf[DATA]) if (DATA in conf) else local_path model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path) # type: ignore return PyFuncModel(model_meta=model_meta, model_impl=model_impl) MLFlowServer.py\ntransform_input 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 import logging import os from typing import Dict, List, Union import numpy as np import pandas as pd import requests import yaml from seldon_core import Storage from seldon_core.user_model import SeldonComponent from pyfunc import load_model logger = logging.getLogger() MLFLOW_SERVER = \u0026#34;model\u0026#34; class MLFlowServer(SeldonComponent): def __init__(self, model_uri: str, xtype: str = \u0026#34;ndarray\u0026#34;): super().__init__() logger.info(f\u0026#34;Creating MLFLow server with URI {model_uri}\u0026#34;) logger.info(f\u0026#34;xtype: {xtype}\u0026#34;) self.model_uri = model_uri self.xtype = xtype self.ready = False def load(self): logger.info(f\u0026#34;Downloading model from {self.model_uri}\u0026#34;) model_folder = Storage.download(self.model_uri) self._model = load_model(model_folder) self.ready = True def predict( self, X: np.ndarray, feature_names: List[str] = [], meta: Dict = None ) -\u0026gt; Union[np.ndarray, List, Dict, str, bytes]: logger.debug(f\u0026#34;Requesting prediction with: {X}\u0026#34;) if not self.ready: raise requests.HTTPError(\u0026#34;Model not loaded yet\u0026#34;) if self.xtype == \u0026#34;ndarray\u0026#34;: result = self._model.predict(X) else: if feature_names is not None and len(feature_names) \u0026gt; 0: df = pd.DataFrame(data=X, columns=feature_names) else: df = pd.DataFrame(data=X) result = self._model.predict(df) logger.debug(f\u0026#34;Prediction result: {result}\u0026#34;) return result def transform_input( self, X: np.ndarray, feature_names: List[str] = [], meta: Dict = None ) -\u0026gt; Union[np.ndarray, List, Dict, str, bytes]: logger.info(f\u0026#34;Requesting transformation with: {X}\u0026#34;) if not self.ready: raise requests.HTTPError(\u0026#34;Model not loaded yet\u0026#34;) if self.xtype == \u0026#34;ndarray\u0026#34;: result = self._model.transform(X) else: if feature_names is not None and len(feature_names) \u0026gt; 0: df = pd.DataFrame(data=X, columns=feature_names) else: df = pd.DataFrame(data=X) result = self._model.transform(df) logger.debug(f\u0026#34;transformation result: {result}\u0026#34;) return result def init_metadata(self): file_path = os.path.join(self.model_uri, \u0026#34;metadata.yaml\u0026#34;) try: with open(file_path, \u0026#34;r\u0026#34;) as f: return yaml.safe_load(f.read()) except FileNotFoundError: logger.debug(f\u0026#34;metadata file {file_path} does not exist\u0026#34;) return {} except yaml.YAMLError: logger.error( f\u0026#34;metadata file {file_path} present but does not contain valid yaml\u0026#34; ) return {} def class_names(self) -\u0026gt; Iterable[str]: output_schema = self._model.metadata.get_output_schema() if output_schema is not None: columns = [schema[\u0026#34;name\u0026#34;] for schema in output_schema.to_dict()] return columns 위와 같이 수정하면 디렉토리 구성은 다음과 같이 됩니다.\n1 2 3 4 5 6 7 8 9 mlflowserver ├── Dockerfile └── mrxflowserver ├── MLFlowServer.py ├── before-run ├── image_metadata.json ├── pip_env_create.py ├── pyfunc.py └── requirements.txt Seldon-deploy preprocessor.yaml deploy 파일을 작성합니다.\npreprocessor.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: graph-iris-preprocessor namespace: seldon spec: name: preprocessor predictors: - name: preprocessor componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} containers: - image: ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest name: preprocessor imagePullPolicy: Always securityContext: privileged: true runAsUser: 0 runAsGroup: 0 volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true imagePullSecrets: - name: mrx-ghcr initContainers: - name: preprocessor-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 imagePullPolicy: IfNotPresent args: - s3://mlflow/mlflow/artifacts/17/b84160df245441fa8d5ad7c5b62a424d/artifacts/iris_preprocessor_model/ - /mnt/models envFrom: - secretRef: name: seldon-init-container-secret volumeMounts: - mountPath: /mnt/models name: model-provision-location graph: name: preprocessor type: TRANSFORMER children: [] parameters: - name: xtype type: STRING value: DataFrame - name: model_uri type: STRING value: /mnt/models replicas: 1 apply 합니다.\n1 kubectl apply -f preprocessor.yaml predict를 요청합니다.\n1 2 3 4 5 6 7 8 9 10 11 curl -X POST \u0026lt;http://10.102.231.216/seldon/seldon/graph-iris-preprocessor/api/v1.0/predictions\u0026gt; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [ [6.8, 2.8, 4.8, 1.4], [6.0, 3.4, 4.5, 1.6] ], \u0026#34;names\u0026#34;:[\u0026#34;sepal length (cm)\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;petal length (cm)\u0026#34;, \u0026#34;petal width (cm)\u0026#34;] } }\u0026#39; 다음과 같은 결과를 받을 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 { \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [ \u0026#34;sepal length (cm)\u0026#34;, \u0026#34;sepal width (cm)\u0026#34;, \u0026#34;petal length (cm)\u0026#34;, \u0026#34;petal width (cm)\u0026#34; ], \u0026#34;ndarray\u0026#34;: [ [ 1.1591726272442633, -0.5923730118389178, 0.5922459884397444, 0.26414191647586993 ], [ 0.18982966369505322, 0.7888075857129604, 0.4217337076989351, 0.5274062850564719 ] ] }, \u0026#34;meta\u0026#34;: { \u0026#34;requestPath\u0026#34;: { \u0026#34;preprocessor\u0026#34;: \u0026#34;ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest\u0026#34; } } } Postprocessor 이번에는 score에 후처리를 하는 postprocessor를 작성해보겠습니다.\nDocker seldon-core 에서 제공하는 서버들의 디렉토리 구성은 다음과 같습니다.\n1 2 3 4 5 6 mlflowserver ├── MLFlowServer.py ├── before-run ├── conda_env_create.py ├── image_metadata.json └── requirements.txt 앞서 사용했던 파일에서 MLFLowServer.py 를 수정하겠습니다. 이 예제에서는 postprocessor 또한 sklearn의 scaler를 사용하겠습니다. sklearn의 scaler는 predict 함수를 갖고 있지 않습니다. 그래서 pyfunc을 이용해서 load할 수 없습니다. 이를 위해서 pyfunc을 새로 생성하고 transform을 할 수 있는 transform_output 를 작성합니다.\npyfunc.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 import importlib import os from typing import Any, Dict, List, Union import mlflow import numpy as np import pandas import yaml from mlflow.exceptions import MlflowException from mlflow.models import Model from mlflow.models.model import MLMODEL_FILE_NAME from mlflow.protos.databricks_pb2 import RESOURCE_DOES_NOT_EXIST from mlflow.pyfunc import ( _enforce_schema, _warn_potentially_incompatible_py_version_if_necessary, ) from mlflow.tracking.artifact_utils import _download_artifact_from_uri FLAVOR_NAME = \u0026#34;python_function\u0026#34; MAIN = \u0026#34;loader_module\u0026#34; CODE = \u0026#34;code\u0026#34; DATA = \u0026#34;data\u0026#34; ENV = \u0026#34;env\u0026#34; PY_VERSION = \u0026#34;python_version\u0026#34; PyFuncInput = Union[pandas.DataFrame, np.ndarray, List[Any], Dict[str, Any]] PyFuncOutput = Union[pandas.DataFrame, pandas.Series, np.ndarray, list] class PyFuncModel: \u0026#34;\u0026#34;\u0026#34; MLflow \u0026#39;python function\u0026#39; model. Wrapper around model implementation and metadata. This class is not meant to be constructed directly. Instead, instances of this class are constructed and returned from :py:func:`load_model() \u0026lt;mlflow.pyfunc.load_model\u0026gt;`. ``model_impl`` can be any Python object that implements the `Pyfunc interface \u0026lt;https://mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#pyfunc-inference-api\u0026gt;`_, and is returned by invoking the model\u0026#39;s ``loader_module``. ``model_meta`` contains model metadata loaded from the MLmodel file. \u0026#34;\u0026#34;\u0026#34; # Customized from mlflow==1.16.0 def __init__(self, model_meta: Model, model_impl: Any): if not model_meta: raise MlflowException(\u0026#34;Model is missing metadata.\u0026#34;) self._model_meta = model_meta self._model_impl = model_impl def predict(self, data: PyFuncInput) -\u0026gt; PyFuncOutput: \u0026#34;\u0026#34;\u0026#34; Generate model predictions. If the model contains signature, enforce the input schema first before calling the model implementation with the sanitized input. If the pyfunc model does not include model schema, the input is passed to the model implementation as is. See `Model Signature Enforcement \u0026lt;https://www.mlflow.org/docs/latest/models.html#signature-enforcement\u0026gt;`_ for more details.\u0026#34; :param data: Model input as one of pandas.DataFrame, numpy.ndarray, or Dict[str, numpy.ndarray] :return: Model predictions as one of pandas.DataFrame, pandas.Series, numpy.ndarray or list. \u0026#34;\u0026#34;\u0026#34; input_schema = self.metadata.get_input_schema() if input_schema is not None: data = _enforce_schema(data, input_schema) return self._model_impl.predict(data) # Customized from mlflow==1.16.0 def transform(self, data: PyFuncInput) -\u0026gt; PyFuncOutput: \u0026#34;\u0026#34;\u0026#34;transform method\u0026#34;\u0026#34;\u0026#34; input_schema = self.metadata.get_input_schema() if input_schema is not None: data = _enforce_schema(data, input_schema) return self._model_impl.transform(data) @property def metadata(self) -\u0026gt; Model: \u0026#34;\u0026#34;\u0026#34;Model metadata.\u0026#34;\u0026#34;\u0026#34; if self._model_meta is None: raise MlflowException(\u0026#34;Model is missing metadata.\u0026#34;) return self._model_meta def __repr__(self) -\u0026gt; Any: info = {} if self._model_meta is not None: if ( hasattr(self._model_meta, \u0026#34;run_id\u0026#34;) and self._model_meta.run_id is not None ): info[\u0026#34;run_id\u0026#34;] = self._model_meta.run_id if ( hasattr(self._model_meta, \u0026#34;artifact_path\u0026#34;) and self._model_meta.artifact_path is not None ): info[\u0026#34;artifact_path\u0026#34;] = self._model_meta.artifact_path info[\u0026#34;flavor\u0026#34;] = self._model_meta.flavors[FLAVOR_NAME][\u0026#34;loader_module\u0026#34;] return yaml.safe_dump( {\u0026#34;mlflow.pyfunc.loaded_model\u0026#34;: info}, default_flow_style=False, ) def load_model(model_uri: str, suppress_warnings: bool = True) -\u0026gt; PyFuncModel: local_path = _download_artifact_from_uri(artifact_uri=model_uri) model_meta = Model.load(os.path.join(local_path, MLMODEL_FILE_NAME)) conf = model_meta.flavors.get(FLAVOR_NAME) if conf is None: raise MlflowException( f\u0026#39;Model does not have the \u0026#34;{FLAVOR_NAME}\u0026#34; flavor\u0026#39;, RESOURCE_DOES_NOT_EXIST, ) model_py_version = conf.get(PY_VERSION) if not suppress_warnings: _warn_potentially_incompatible_py_version_if_necessary( model_py_version=model_py_version, ) if CODE in conf and conf[CODE]: code_path = os.path.join(local_path, conf[CODE]) mlflow.pyfunc.utils._add_code_to_system_path(code_path=code_path) data_path = os.path.join(local_path, conf[DATA]) if (DATA in conf) else local_path model_impl = importlib.import_module(conf[MAIN])._load_pyfunc(data_path) # type: ignore return PyFuncModel(model_meta=model_meta, model_impl=model_impl) MLFlowServer.py\ntransform_output 구현 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 import logging import os from typing import Dict, List, Union import numpy as np import pandas as pd import requests import yaml from seldon_core import Storage from seldon_core.user_model import SeldonComponent from pyfunc import load_model logger = logging.getLogger() MLFLOW_SERVER = \u0026#34;model\u0026#34; class MLFlowServer(SeldonComponent): def __init__(self, model_uri: str, xtype: str = \u0026#34;ndarray\u0026#34;): super().__init__() logger.info(f\u0026#34;Creating MLFLow server with URI {model_uri}\u0026#34;) logger.info(f\u0026#34;xtype: {xtype}\u0026#34;) self.model_uri = model_uri self.xtype = xtype self.ready = False def load(self): logger.info(f\u0026#34;Downloading model from {self.model_uri}\u0026#34;) model_folder = Storage.download(self.model_uri) self._model = load_model(model_folder) self.ready = True def predict( self, X: np.ndarray, feature_names: List[str] = [], meta: Dict = None ) -\u0026gt; Union[np.ndarray, List, Dict, str, bytes]: logger.debug(f\u0026#34;Requesting prediction with: {X}\u0026#34;) if not self.ready: raise requests.HTTPError(\u0026#34;Model not loaded yet\u0026#34;) if self.xtype == \u0026#34;ndarray\u0026#34;: result = self._model.predict(X) else: if feature_names is not None and len(feature_names) \u0026gt; 0: df = pd.DataFrame(data=X, columns=feature_names) else: df = pd.DataFrame(data=X) result = self._model.predict(df) logger.debug(f\u0026#34;Prediction result: {result}\u0026#34;) return result def transform_output( self, X: np.ndarray, feature_names: List[str] = [], meta: Dict = None ) -\u0026gt; Union[np.ndarray, List, Dict, str, bytes]: logger.info(f\u0026#34;Requesting transformation with: {X}\u0026#34;) if not self.ready: raise requests.HTTPError(\u0026#34;Model not loaded yet\u0026#34;) if self.xtype == \u0026#34;ndarray\u0026#34;: result = self._model.transform(X) else: if feature_names is not None and len(feature_names) \u0026gt; 0: df = pd.DataFrame(data=X, columns=feature_names) else: df = pd.DataFrame(data=X) result = self._model.transform(df) logger.debug(f\u0026#34;transformation result: {result}\u0026#34;) return result def init_metadata(self): file_path = os.path.join(self.model_uri, \u0026#34;metadata.yaml\u0026#34;) try: with open(file_path, \u0026#34;r\u0026#34;) as f: return yaml.safe_load(f.read()) except FileNotFoundError: logger.debug(f\u0026#34;metadata file {file_path} does not exist\u0026#34;) return {} except yaml.YAMLError: logger.error( f\u0026#34;metadata file {file_path} present but does not contain valid yaml\u0026#34; ) return {} def class_names(self) -\u0026gt; Iterable[str]: output_schema = self._model.metadata.get_output_schema() if output_schema is not None: columns = [schema[\u0026#34;name\u0026#34;] for schema in output_schema.to_dict()] return columns 위와 같이 수정하면 디렉토리 구성은 다음과 같이 됩니다.\n1 2 3 4 5 6 7 8 9 mlflowserver ├── Dockerfile └── mrxflowserver ├── MLFlowServer.py ├── before-run ├── image_metadata.json ├── pip_env_create.py ├── pyfunc.py └── requirements.txt seldon-deploy 다음과 같은 postprocessor.yaml deploy 파일을 작성합니다.\npostprocessor.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: graph-iris-postprocessor namespace: seldon spec: name: postprocessor predictors: - name: postprocessor componentSpecs: - spec: volumes: - name: model-provision-location emptyDir: {} containers: - image: ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest name: postprocessor imagePullPolicy: Always securityContext: privileged: true runAsUser: 0 runAsGroup: 0 volumeMounts: - mountPath: /mnt/models name: model-provision-location readOnly: true imagePullSecrets: - name: mrx-ghcr initContainers: - name: postprocessor-initializer image: gcr.io/kfserving/storage-initializer:v0.4.0 imagePullPolicy: Always args: - s3://mlflow/mlflow/artifacts/17/b84160df245441fa8d5ad7c5b62a424d/artifacts/iris_postprocessor_model/ - /mnt/models envFrom: - secretRef: name: seldon-init-container-secret volumeMounts: - mountPath: /mnt/models name: model-provision-location graph: name: postprocessor type: OUTPUT_TRANSFORMER parameters: - name: xtype type: STRING value: DataFrame - name: model_uri type: STRING value: /mnt/models children: [] replicas: 1 apply 합니다.\n1 kubectl apply -f postprocessor.yaml predict를 요청합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 curl -X POST \u0026lt;http://10.102.231.216/seldon/seldon/graph-iris-postprocessor/api/v1.0/predictions\u0026gt; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [ [ -0.22568508012717417, 2.236410242744829, 0.9149322555240071 ] ], \u0026#34;names\u0026#34;: [ \u0026#34;Class_0\u0026#34;, \u0026#34;Class_1\u0026#34;, \u0026#34;Class_2\u0026#34; ] } }\u0026#39; 다음과 같은 결과를 받을 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 { \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [ \u0026#34;Class_0\u0026#34;, \u0026#34;Class_1\u0026#34;, \u0026#34;Class_2\u0026#34; ], \u0026#34;ndarray\u0026#34;: [ [ -0.7139912744706434, 1.41185905424711, -0.064123229924972 ] ] }, \u0026#34;meta\u0026#34;: { \u0026#34;requestPath\u0026#34;: { \u0026#34;postprocessor\u0026#34;: \u0026#34;ghcr.io/\u0026lt;github-owner\u0026gt;/serving-tutorial:latest\u0026#34; } } } ","date":"2021-06-28T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-seldon-core-custom-server/","title":"밑바닥부터 시작하는 Seldon-Core - Custom Server"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Design a k8s cluster Purpose Education Minikube Single node cluster w\\ kubeadm/GCP/A Development \u0026amp; Testing Multi-node cluster with a single master and multiple workers setup using kbeadm tool or quick provision on GCP or AWS or AKS Hosting Production Applications Hosting Production Applications High Availability Multi node cluster with multiple master nodes kubeadm or GCP or Kops on AWS or other supported platforms Upto 5000 nodes Upto 150,000 PODs in the cluster Upto 300,000 Total Containers Upto 100 PODs per Node Cloud or OnPrem Use Kubeadm for on-prem GKE for GCP Kops for AWS AKS for Azure Storage High Performance - SSD Backed Storage Multiple Concurrent connections - Network based storage Persistent shared volumes for shared access across multiple PODs Label nodes with specific disk types Use Node Selectors to assign applications to nodes with specific disk types Nodes Virtual or Physical Machines Minimum of 4 Node cluster Master vs Worker Nodes Linux X86_64 Architecture Master nodes can host worloads Best practice is to not host workloads on Master nodes ETCD in HA ETCD a distributed reliable key-value store that is Simple, Secure \u0026amp; Fast Key-value store Tablular/Relational Databases key-value store Distributed it is possible to have your database across multiple servers Consistent how does it ensure the data on all the nodes are consitent? ETCD ensures that the same consistent copy of the data is available on all instances at the same time how does do that? read ← easy write two write request coming only one of the instances is responsible for processing the writes interanally the other nodes elect a leader among them. one node: leader other nodes: followers leader process write leader makes sure that the other nodes are sent a copy of the data if the writes come in through any of the other follower nodes, then they forward the wirtes to the leader internally Leader Election - RAFT How to elect leader in ETCD → use RAFT protocol 방법 use RAFT Algorithm random timers for initiating requets first nodes sent out a requetst to the otehr node requesting permission to be the leader other managers receiving the request responds with their vote and node assumens the leader role leader is elected, it sends out notifications at regular intervals to other masters informing them that it is continuing to assume the roloe of the leader other nodes do not receive a notifications from the leader at some point time nodes initiate a re-election process among themselves new leader is identified ","date":"2021-06-07T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-09.-design-and-install-a-k8s-cluster/","title":"(CKA) 09. Design and Install a k8s Cluster"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? POD Networking Every POD should have and IP Address Every POD should be able to communicate with every other POD in the same node Every POD should be able to communicate with every other POD on other nodes without NAT CNI in kubernetes Configuring CNI kubelete.service ps -aux | grep kubelet --cni-conf-dir=/etc/cni/net.d ls /opt/cni/bin ls /etc/cni/net.d CNI WeaveWorks Script Networking solution has a routing table which mapped what networks are on what hosts\nwhen a packet is sent from pod to the other, it goes out to the network, to the router, find its way to the node that hosts that pod → works for a small environment and in a simple network → Larger environment, this is not practical routing table may not support so many entries WeaveWorks deply an agent or service on each node they communicate with each other to exchange information withing them nodes networks PODs How it works Send a packet is sent from one pod to another on another node weave intercepts the packet and identifies that it\u0026rsquo;s on a seperate network encapsulates this packet into a new one with new source and destination sends it across the network Recieve other weave agent retrieves the packet decapsulates routes the packet to the right pod Deploy weave and weave peers deployed as service or daemons on each node kubectl apply -f \u0026quot;\u0026lt;https://cloud.weave.works/k8s/net?k8s-version=$\u0026gt;(kubectl version | base64 | tr -d '\\\\n')\u0026quot; deploys daemonset 상태 kubectl get pods -n kubesystem | grep weave-net IP Adress Management how the tool brutes networks the nodes assign an IP subset how are the pods assigned an IP where is this information stored who is responsible for ensuring there are no duplicate IP is assigned Script Who CNI who define the standards CNI it is responsibility of the CNI plugin, the network provider to take care of assigning IP to the containers How get free IP list from file placed on the each host ip = get_free_ip_from_host_local() cat /etc/cni/net.d/net-script.conf 1 2 3 4 5 \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: ..., \u0026#34;subnet\u0026#34;: ..., \u0026#34;routes\u0026#34;: ..., } WeaveWorks entire network 10.32.0.0/12 10.32.0.1 ~ 10.47.255.254 Service Networking Type of service Cluster IP if created it is accessible from all pods of the cluster, irrespective of what nodes the pods are on hosted across the cluster only aceescible from within ther cluster NodePort works as cluster IP in addtion, it also exposes the applications on a port on all nodes in ther cluster external user or applications have access to the service Question How are the services getting these IP addresses How are they made avaliable acorss all the nodes in the cluster How is service made avalible to external users through a port on each node Who is doing that, and how and where do we see it Answer kubelet process which is responsible for creating PODs each kubelet service on each node whatches the changes in the cluster through the kube-api server everty time a new pod is to be created, it creates the pod on the nodes invokes the CNI plugin to configure networking for that pod kube-proxy runs on each node watches the changes in the cluster through kube-api server every time a new service is to be crated service are not created on each node or assigned to each node service is the cluster wide concept virtual object Service IP create service obejct in k8s it is assigned an IP address from a pre-defined range kube-proxy components running on each node, get\u0026rsquo;s that IP address creates forwarding rulse on each node in ther clsuter any traffic coming in to service IP, should go the IP of the pod Rules of service IP kube-proxy supports serveral-way userspace ipvs iptables setting kube-proxy --proxy-mode [userspaces | iptables | ipvs] ... default: ip tables ip tables pod is created it has ip 10.244.1.2 create a ClusterIP service k8s assigns an IP address to it, 10.103.132.104 range is specified in kube-api-server option kube-api-server --service-cluster-ip-range ipNet default : 10.0.0.0/24 ps aux | grep kube-api-server pod ip range and service ip range shoulde be not duplicat ed see the tables iptables -L -t net | grep db-service Logs cat /var/log/kube-proxy.log what proxy it uses IP tables and Add and entry whe new serviec is created DNS in kubernetes Objectives what names are assigned to what obejcts? Service DNS records POD DNS records Kube DNS service is created k8s DNS service creates a record for the service it maps the service name to the IP address any pod can reach the service using its service name \u0026lt;service-name\u0026gt;.\u0026lt;namesapce\u0026gt;.\u0026lt;type\u0026gt;.\u0026lt;Root\u0026gt; subdomain = namespace type = svc root = cluster.local eg) curl http://web-service.apps.svc.cluster.local pod hostname 10.224.2.5 → 10-224-2-5 type → pod eg) curl http://10-224-2-5.apps.pod.cluster.local How k8s implements DNS? CoreDNS cat /etc/coredns/Corefile kubectl get configmap -n kube-system DNS Server node cat /etc/resolv.conf nameserver 10.96.0.10 k8s do automatically Ingress Ingress Controller nginx deployment 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 apiVersion: extensrions/v1beta1 kind: Deployment metadata: name: nginx-ingress-controller spec: replicas: 1 selector: matchLabels: name: nginx-ingress template: metadata: labels: name: nginx-ingress spec: containers: - name: nginx-ingress-controller image: ... args: - /nginx-ingress-controller - --configmap=$(POD_NAMESPACE)/nginx-configuration env: - name: POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namesapce ports: - name: http contrainerPort: 80 - name: https contrainerPort: 443 configmap feed nginx configuration data 1 2 3 4 kind: ConfigMap apiVersion: v1 metadata: name: nginx-configuration service expose 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion: v1 kind: Service metadata: name: nginx-ingress spec: type: NodePort ports: - port: 80 targetPort: 80 protocol: TCP name: http - port: 443 targetPort: 443 protocol: TCP name: https selector: name: nginx-ingress service account right permissions to access Roles ClusterRoles RoleBindings 1 2 3 4 apiVersion: v1 kind: ServiceAccount metadata: name: nginx-ingress-serviceaacount Ingress Resource simple one application single domain and several subdomains several domains 상태 kubectl get ingress Rules Single definition 1 2 3 4 5 6 7 8 apiVersion: extensrions/v1beta1 kind: Ingress metadata: name: ingress -wear spec: backend: serviceName: web-service servicePort: 80 Several rules definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: extensrions/v1beta1 kind: Ingress metadata: name: ingress -wear spec: rules: - http: paths: - path: /wear backend: serviceName: wear-service servicePort: 80 - path: /watch backend: serviceName: watch-service servicePort: 80 상태 kubectl describe ingress ingress-wear-watch default backend if user access to not specified server Several domains definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: extensrions/v1beta1 kind: Ingress metadata: name: ingress-wear spec: rules: - host: wear.my-online-store.com http: paths: - backend: serviceName: wear-service servicePort: 80 - host: watch.my-online-store.com http: paths: - backend: serviceName: watch-service servicePort: 80 ","date":"2021-06-06T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-08.-networking/","title":"(CKA) 08. Networking"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Pre-Requisites Switching and Routing Switching Routing Default Gateway DNS DNS Configurations on Linux CoreDNS Introduction Network Namespaces Docker Networking Switching Routing Switching 상황 There are two computers and want to communicate to each other → use switch 방법 see the interface of host ip link look at the interface named eth0 assume ip address switch: 192.168.1.0 give ip to hosts ip addr add \u0026lt;ip\u0026gt;/\u0026lt;port\u0026gt; \u0026lt;hostname\u0026gt; \u0026lt;interface\u0026gt; A: 192.168.1.10 ip addr add 192.168.1.10/24 dev eth0 B: 192.168.1.11 ip addr add 192.168.1.11/24 dev eth0 Two computers can communicate each other through switch ping 192.16.1.11 특징 switch can only enable communications within network it can receive packets from a host on the network and deliver it to other system within the same network Routing 상황 New system exists Two computer name with C, D Ip address C: 192.168.2.10 D: 192.168.2.11 How does computer A reach to computer c? → Router helps connect two networks together. 방법 Connecting two networks, it need two ip assigned. first network (A\u0026amp;B): 192.168.1.1 second network (C\u0026amp;D): 192.168.2.1 Send A to C how does know where is C? → Gateway Gateway 상황 network ← room gateway ← a door to the outside world to other networks 방법 systems need to know where that door is to go through to see the existing routing configuration on a system route displays the kernels routing table Configure a gateway ip route add 192.168.2.0/24 via 192.168.1.1 → you can reach the 192.168.2.0 network through the door or gateway at 192.168.1.11 route see the route added Configure on all system ip route add 192.168.1.0/24 via 192.168.2.1 route Internet 상황 System need acces to the internet 방법 ip route add 172.217.194.0/24 via 192.168.2.1 add a new road in routing table to road all traffic to the network 172.217.194.0 Default gateway 상황 any request to any network outside of existing network goes to this particular router 방법 ip route add default via 192.168.2.1 also can called 0.0.0.0 ip route add 0.0.0.0 via 192.168.2.1 if default gateway is defined, in same network there is no need to add 192.168.1.0 in sepreated network one entry for private network ip addr add 192.168.1.0/24 via 192.168.2.2 Linux as a host 상황 3 hosts, A, B, C host B is connected to both networks using eth0 and eth1 A\u0026amp;B are connected with 192.168.1.0 B\u0026amp;C are connected with 192.168.2.0 how to reach from A to C → need to tell host A that the door or gateway to network C is through host B 방법 add a routing table entry ip route add 192.168.2.0/24 via 192.168.1.6 when packet is reached to host C, host C will have to send back response to host A ip route add 192.168.1.0/24 via 192.168.2.6 in Linux, packets are not forwarded from one interface to the next eg) pacets receivced on eht0 on host B, are not forwarded to through eth1 eth0: private, eth1: public allow forward default: denied 1 2 \u0026gt; cat /proc/sys/net/ipv4/ip_forward 0 set allowed 1 2 \u0026gt; echo 1\u0026gt; /proc/sys/net/ipv4/ip_forward 1 this is not persistant, changed if reboot. to make persistent, modify the same value in the /etc/sysctl.conf 1 net.ipv4.ip_forward=1 Take Aways ip ip link to list and modify interfactes on the host ip addr to see the ip addresses assigned to those interface ip addr ad add IP addresses on the interfaces DNS Name Resolution 상황 two computers A and B same network assigned IP A: 192.168.1.0 B: 192.168.1.11 able to ping one computer to others with IP ping 192.168.1.11 B has a database service give a name DB want to ping to B, using DB instead of IP address ping db → err: ping :unkown host db 방법 When i say db, it means IP 192.161.1.11 1 2 cat \u0026gt;\u0026gt; /etc/hosts 192.168.1.11 db ping db successful 특징 doesn\u0026rsquo;t matter real host name can have many names need to all specify other system DNS Server 상황 until the enviroment grew and these files got filled with too many entries managing these files too hard move all these files in one server → DNS Server point all hosts to look up tahat server ip: 192.168.1.100 방법 add entry 1 2 cat /etc/resolv.conf nameserver 192.168.1.100 configure in all host 특징 also can use /etc/hosts 1 2 cat \u0026gt;\u0026gt; /etc/hosts 192.168.1.115 test ping test if an entry in both places host first looks in the local /etc/hosts and then look at the name sever order can change etc/nsswitch.conf multiple name server is possible Domain Names Structure tree structure .com top level domain google domain name www subdomain helping further grouping things together eg) maps, drive, .. In public request hits organization\u0026rsquo;s internal DNS server forwards request to the Internet point to a serving .com forwards to Google Google DNS server provides IP of ther serving the apps applications to speed up all future results, organization\u0026rsquo;s DNS server choose to cache this IP In private within in company do not wany to use domain name 1 2 3 cat \u0026gt;\u0026gt; /etc/resolv.conf nameserver 192.168.1.100 search mycompany.com Record Types Tools nslookup to query hostname from a DNS server does not consider the entries in the local /etc/hosts file entry for web application has to be present in DNS server. dig give more information Network Namespaces Process Namespace container host create a container, want to make sure that it is isolated create a special room, called namespace only sees the processes run by container underlying host has visibility in to all of the proceeses including those running inside containers Network namespace host has its own intercates that connect to the local area network own routing table, arp table information about rest of the network 방법 create a container create a network namespace for it it has no visibility to any network related infromation on the host within its namespace the container have its own virtual interfaces, routing and arp tables Create network namespace 생성 ip netns add red ip netns add blue 상태 ip netns Exec in network ns interfaces to list interfaces on my host ip link to list interfaces on network namespace ip netns exec red ip link ip -n red link arp \u0026amp; route table no entries in namespace these namespaces has no connectivity to other network Connect connect namespaces together using a virtual ehternet pair or a virtual cable 방법 create the cable ip link add veth-red type veth peer name veth-blue attach each interface to the appropriate namespace ip link set veth-red netns red ip link set veth-blue netns blue assign IP addresses to each of these namespaces ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue bring up the interface ip -n red link set veth-red up ip -n blue link set veth-blue up 확인 arp table is created Virtual network many pods → need a switch create a virtual switch how to create virtual networks? 여러 open source들 여기선 linux bridge 생성 add a new interface to the host ip link add v-net-0 type bridge check ip link turn up ip link set de v-net-0 up connect the namespaces to new virtual network switch 연결 create a cable ip link add veth-red type veth peer name veth-red-br ip link add veth-blue type veth peer name veth-blue-br attach one end of this interface to the red namespace ip link set veth-red netns red ip link set veth-red-br master v-net-0 same job with 2 in blue ip link set veth-blue netns blue ip link set veth-blue-br master v-net-0 assign ip address ip -n red addr add 192.168.15.1 dev veth-red ip -n blue addr add 192.168.15.2 dev veth-blue turn devices up ip -n red link set veth-red up ip -n blue link set veth-blue up Host and namespace try trired to reach one of these interfaces in this namespaces from my host → doesn\u0026rsquo;t work connect interface with host ip addr add 192.168.15.5/25 dev net-0 Localhost 상황 can\u0026rsquo;t reach the outside world and no one from outside world can not reach hosted inside how to configure this bridge to reach the network through the internet port? → need to add an entry in to the routing table to provide a gateway or door to the outside world localhost has all these namesapces has an interface to attach to the private network a gateway that connects the two networks toghter 방법 add localhost to router ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5 host has two ip addresses 192.168.15.5 external network: 192.168.1.2 forward iptables -t nat -A POSTROUT -s 192.168.15.0/25 -j MASQUERADE connect to internet ip netns exec blue ip route add default via 192.168.15.5 Docker Networking Single Docker host Has an ethernet interface at eth0 that connects to the local network with the IP address 192.168.1.10 run a container, there is different networking option to choose from none network host network bridge None network docker container is not attached to any network Host network container is attached to the host\u0026rsquo;s network sharing host network, cannot use same port at same time Bridge internal private network is created which the docker host and containers attach to ip address: 172.17.0.0 Bridge 상태 docker network ls docker calls the network by the name bridge ip link host the name of network is created with docker0 interface is currently down ip addr 생성 container is created docker creates a network namepsace for it ip netns docker inspect ~~ Attach container to the bridge creates a virtual cable ip link on docker host end of the interface which is attached to the local bridge docker0 ip -n ~~ link check the ip ip -n ~~ addr Port Mapping port publishing or port mapping options to allow to access the applications hosted on containers docker run -p 8080:80 nginx 방법 requirements forward traffic coming in one port to another port on the server add rules to docker chain set destination to include the containers IP see the rule iptables -nvL -t nat Cluster Networking IP \u0026amp; FQDN k8s cluster consists of master and worker nodes each node must have at least 1 interface connected to a network each interface must have an address configured the host must have a unique hostname set, unique MAC address note this if you created the VMs by cloning from existing ones. PORTS There are some ports to be opened.\nMaster kube-api - 6443 kubelet - 10250 kube-scheduler - 10251 kube-controller-manager - 10252 etcd 2379 2380, if multiple master nodes Worker kubelet - 10250 services - 30000~32767 Practice What is the network interface configured for cluster connectivity on the master node? node-to-node communication Run: kubectl get nodes -o wide to see the IP address assigned to the controlplane node.\n1 2 3 root@controlplane:~# kubectl get nodes controlplane -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME controlplane Ready control-plane,master 4h46m v1.20.0 10.3.116.12 \u0026lt;none\u0026gt; Ubuntu 18.04.5 LTS 5.4.0-1041-gcp docker://19.3.0 In this case, the internal IP address used for node for node to node communication is 10.3.116.12.\nImportant Note : The result above is just an example, the node IP address will vary for each lab.\nNext, find the network interface to which this IP is assigned by making use of the ip a command:\n1 2 3 4 5 root@controlplane:~# ip a | grep -B2 10.3.116.12 16476: eth0@if16477: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UP group default link/ether 02:42:0a:03:74:0c brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 10.3.116.12/24 brd 10.3.116.255 scope global eth0 root@controlplane:~# Here you can see that the interface associated with this IP is eth0 on the host.\nWhat is the MAC address assigned to node01? 1 2 3 \u0026gt; arp node01 Address HWtype HWaddress Flags Mask Iface 10.153.142.8 ether 02:42:0a:99:8e:07 C eth0 ","date":"2021-06-06T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-08.-networking-pre-requisites/","title":"(CKA) 08. Networking Pre-requisites"},{"content":"Basic Docker Commands Containers Start a container docker run nginx if image is not present pull image from docker hub if image is present it will reuse docker run -it nginx bash to see directly in terminal use -it option Give name to a container docker run --name \u0026lt;name\u0026gt; \u0026lt;image\u0026gt; List containers → docker randomly give conatiner_id and names to running container\ndocker ps list containers running containers docker ps -a running containers + previously stopped or exited containers Stop a container docker stop \u0026lt;container id\u0026gt; docker stop \u0026lt;docker name\u0026gt; no running containers remains in docker ps -a with Exited status Remove containers → when want to get rid of it from lying space\ndocker rm \u0026lt;container id\u0026gt; docker rm \u0026lt;docker name\u0026gt; no longer present in docker ps -a Get container ids docker ps -aq remove all containers docker rm $(docker ps -aq) Images List images docker images Get all image ids docker images -aq Remove images docker rmi nginx delete all dependent containers to remove image! Download an image docker pull nginx only pull image, not run docker image Docker run No hosting in docker Containers are not meant to host an operating system.\nContainers are meant to run a specific task or process.\nsuch as to host an instance of a web server or application server, \u0026hellip; Once the task is complete the container exits a container.\nContainer only lives as long as the process inside it is alive.\nAppend a command docker run ubuntu sleep 5 Execute a command I would like to see the contents of a file inside some particular containers.\ndocker exec \u0026lt;run name\u0026gt; cat /etc/hosts Attach and Detach docker run kodekloud/simple-webapp when attached, you can not do anything. docker run -d kodekloud/simple-webapp detach mode, it runs in background mode. To attach again docker attach \u0026lt;name or run_id\u0026gt; Docker Run Tag docker run \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt; no specific tag → use latest tag. STDIN by default, docker container does not listen to standard input, even attached.\n-i interactive mode -t pseudo terminal -it use two options PORT Mapping IP of docker container internal IP and is only accessible within the docker host. IP of docker host -p docker run -p \u0026lt;host_port\u0026gt;:\u0026lt;internal_port\u0026gt; \u0026lt;image name\u0026gt; Volume Mapping docker container has its own isolated file system. if want to persist data, map a directory outside the container on the docker host to ad directory inside the container -v docker run -v \u0026lt;host volume\u0026gt;:\u0026lt;docker volume \u0026lt;image name\u0026gt; Inspect Container docker inspect \u0026lt;container id | name\u0026gt; Container Logs docker logs \u0026lt;container id | name\u0026gt; ","date":"2021-05-27T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/docker-commands-run/","title":"Docker Commands \u0026 Run"},{"content":"Docker Images How to create my own image? Order OS - Ubuntu Update apt repo Install dependencies using apt Install Python dependencies using pip Copy source code to /opt folder Run the web server using \u0026ldquo;flask\u0026rdquo; command Dockerfile 1 2 3 4 5 6 7 8 9 FROM Ubuntu RUN apt-get update \u0026amp;\u0026amp; apt-get -y install python RUN pip install flask flask-mysql COPY . /opt/source-code ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run INSTRUCTION ARGUMENT Build docker build . -f Dockerfile -t \u0026lt;image name\u0026gt; Push docker push \u0026lt;image name\u0026gt; Layered architecture each layer only store the changes from the previous layer docker history \u0026lt;image name\u0026gt; Environment Variables -e docker run -e \u0026lt;KEY\u0026gt;=\u0026lt;value\u0026gt; \u0026lt;image name\u0026gt; Inspect Environment Variable docker inspect \u0026lt;container name | id\u0026gt; Command vs Entrypoint 설명 Command CMD instruction the command line parameters passed will get replaced entirely. Entrypoint ENTRYPOINT command line parameters will get appended. if parameter is not given, it will get error that the operand is missing How to give and default value for entrypoint? 1 2 ENTRYPOINT [\u0026#34;command\u0026#34;] CMD [\u0026#34;param1\u0026#34;] how to override entrypoint? docker run --entrypoint \u0026lt;executable\u0026gt; \u0026lt;image-name\u0026gt; \u0026lt;param\u0026gt; 명령어 CMD CMD command param1 CMD [\u0026quot;command\u0026quot;, \u0026quot;param1\u0026quot;] ENTRYPOINT ENTRYPOINT [\u0026quot;command\u0026quot;] ","date":"2021-05-27T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/docker-images/","title":"Docker Images"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Docker Storage File system Layered architecture build application 1 application 2 run copy-on-write Volumes Volume mounting create and run create docker volume create data_volume run docker run -v data_volume:/var/lib/mysql mysql create by run docker run -v data_volume2:/var/lib/mysql mysql automatically create data_volume Bind mounting docker run -v /data/mysql:/var/lib/mysql mysql Preferred old -v new --mount docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql Storage drivers AUFS ZFS BTFS Device Mapper Overlay Overlay2 Volume Drivers Local Azure File Storage Convoy \u0026hellip; Volumes docker k8s\nVolumes \u0026amp; Mounts definition\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadat: name: random-number-generator spec: containers: - image: alpine name: alpine command: [\u0026#34;bin/sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;shuf -i 0-100 -n 1 \u0026gt;\u0026gt; /opt/number.out;\u0026#34;] volumeMounts: - mountPath: /opt name: data-volume volumes: - name: data-volume hostPath: path: /data type: Directory hostPath → single node is possible → not recommended in multi node AWS 1 2 3 4 5 volumes: - name: data-volume awsElastricBlockStore: volumeID: \u0026lt;volume-id\u0026gt; fsType: ext4 Persistent Volume 생성 definition 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol1 spec: persistentVolumeReclaimPolicy: Retain accessModes: - ReadWriteOnce capacity: storage: 1Gi hostPath: path: /tmp/data accessModes ReadOnlyMany ReadWriteOnce ReadWriteMany persistentVolumeReclaimPolicy delete recycle retain 상태 kubectl get persistentvolume kubectl get pv Persistent Volume Claims Persistent Volume administrator crates Persistent Volume Claims users create to use persistent volume 생성 definition 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi 상태 kubectl get persistentvolumeclaim kubectl get pvc 삭제 kubectl delete persistentvolumne myclaim Pod with PVC 생성 definition w\\ hostPath 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: webapp spec: containers: - name: event-simulator image: kodekloud/event-simulator env: - name: LOG_HANDLERS value: file volumeMounts: - mountPath: /log name: log-volume volumes: - name: log-volume hostPath: ## directory location on host path: /var/log/webapp ## this field is optional type: Directory definition w\\ PVC 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion: v1 kind: Pod metadata: name: webapp spec: containers: - name: event-simulator image: kodekloud/event-simulator env: - name: LOG_HANDLERS value: file volumeMounts: - mountPath: /log name: log-volume volumes: - name: log-volume persistentVolumeClaim: claimName: claim-log-1 Storage Class 상태 kubectl get storageclass kubectl get sc ","date":"2021-05-23T09:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-07.-storage/","title":"(CKA) 07. Storage"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Kubernetes Security Primitives Secrure Hosts Password based authentication disabled SSH Key based authentication Secure kubernetes kube-apiserver Who can access? What can they do? Authentication Who can access? Files - Username and Passwords Fiels - Username and Tokens Certificates External Authenication providers - LDAP Service Accounts Authorization What can they do? RBAC Authorization ABAC Authorization Node Authorization Webhook Mode TLS Certificates Network Policies Authentication Users\nAdmins Developers Application End Users Bots Accounts User Admins Developers Servcie Accounts Bots not supported officialy in k8s Auth Mechanisms kube-apiserver Static Password File Static Token File Certificates Identity Service Basic - file user-details.csv 1 2 password123,user1,u0001 password123,user2,u0002 kube-apiserver.service 1 2 3 ExecStart=/usr/local/bin/kube-apiserver \\ --basic-auth-file=user-details.csv \\ ... kubeadm 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --authorization-mode=Node,RBAC \u0026lt;content-hidden\u0026gt; - --basic-auth-file=/tmp/users/user-details.csv pod 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver \u0026lt;content-hidden\u0026gt; image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3 name: kube-apiserver volumeMounts: - mountPath: /tmp/users name: usr-details readOnly: true volumes: - hostPath: path: /tmp/users type: DirectoryOrCreate name: usr-details Authenticate User curl -v -k \u0026lt;https://master-node-ip:6443/api/v1/pods\u0026gt; -u \u0026quot;user1:password123\u0026quot; Basic - token user-token-details.csv 1 2 asdfbasdhjasdhjkfhjk12312,user1,u0001 1234g23rgy8werfg89asdfasd,user2,u0002 kube-apiserver.service 1 2 3 ExecStart=/usr/local/bin/kube-apiserver \\ --token-auth-file=user-details.csv \\ ... kubeadm kubectl edit Authenticate User curl -v -k \u0026lt;https://master-node-ip:6443/api/v1/pods\u0026gt; --header \u0026quot;Authorization: Bearer asdfbasdhjasdhjkfhjk12312 TLS master node worker nodes Server Certificates for Servers Kube-Api server exposes a service that other components as well as external users use to manage the k8s cluster server certificates to secure all communications with its clients apiserver.crt apiserver.key ETCD Server etcdserver.crt etcdserver.key KUBELET Server kubelet.crt kubelet.key Client Certificates for Clients Admin admin.crt admin.key Kube-Scheduler scheduler.crt scheduler.key Kube-Controller-Manager controll-manager.crt controll-manager.key Kube-Proxy kube-proxy.crt kube-proxy.key Certification Creation OpenSSL to Create Client Certification Certificate Authority(CA) Generate Keys (ca.key) openssl genrsa -out ca.key 2048 Certificate Signing Requests (ca.csr) openssl req -new -key ca.key -subj \u0026quot;/CN=KUBERNETES-CA\u0026quot; -out ca.csr Sign Certificates (ca.crt) openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt Admin User Generate Keys (admin.key) openssl genrsa -out admin.key 2048 Certificate Signing Requests (admin.csr) openssl req -new -key admin.key -subj \u0026quot;/CN=kube-admin/O=system:masters\u0026quot; -out admin.csr \u0026quot;/CN=kube-admin/O=system:masters\u0026quot; ← admin user Sign Certificates (admin.crt) openssl x509 -req -in admin.csr -signkey ca.key -out admin.crt Kube Scheduler Kube Controller Manager kube Proxy How to use curl \u0026lt;https://kube-apisever:6443/api/v1/pods\u0026gt; --key admin.key --cert admin.crt --cacert ca.crt definition 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: v1 clusters: - cluster: certificate-authority: ca.crt server: \u0026lt;https://kube-apiserver:6443\u0026gt; name: kubernetes kind: Config users: - name: kubernetes-admin user: client-certificate: admin.crt clinet-key: admin.key OpenSSL to Create Server Certification ETCD Servers 1 2 3 4 5 6 7 8 - etcd --key-file= --cert-file= --peer-cert-file= --peer-client-cert-auth= --peer-key-file= --peer-trusted-ca-cile --trusted-ca-file= Kube-Api Server Generate Keys (apiserver.key) openssl genrsa -out apiserver.key 2048 Certificate Signing Requests (apiserver.csr) openssl.cnf 1 2 3 4 5 6 7 8 9 10 11 12 13 [req] req_extensions = v3_req [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRpudiation, subjectAltName = @alt_names [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 10.96.0.1 IP.2 = 172.17.0.87 openssl req -new -key apiserver.key -subj \u0026quot;/CN=kube-apiserver\u0026quot; -out apiserver.csr -config openssl.cnf Sign Certificates (apiserver.crt) openssl x509 -req -in apiserver.csr -CA ca.crt -CAkey ca.key -out apiserver.crt Kubelet Server server cert definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: kubelet.config.k8s.io/v1beta1 kind: KubeletConfiguration authentication: x509: clientCAFile: \u0026#34;/var/lib/kubernetes/ca.pem\u0026#34; authorization: mode: Webhook clusterDomain: \u0026#34;cluster.local\u0026#34; clusterDNS: - \u0026#34;10.32.0.10\u0026#34; podCIDR: \u0026#34;$(POD_CIDR)\u0026#34; resolveConf: \u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; reuntimeRequrestTimeout: \u0026#34;15m\u0026#34; tlsCertFile: \u0026#34;/var/lib/kubelet/kubelet-node01.crt\u0026#34; tlsPrivateKeyFile: \u0026#34;/var/lib/kubelet/kubelet-node01.key\u0026#34; client cert View Certificate Details The Hard Way cat /etc/systemd/system/kube-apiserver.service kubeadm cat /etc/kubernetes/manifest/kube-apiserver.yaml Inspect Service Logs journalctl -u etcd.service View Logs kubectl logs etcd-master Certificates API Certificate Signing Request Order Create CertificateSigningReqeust Object Review Requests Approve Requests Share Certs to Users Request openssl genrsa -out jan.key 2048 openssl req -new -key jane.key -subj \u0026quot;/CN=jane\u0026quot; -out jan.csr send to the administrator administrator takes the key and creates a CertifacteSigningRequest obejct 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: certificates.k8s.io/v1beta1 kind: CeritifcateSigningRequest metadata: name: jane spec: groups: - system:authenticated usage: - digital signature - key encipherment - server auth request: \u0026lt;cat jane.csr | base64\u0026gt; cat akshay.csr | base64 | tr -d \u0026quot;\\\\n\u0026quot; Approve kubectl get csr 1 2 3 \u0026gt; k get csr NAME AGE SIGNERNAME REQUESTOR CONDITION akshay 10s kubernetes.io/kube-apiserver-client kubernetes-admin Pending kubectl certificate approve jane 상세 요청 kubectl get csr jane -0 yaml Deny kubectl certificate deny agent-smith Delete kubectl delete csr agent-smith Contoller Manager CSR-APPROVING CSR-SIGNING cat /etc/kubernetes/manifests/kube-controller-manager.yaml Kubeconfig Using kubectl with crt 1 2 3 4 5 kubectl get pods --server my-kube-playground:6443 --client-key admin.key --client-certificated admin.crt --certificate-authority ca.crt this is tedious task → move it to config file Using kubectl with config file config 1 2 3 4 --server my-kube-playground:6443 --client-key admin.key --client-certificated admin.crt --certificate-authority ca.crt kubectl kubectl get pods --kubeconfig config default config file path .kube/config KubeConfig File clusters development production google --server my-kube-playground:6443 contexts match cluster and users admin@production dev@google mykubeadmin@mykubeplayground users admin dev user prod user 1 2 3 --client-key admin.key --client-certificated admin.crt --certificate-authority ca.crt defintion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: v1 kind: Config current-context: dev-user@google clusters: - name: my-kube-playground cluster: certificate-authority: ca.crt server: my-kube-playground:6443 - name: ... contexts: - name: my-kube-admin@my-kube-playground context: clusters: my-kube-playground user: my-kube-admin - name: ... users: - name: my-kube-admin user: client-certificate: admin.crt client-key: admin.key - name: ... current-context default context certificate-authority instead file path using data directly certificate-authority-data with base64 encoding Command Line Tool view kubectl config view kubectl config view --kubeconfig=\u0026lt;config-file-name\u0026gt; update kubectl config use-context prod-user@production helo kubectl config -h API Groups Example curl \u0026lt;https://kube-master:6443/version\u0026gt; curl \u0026lt;https://kube-master:6443/api/v1/pods\u0026gt; Types /metrics /healthz /version /api /apis /logs API, APIS api core group apis named group CLT curl \u0026lt;http://localhost:6443\u0026gt; -k curl \u0026lt;http://localhost:6443\u0026gt; -k | grep \u0026quot;name\u0026quot; need certificates file kubectl proxy to see proxy kube proxy ≠ kubectl proxy kube proxy enable connectivity betewwnd pods and services across different nodes in the cluster. kubectl proxy HTTP proxy service created by kubectl utility to access the kube-api sever Authorization Authorization Mechanisms Types Node ABAC RBAC Webhook Node Node Authorizer ABAC User Attribute Based Authorization\ndev-user → Can view PODs → Can create PODs → Can delete PODs 1 {\u0026#34;kind\u0026#34;: \u0026#34;Policy\u0026#34;, \u0026#34;spec\u0026#34;: {\u0026#34;user\u0026#34;: \u0026#34;dev-user\u0026#34;, \u0026#34;namespace\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;resource\u0026#34;: \u0026#34;pods\u0026#34;, \u0026#34;apiGroup\u0026#34;: \u0026#34;*\u0026#34;}} RBAC Rule Based Authorizaion\nDeveloper → Can view PODs → Can create PODs → Can delete PODs definition 1 2 3 4 5 6 7 8 9 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: developer rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;] resourceNames: [\u0026#34;blue\u0026#34;, \u0026#34;orange\u0026#34;] rules have 3 sections apiGroups resources verbs link users to rule 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: devuser-developer-binding subjects: - kind: User name: dev-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: developer apiGroup: rbac.authorization.k8s.io view kubectl get roles kubectl get rolebindings Check Access kubectl auth can-i create deployments kubectl auth can-i delete nodes kubectl auth can-i create deployments --as dev-user Webhook outsource all the mechanism AlwaysAllow always allow AlwaysDeny always deny Setting 1 2 ExecStart = ...\\\\ --authorization-mode=AlwaysAllow default : AlwaysAllow multiple mode: 1 Node,RBAC,Webhook if one\u0026rsquo;s requiest is denied, it pass to next chain if approved no more checks. Inspect 1 kubectl describe pod kube-apisever-controlplane -n kube-system --authorizastion mode Cluster Roles Types namespace ← role pods replicasets \u0026hellip; kubectl api-resources --namespaced=true cluster scoped nodes pv \u0026hellip; kubectl api-resources --namespaced=false Clusterroles Cluster admin Can view Nodes can create Nodes Can delete Nodes definition 1 2 3 4 5 6 7 8 apiVersion: rbac.authorizastion.k8s.io/v1 kind: ClusterRole metadata: name: cluster-administrator rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;list\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;] Clusterrolebining definition 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: cluster-admin-role-binding subjects: - kind: User name: cluster-admin apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: cluster-administrator apiGroup: rbac.authorization.k8s.io Image Security Private Repository Login to private repository docker login private-registry.io docker run private-registry.io/apss/internal-app Create secret 1 2 3 4 5 kubectl create secrete docker-registry regcred \\\\ --docker-server=private-registry.io \\\\ --docker-username=registry-user \\\\ --docker-password=registry-password \\\\ --docker-email=registry-user@org.com Create pod definition 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Poid metadata: name: nginx-pod spec: containers: - name: nginx image: private-registry.op/apps/internal-app imagePullSecrets: - name: regcred Security Context definition for pod 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: web-pod spec: securityContext: runAsUser: 1000 containers: - name: ubuntu image: ubuntu command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] definition for container 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: web-pod spec: containers: - name: ubuntu image: ubuntu command: [\u0026#34;sleep\u0026#34;, \u0026#34;3600\u0026#34;] securityContext: runAsUser: 1000 capabilities: add: [\u0026#34;AMC_ADMIN\u0026#34;] capabilities only supported at the container level Network Policy Ingress \u0026amp; Egress ingress for a web serever the incoming traffic from the users is an ingress egress outgoing request to the app server allow external calls traffic Network Security Network Policy eg) Allow Ingress Raffic From API pod on Port 3306 Selectors definition 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersions: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: mathLabels: role: db policyTpes: - Ingress ingress: - from: - podSelector: matchLabels: name: api-pod ports: - protocol: TCP port: 3306 Network Policies network policy to pod to protect defintion 1 2 3 4 5 6 7 8 apiVersions: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: mathLabels: role: db what type of policies? ingress or egress or both ? allow incoming request → ingress if incoming request is allowed, response is allowed automatically definition 1 2 policyTpes: - Ingress define specific rules 1 2 3 4 5 6 7 8 ingress: - from: - podSelector: matchLabels: name: api-pod ports: - protocol: TCP port: 3306 allow only in prod namespace definition (and) 1 2 3 4 5 6 7 8 9 10 11 ingress: - from: - podSelector: matchLabels: name: api-pod namespaceSelector: matchLabels: name: prod ports: - protocol: TCP port: 3306 if only namespaceSelector is defined allow or request in same namespace definition (or) 1 2 3 4 5 6 7 8 9 10 11 ingress: - from: - podSelector: matchLabels: name: api-pod - namespaceSelector: matchLabels: name: prod ports: - protocol: TCP port: 3306 use array to backup in server 1 2 - ipBlock: - cidr: 192.168.5.10/32 to push to bakcup server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 apiVersions: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: mathLabels: role: db policyTpes: - Ingress - Egress ingress: - from: - podSelector: matchLabels: name: api-pod ports: - protocol: TCP port: 3306 egress: - to: - ipBlock: cidr: 192.168.5.10/32 ports: - protocol: TCP port: 80 ","date":"2021-05-23T08:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-06.-security/","title":"(CKA) 06. Security"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Operating System Upgrade upgrading base software applying security patches Pod Eviction Timeout waiting pod healthy pod-eviction-timeout=5m0s Drain Do not use this node, and all pods are out.\nkubectl drain node01 kubectl drain node01 --ignore-daemonsets 1 2 3 4 5 6 7 8 9 10 root@controlplane:~# kubectl drain node01 --ignore-daemonsets node/node01 cordoned WARNING: ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-x6dgs, kube-system/kube-proxy-jfmxw evicting pod default/blue-746c87566d-wn2qg evicting pod default/blue-746c87566d-kt266 evicting pod default/blue-746c87566d-9w9zq pod/blue-746c87566d-kt266 evicted pod/blue-746c87566d-wn2qg evicted pod/blue-746c87566d-9w9zq evicted node/node01 evicted kubectl drain node01 --ignore-daemonsets --force\nif a pod in node has no replicaset, use --force option, but a pod will be lost. 1 2 3 4 5 6 root@controlplane:~# kubectl drain node01 --ignore-daemonsets --force node/node01 already cordoned WARNING: deleting Pods not managed by ReplicationController, ReplicaSet, Job, DaemonSet or StatefulSet: default/hr-app; ignoring DaemonSet-managed Pods: kube-system/kube-flannel-ds-x6dgs, kube-system/kube-proxy-jfmxw evicting pod default/hr-app pod/hr-app evicted node/node01 evicted Cordon Do not use this node, but exist pods will be running.\nkubectl cordon node01 This will ensure that no new pods are scheduled on this node. The existing pods will not be affected by this operation. Uncordon Can use this node.\nkubectl uncordon node01 Cluster Upgrade Process Supported versions gcp kubeadm kubectl upgrade plan\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 root@controlplane:~## kubeadm upgrade plan [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with \u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39; [preflight] Running pre-flight checks. [upgrade] Running cluster health checks [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.19.0 [upgrade/versions] kubeadm version: v1.19.0 I0522 08:54:38.911002 21647 version.go:252] remote version is much newer: v1.21.1; falling back to: stable-1.19 [upgrade/versions] Latest stable version: v1.19.11 [upgrade/versions] Latest stable version: v1.19.11 [upgrade/versions] Latest version in the v1.19 series: v1.19.11 [upgrade/versions] Latest version in the v1.19 series: v1.19.11 Components that must be upgraded manually after you have upgraded the control plane with \u0026#39;kubeadm upgrade apply\u0026#39;: COMPONENT CURRENT AVAILABLE kubelet 2 x v1.19.0 v1.19.11 Upgrade to the latest version in the v1.19 series: COMPONENT CURRENT AVAILABLE kube-apiserver v1.19.0 v1.19.11 kube-controller-manager v1.19.0 v1.19.11 kube-scheduler v1.19.0 v1.19.11 kube-proxy v1.19.0 v1.19.11 CoreDNS 1.7.0 1.7.0 etcd 3.4.9-1 3.4.9-1 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.19.11 Note: Before you can perform this upgrade, you have to update kubeadm to v1.19.11. _____________________________________________________________________ The table below shows the current state of component configs as understood by this version of kubeadm. Configs that have a \u0026#34;yes\u0026#34; mark in the \u0026#34;MANUAL UPGRADE REQUIRED\u0026#34; column require manual config upgrade or resetting to kubeadm defaults before a successful upgrade can be performed. The version to manually upgrade to is denoted in the \u0026#34;PREFERRED VERSION\u0026#34; column. API GROUP CURRENT VERSION PREFERRED VERSION MANUAL UPGRADE REQUIRED kubeproxy.config.k8s.io v1alpha1 v1alpha1 no kubelet.config.k8s.io v1beta1 v1beta1 no _____________________________________________________________________ kubectl upgrade apply\nKubeadm Strategy strategy-1 all node down and up strategy-2 upgrade node one by one strategy-3 add new version node and remove old node easy in cluster Procedure master node ver 1 apt-get upgrade -y kuebadm=1.12.0-00 kubeadm upgrade apply v1.12.0 apt-get upgrade -y kubelet=1.12.0-00 systemctl restart kubelet ver 2 apt update apt install kubeadm=1.20.0-00 kubeadm upgrade apply v1.20.0 apt install kubelet=1.20.0-00 systemctl restart kubelet worker node ver 1 kubectl drain node01 apt-get upgrade -y kuebadm=1.12.0-00 apt-get upgrade -y kubelet=1.12.0-00 kubeadm upgrade node config --kubelet-version v1.12.0 systemctl restart kubelet kubectl uncordon node01 ver 2 apt update apt install kubeadm=1.20.0-00 kubeadm upgrade node apt install kubelet=1.20.0-00 systemctl restart kubelet Backup and Restore Backup Candidates Resource Configuration ETCD Cluster Persistent Volumes Resource Configuration kube-apiserver kubectl get all -A -o yaml \u0026gt; all-deploy-svc.yaml too many resource to do → opensource like VELERO ETCD method 1\nExecStart= ... \\\\ --data-dir=/var/lib/etct method 2\nETCDCTL_API=3 etcdctl snapshot save snapshot.db servcie kube-apiserver stop ETCDCTL_API=3 etcdctl snapshot --data-dir /var/lib/etcd-from-backup snapshot restore snapshot.db systemctl daemon-reload service etcd restart service kube-apiserver start backup practice\n1 2 3 4 5 ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 \\\\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\\ --cert=/etc/kubernetes/pki/etcd/server.crt \\\\ --key=/etc/kubernetes/pki/etcd/server.key \\\\ snapshot save /opt/snapshot-pre-boot.db restore practice\nRestore snapshot 1 2 ETCDCTL_API=3 etcdctl --data-dir /var/lib/etcd-from-backup \\\\ snapshot restore /opt/snapshot-pre-boot.db update the /etc/kubernetes/manifests/etcd.yaml 1 2 3 4 5 volumes: - hostPath: path: /var/lib/etcd-from-backup type: DirectoryOrCreate name: etcd-data ","date":"2021-05-22T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-05.-cluster-maintenance/","title":"(CKA) 05. Cluster Maintenance"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Rollout and Versioning Rollout Command kubectl rollout status deployment/myapp-deployment kubectl rollout history deployment/myapp-deployment Deployment Strategy Recreate destruction 5 create 5 Rolling Update → default strategy\ndestruction and create one by one Revision kubectl apply -f deployment.yaml kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 Upgrades make new replicaset when upgrade deploy Rollback kubectl rollout undo deployment/myapp-deployment before rollback vs after rollback Commands and Arguments docker\n1 2 3 From Ubuntu Entrypoint [\u0026#34;sleep\u0026#34;] cmd [\u0026#34;5\u0026#34;] definition\n1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper image: ubuntu-sleeper command: [\u0026#34;sleep2.0\u0026#34;] args: [\u0026#34;10\u0026#34;] Environment plain Key Value\n1 2 3 env: - name: APP_COLOR value: pink configMap\n1 2 3 4 env: - name: APP_COLOR valueFrom: configMapKeyRef: Secrets\n1 2 3 4 env: - name: APP_COLOR valueFrom: secretKeyRef: ConfigMaps create ConfigMap Inject into pod create imperative\n1 2 3 kubectl create configmap\\\\ app-config --from-literal=APP_COLOR=blue \\\\ --from-literal=APP_MODE=prod kubectl create configmap app-config --from-file=\u0026lt;path-to-file\u0026gt; declartive\n1 2 3 4 5 6 7 apiVersion: v1 kind: ConfigMap metadata: name: app-config data: APP_COLOR: blue APP_MODE: prod view kubectl get configmaps kubectl describe configmaps ConfigMap in Pods 1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color envFrom: - configMapRef: name: app-config Secrets create secret inject into pod create imperative\n1 2 kubectl create secret generic\\\\ \u0026lt;secret-name\u0026gt; --from-literal=\u0026lt;key\u0026gt;=\u0026lt;value\u0026gt; kubectl create secret \u0026lt;secret-name\u0026gt; --from-file=\u0026lt;path-to-file\u0026gt; declartive\n1 2 3 4 5 6 7 8 apiVersion: v1 kind: Secret metadata: name: app-secret data: DB_Host: mysql DB_User: root DB_Passwird: paswrd data → encoded format for safe echo -n 'mysql' | base64 view kubectl get secrets kubectl describe secrets decode echo -n 'abalksdfas=' | base54 --decode Secrets in Pods defintion\n1 2 3 4 5 6 7 8 9 10 11 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color envFrom: - configMapRef: name: app-secret env\n1 2 3 envFrom: - secretRef: name: app-config single env\n1 2 3 4 5 6 env: - name: DB_Password valueFrom: secretKeyRef: name: app-secret key: DB_Password volume\n1 2 3 4 volumes: - name: app-secret-volumne secret: secretName: app-secret inside the container list 1 ls /opt/app-secret-volumes content 1 cat /opt/app-secret-volumes/DB_Password InitContainers In multi-container pod, want to run a process that runs to completion in a container\ninitContainer\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] initContainers: - name: init-myservice image: busybox command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;git clone \u0026lt;some-repository-that-will-be-used-by-application\u0026gt; ; done;\u0026#39;] multiple initContainers\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo The app is running! \u0026amp;\u0026amp; sleep 3600\u0026#39;] initContainers: - name: init-myservice image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup myservice; do echo waiting for myservice; sleep 2; done;\u0026#39;] - name: init-mydb image: busybox:1.28 command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;until nslookup mydb; do echo waiting for mydb; sleep 2; done;\u0026#39;] ","date":"2021-05-18T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-04.-application-lifecycle-management/","title":"(CKA) 04. Application Lifecycle Management"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Metrics Server In-memory does not store Install minikube minikube addons enable metrics-server others git clone \u0026lt;https://github.con/kubernetes-incubator/metrics-server\u0026gt; kubectl create -f deploy/1.8+ View kubeclt top node kubeclt top pod Managing Application Logs Logs - k8s 로그 보기 w\\ defintion kubectl create -f sample.yaml kubectl logs -f sample.yaml 여러 container kubectl logs -f sample.yaml specific-container ","date":"2021-05-17T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-03.-logging-monitoring/","title":"(CKA) 03. Logging \u0026 Monitoring"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? Node name default: kubectl-scheduler 위 작업이 기본적으로 pod을 node에 할당해 줌 상태 확인 kubectl get node\nnode = no\n1 2 3 4 \u0026gt; k get no NAME STATUS ROLES AGE VERSION controlplane Ready control-plane,master 11m v1.20.0 node01 Ready \u0026lt;none\u0026gt; 9m48s v1.20.0 수동 배정 To schedule node manually NodeName 1 2 3 4 5 6 7 8 9 10 --- apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx nodeName: node01 Label \u0026amp; Selectors Label definition\n1 2 3 4 5 6 7 8 apiVersion: ~~ kind: ~~ metadata: name: ~~ labels: app: App1 function: Front-end sepc: ~~ → on metadata: labels\nimperative\n1 kubectl label node node01 key=value Select kubectl get pods --selector app=App1 several selector field kubectl get pods --selector app=App1,env=dev Replicaset replica-definition.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 apiVersion: apps/v1 kind: Replcaset metadata: name: simple-webapp labels: app: App1 function: Front-end spec: replicas: 3 selector: matchLabels: app: App1 template: metadata: labels: app: App1 function: Front-end spec: containers: - name: simple-webapp image: simple-webapp labels at the top are the labels of the replicaset itself. 1 2 3 4 5 6 7 apiVersion: apps/v1 kind: Replcaset metadata: name: simple-webapp labels: app: App1 function: Front-end in order to connect the replicaset to the pod, configure the selector field 1 2 3 selector: matchLabels: app: App1 Annotations used to record other detail for informatively purpose eg) tool details Taints, Tolerations taints set on the nodes toleartions set on the pods taints and toleration does not tell the pod to go to a particular node node to only accept pod with certain tolerations Taints 설정 kubectl taint nodes node-name key=value:taint-effect taint-effect : NoSchedule not to be scheduled, if they do not tolerate the taint PreferNoSchedule system will be try to avoid placing a pod on the node, not guaranteed NoExecute new pods will not be scheduled on the node existing pods on the node will be evicted if the do not tolertae the taint 확인 describe \u0026amp; grep 1 2 k describe node node01 |grep -i taints Taints: \u0026lt;none\u0026gt; 제거 kubectl taint nodes node-name key=value:taint-effect- 1 kubectl taint nodes master/controlplane node-role.kubernetes.io/master:NoSchedule- Tolerations 설정 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: kind: Pod metadata: name: myapp-pod spec: containers: - name: nginx-container - image: nginx tolerations: - key: \u0026#34;app\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;blue\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; PODs to Node Node Selector Node Affinity Node Selector Label nodes\nuse nodeSelector with label\n1 2 3 4 5 6 7 8 9 10 apiVersion: kind: metadata: name: spec: containers: - name: image: nodeSelector: size: Large limitations\nonly use single label there should be complex constraints Node Affinity nodeSelector vs affinity two are working equally, schedule pods to Large node. 설정 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: kind: metadata: name: spec: containers: - name: image: affinity: nodeAffinity: requireDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: operator: values: operator In guarantee to schedule in values NotIn guarantee not to schedule in values Exists given key exists or not Node Affinity Types available requireDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution planned requireDuringSchedulingRequiredDuringExecution two states in the life cycle of pod DuringScheduling Required must be Preferred best to DuringExecution Ignored if they are scheduled, will not impact them. Required pod will be evicted Resource Limits Resource Requests 설정 1 2 3 4 5 6 7 8 9 10 11 12 apiVersion: kind: metadata: name: spec: containers: - name: image: resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: 1 cpu: 0.1 == 100mi lower: 1mi memory Resources Limits 설정 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion: kind: metadata: name: spec: containers: - name: image: resources: requests: memory: \u0026#34;1Gi\u0026#34; cpu: 1 limits: memory: \u0026#34;2Gi\u0026#34; cpu: 2 default 1v CPU 512Mi Daemon Sets one copy of the pod is always present in all nodes in the cluster use case monitoring kube-proxy networking 생성 DaemonSet vs ReplicaSet definition\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: apps/v1 kind: DaemonSet metadata: name: monitoring-daemon spec: selector: matchLabels: app: monitoring-daemon template: metadata: labels: app: monitoring-daemon spec: contaiers: - name: monitoring-agent image: monitoring-agent 상태 kubectl get daemonsets Static PODs Static PODs vs DaemonSets static pod created directly by kubelet Config --pod-manifeset-path=/etc/Kubernetes/manifest --config=kubeconfig.yaml 1 2 # kubeconfig.yaml staticPodPath: /etc/Kubernetes/manifest 파일 위치 First idenity the kubelet config file:\n1 2 3 4 root@controlplane:~# ps -aux | grep /usr/bin/kubelet root 3668 0.0 1.5 1933476 63076 ? Ssl Mar13 16:18 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 root 4879 0.0 0.0 11468 1040 pts/0 S+ 00:06 0:00 grep --color=auto /usr/bin/kubelet root@controlplane:~# From the output we can see that the kubelet config file used is /var/lib/kubelet/config.yaml\nNext, lookup the value assigned for staticPodPath:\n1 2 3 root@controlplane:~# grep -i staticpod /var/lib/kubelet/config.yaml staticPodPath: /etc/kubernetes/manifests root@controlplane:~# As you can see, the path configured is the /etc/kubernetes/manifests directory.\n상태 kubectl get po -A -controlplane appended pods 1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; k get po -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system coredns-74ff55c5b-578df 1/1 Running 0 25m kube-system coredns-74ff55c5b-nnjdp 1/1 Running 0 25m kube-system etcd-controlplane 1/1 Running 0 25m kube-system kube-apiserver-controlplane 1/1 Running 0 25m kube-system kube-controller-manager-controlplane 1/1 Running 0 25m kube-system kube-flannel-ds-q7plw 1/1 Running 0 24m kube-system kube-flannel-ds-w5rnm 1/1 Running 0 25m kube-system kube-proxy-5dg8f 1/1 Running 0 24m kube-system kube-proxy-ld2qq 1/1 Running 0 25m kube-system kube-scheduler-controlplane 1/1 Running 0 25m grep -i controlplane 1 2 3 4 5 \u0026gt; k get po -A |grep -i controlplane kube-system etcd-controlplane 1/1 Running 0 27m kube-system kube-apiserver-controlplane 1/1 Running 0 27m kube-system kube-controller-manager-controlplane 1/1 Running 0 27m kube-system kube-scheduler-controlplane 1/1 Running 0 27m 삭제 First, let\u0026rsquo;s identify the node in which the pod called static-greenbox is created. To do this, run:\n1 2 3 root@controlplane:~# kubectlget pods --all-namespaces -o wide | grep static-greenbox defaultstatic-greenbox-node01 1/1Running 0 19s 10.244.1.2 node01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; root@controlplane:~# From the result of this command, we can see that the pod is running on node01.\nNext, SSH to node01 and identify the path configured for static pods in this node.\nImportant: The path need not be /etc/kubernetes/manifests.\nMake sure to check the path configured in the kubelet configuration file.\n1 2 3 4 5 6 7 root@controlplane:~# ssh node01 root@node01:~# ps -ef | grep /usr/bin/kubelet root 752 654 0 00:30 pts/0 00:00:00 grep --color=auto /usr/bin/kubelet root 28567 1 0 00:22 ? 00:00:11 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --network-plugin=cni --pod-infra-container-image=k8s.gcr.io/pause:3.2 root@node01:~# grep -i staticpod /var/lib/kubelet/config.yaml staticPodPath: /etc/just-to-mess-with-you root@node01:~# Here the staticPodPath is /etc/just-to-mess-with-you\nNavigate to this directory and delete the YAML file:\n1 2 3 4 root@node01:/etc/just-to-mess-with-you# ls greenbox.yaml root@node01:/etc/just-to-mess-with-you# rm -rf greenbox.yaml root@node01:/etc/just-to-mess-with-you# Exit out of node01 using CTRL + D or type exit. You should return to the controlplane node. Check if the static-greenbox pod has been deleted:\n1 2 root@controlplane:~# kubectl get pods --all-namespaces -o wide | grep static-greenbox root@controlplane:~# Multiple Schedulers 생성 cli kubeadm --leader-elect used when multiple copies of the scheduler running on different master nodes who will lead activity --scheduler-name name of scheduler --lock-object-name 사용 definition 1 2 3 4 5 6 7 8 9 apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx schedulerName: my-custom-scheduler 상태 kubectl get events kubectl logs my-custom-scheduler --n kube-system ","date":"2021-05-16T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-02.-scheduler/","title":"(CKA) 02. Scheduler"},{"content":"이번 포스트에서는 Ambassador를 사용하는 Seldon Core를 설치하는 방법에 대해서 설명합니다.\nIstio를 사용해 설치하려면 다음 글을 확인해 주세요.\nPre-requisites 이번 실습에서는 Local에서 Seldon Core를 설치후 사용해보기 위한 k8s Tool로 minikube를 사용합니다.\nMinikube Helm Ingress Istio 1. Minikube 사용한 minikube version은 다음과 같습니다.\n1 2 3 \u0026gt; minikube version minikube version: v1.20.0 commit: c61663e942ec43b20e8e70839dcca52e44cd85ae minikube의 default config는 memory의 경우 2048mb 입니다. 이 경우 실습 중 메모리가 부족해서 OOM이슈가 생겨서 정상적으로 진행이 어려울 수 있습니다.\nOOM을 방지하기 위해서 minikube의 메모리를 4096mb로 늘린 후 진행하도록 하겠습니다.\n1 minikube config set memory 4096 minikube를 실행합니다.\n1 minikube start 2. Helm helm 공식 홈페이지의 방법을 따라 합니다. 이 때 설치해야 하는 버전은 3.x.x 입니다.\n스크립트 방법을 이용해 설치하겠습니다.\n1 2 3 curl -fsSL -o get_helm.sh \u0026lt;https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\u0026gt; chmod 700 get_helm.sh ./get_helm.sh 3. Ambassador helm chart를 이용해 설치합니다.\n1 2 3 4 5 6 7 8 kubectl create namespace ambassador || echo \u0026#34;namespace ambassador exists\u0026#34; helm repo add datawire \u0026lt;https://www.getambassador.io\u0026gt; helm install ambassador datawire/ambassador \\\\ --set image.repository=quay.io/datawire/ambassador \\\\ --set enableAES=false \\\\ --set crds.keep=false \\\\ --namespace ambassador 상태를 확인합니다.\n1 kubectl get all -n ambassador 다음과 같이 ambassador와 관련된 리소스들이 생성되었습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 NAME READY STATUS RESTARTS AGE pod/ambassador-5bf7689fc6-9hlrd 1/1 Running 0 116s pod/ambassador-5bf7689fc6-bk86c 1/1 Running 0 116s pod/ambassador-5bf7689fc6-r6v9q 1/1 Running 0 116s pod/ambassador-agent-8585f84d86-vbczf 1/1 Running 0 116s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/ambassador LoadBalancer 10.102.231.216 \u0026lt;pending\u0026gt; 80:30904/TCP,443:32156/TCP 116s service/ambassador-admin ClusterIP 10.99.228.85 \u0026lt;none\u0026gt; 8877/TCP,8005/TCP 116s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/ambassador 3/3 3 3 116s deployment.apps/ambassador-agent 1/1 1 1 116s NAME DESIRED CURRENT READY AGE replicaset.apps/ambassador-5bf7689fc6 3 3 3 116s replicaset.apps/ambassador-agent-8585f84d86 1 1 1 116s Seldon-core 1. Install seldon-core namespace 생성\n1 kubectl create namespace seldon-system helm chart 생성\n1 2 3 4 5 helm install seldon-core seldon-core-operator \\\\ --repo \u0026lt;https://storage.googleapis.com/seldon-charts\u0026gt; \\\\ --set usageMetrics.enabled=true \\\\ --namespace seldon-system \\\\ --set istio.enabled=true 상태 확인\n1 kubectl get po -n seldon-system 다음과 같이 출력됩니다.\n1 2 NAME READY STATUS RESTARTS AGE seldon-controller-manager-559c567c9-pjtpl 1/1 Running 0 11s 2. Sample deploy namespace 생성\n1 kubectl create namespace seldon pod 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kubectl apply -f - \u0026lt;\u0026lt; END apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: iris-model namespace: seldon spec: name: iris predictors: - graph: implementation: SKLEARN_SERVER modelUri: gs://seldon-models/sklearn/iris name: classifier name: default replicas: 1 END pod 생성 확인\n1 kubectl get all -n seldon 다음과 같이 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 NAME READY STATUS RESTARTS AGE pod/iris-model-default-0-classifier-546fb8bfff-cd5gm 2/2 Running 0 5m47s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/iris-model-default ClusterIP 10.98.254.42 \u0026lt;none\u0026gt; 8000/TCP,5001/TCP 71s service/iris-model-default-classifier ClusterIP 10.101.118.187 \u0026lt;none\u0026gt; 9000/TCP,9500/TCP 5m47s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/iris-model-default-0-classifier 1/1 1 1 5m47s NAME DESIRED CURRENT READY AGE replicaset.apps/iris-model-default-0-classifier-546fb8bfff 1 1 1 5m47s minikube tunnel\n1 minikube tunnel 다음과 같은 출력을 얻습니다.\n1 2 3 4 5 6 7 8 9 10 Status: machine: minikube pid: 117019 route: 10.96.0.0/12 -\u0026gt; 192.168.49.2 minikube: Running services: [istio-ingressgateway] errors: minikube: no errors router: no errors loadbalancer emulator: no errors ambassador external ip 확인하기\n1 k get svc -n ambassador 1 2 3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE ambassador LoadBalancer 10.102.231.216 10.102.231.216 80:30904/TCP,443:32156/TCP 5m6s ambassador-admin ClusterIP 10.99.228.85 \u0026lt;none\u0026gt; 8877/TCP,8005/TCP 5m6s → 이 경우 10.102.231.216\npredict 요청하기\n1 2 3 curl -X POST \u0026lt;http://10.102.231.216/seldon/seldon/iris-model/api/v1.0/predictions\u0026gt; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [[1,2,3,4]] } }\u0026#39; predict 요청을 하는 format은 다음과 같습니다. http://\u0026lt;AMBASSADOR_URL\u0026gt;/seldon/\u0026lt;NAMESPACE\u0026gt;/\u0026lt;MODEL-NAME\u0026gt;/api/v1.0/doc/ 다음과 같은 결과를 얻을 수 있습니다. 1 {\u0026#34;data\u0026#34;:{\u0026#34;names\u0026#34;:[\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;],\u0026#34;ndarray\u0026#34;:[[0.0006985194531162841,0.003668039039435755,0.9956334415074478]]},\u0026#34;meta\u0026#34;:{\u0026#34;requestPath\u0026#34;:{\u0026#34;classifier\u0026#34;:\u0026#34;seldonio/sklearnserver:1.7.0\u0026#34;}}} Seldon-analytics 1. Install helm chart를 이용해 설치합니다.\n1 2 3 helm install seldon-core-analytics seldon-core-analytics \\\\ --repo \u0026lt;https://storage.googleapis.com/seldon-charts\u0026gt; \\\\ --namespace seldon-system 2.Usage 설치된 seldon-core-analytics port-forward 합니다.\n1 kubectl port-forward svc/seldon-core-analytics-grafana 3000:80 -n seldon-system http://localhost:3000 에 접속합니다.\n기본 ID/PW는 다음과 같습니다. ID: admin PW: password dashboard에서 prediction analytics를 클릭합니다 다음과 같은 dashboard를 볼 수 있습니다. ","date":"2021-05-14T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-seldon-core-%EC%84%A4%EC%B9%98-ambassador-ver./","title":"밑바닥부터 시작하는 Seldon-Core - 설치 [Ambassador Ver.]"},{"content":"이번 포스트에서는 Istio를 사용하는 Seldon Core를 설치하는 방법에 대해서 설명합니다.\nAmbassador를 사용해 설치하려면 다음 글을 확인해 주세요.\nPre-requisites 이번 실습에서는 Local에서 Seldon Core를 설치후 사용해보기 위한 k8s Tool로 minikube를 사용합니다.\nMinikube Helm Ingress Istio 1. Minikube 사용한 minikube version은 다음과 같습니다.\n1 2 3 \u0026gt; minikube version minikube version: v1.20.0 commit: c61663e942ec43b20e8e70839dcca52e44cd85ae minikube의 default config는 memory의 경우 2048mb 입니다. 이 경우 실습 중 메모리가 부족해서 OOM이슈가 생겨서 정상적으로 진행이 어려울 수 있습니다.\nOOM을 방지하기 위해서 minikube의 메모리를 4096mb로 늘린 후 진행하도록 하겠습니다.\n1 minikube config set memory 4096 minikube를 실행합니다.\n1 minikube start 2. Helm helm 공식 홈페이지의 방법을 따라 합니다. 이 때 설치해야 하는 버전은 3.x.x 입니다.\n스크립트 방법을 이용해 설치하겠습니다.\n1 2 3 curl -fsSL -o get_helm.sh \u0026lt;https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\u0026gt; chmod 700 get_helm.sh ./get_helm.sh 3. Ingress minikube에서 ingress addon을 사용합니다.\n1 minikube addons enable ingress 4. Istio Istio 공식홈페이지 가이드를 따라합니다.\n4.1 Download Istio 스크립트를 이용해 다운로드합니다. ```bash curl -L \u0026lt;https://istio.io/downloadIstio\u0026gt; | sh - cd istio-1.9.5 export PATH=$PWD/bin:$PATH ``` 4.2 Install Istio istioctl을 이용해 설치합니다. ```bash istioctl install --set profile=demo -y ``` 4.3 istio-injection default namespace에 istio-injection을 시켜서 istio-system이 생성되게 합니다. ```bash kubectl label namespace default istio-injection=enabled ``` 4.4.ingress ip and ports ingress port 설정하기 1 2 export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39;) export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;https\u0026#34;)].nodePort}\u0026#39;) Ingress port 확인하기 1 echo \u0026#34;$INGRESS_PORT\u0026#34; 저는 다음과 같은 출력을 얻었습니다. 1 30687 Secure Ingress port 확인하기 1 echo \u0026#34;$SECURE_INGRESS_PORT\u0026#34; 저는 다음과 같은 출력을 얻었습니다. 1 30520 Ingress ip 설정하기 1 export INGRESS_HOST=$(minikube ip) ip 설정 확인하기 1 echo \u0026#34;$INGRESS_HOST\u0026#34; 저는 다음과 같은 출력을 얻었습니다. 1 192.168.49.2 Seldon-core 1. Install seldon-core namespace 생성\n1 kubectl create namespace seldon-system helm chart 생성\n1 2 3 4 5 helm install seldon-core seldon-core-operator \\\\ --repo \u0026lt;https://storage.googleapis.com/seldon-charts\u0026gt; \\\\ --set usageMetrics.enabled=true \\\\ --namespace seldon-system \\\\ --set istio.enabled=true 상태 확인\n1 kubectl get po -n seldon-system 다음과 같이 출력됩니다.\n1 2 NAME READY STATUS RESTARTS AGE seldon-controller-manager-559c567c9-pjtpl 1/1 Running 0 11s 2. Istio setting istio-ingress.yaml 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # istio-ingress.yaml apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: seldon-gateway namespace: istio-system spec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - \u0026#34;*\u0026#34; 다음과 같은 istio ingress 를 파일 작성 후 apply 합니다.\n1 k apply -f istio-ingress.yaml 3. Sample deploy namespace 생성\n1 kubectl create namespace seldon pod 생성\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 kubectl apply -f - \u0026lt;\u0026lt; END apiVersion: machinelearning.seldon.io/v1 kind: SeldonDeployment metadata: name: iris-model namespace: seldon spec: name: iris predictors: - graph: implementation: SKLEARN_SERVER modelUri: gs://seldon-models/sklearn/iris name: classifier name: default replicas: 1 END minikube tunnel\n1 minikube tunnel 다음과 같은 출력을 얻습니다.\n1 2 3 4 5 6 7 8 9 10 Status: machine: minikube pid: 117019 route: 10.96.0.0/12 -\u0026gt; 192.168.49.2 minikube: Running services: [istio-ingressgateway] errors: minikube: no errors router: no errors loadbalancer emulator: no errors ingress external ip 확인\n1 k get svc -n istio-system 1 2 3 4 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT (S) AGE istio-egressgateway ClusterIP 10.104.239.89 \u0026lt;none\u0026gt; 80/TCP,443/TCP,15443/ TCP 7m43s istio-ingressgateway LoadBalancer 10.111.24.207 10.111.24.207 15021:30945/TCP,80:30687/TCP,443:30520/TCP,31400:30072/TCP,15443:32717/ TCP 7m43s istiod ClusterIP 10.99.171.243 \u0026lt;none\u0026gt; 15010/TCP,15012/TCP,443/TCP,15014/ TCP 7m58s → 이 경우 10.111.24.207\npredict 요청하기\n1 2 3 curl -X POST \u0026lt;http://10.111.24.207/seldon/seldon/iris-model/api/v1.0/predictions\u0026gt; \\\\ -H \u0026#39;Content-Type: application/json\u0026#39; \\\\ -d \u0026#39;{ \u0026#34;data\u0026#34;: { \u0026#34;ndarray\u0026#34;: [[1,2,3,4]] } }\u0026#39; predict 요청을 하는 format은 다음과 같습니다. http://\u0026lt;INGRESS_URL\u0026gt;/seldon/\u0026lt;NAMESPACE\u0026gt;/\u0026lt;MODEL-NAME\u0026gt;/api/v1.0/doc/ 다음과 같은 결과를 얻을 수 있습니다.\n1 {\u0026#34;data\u0026#34;:{\u0026#34;names\u0026#34;:[\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;],\u0026#34;ndarray\u0026#34;:[[0.0006985194531162841,0.003668039039435755,0.9956334415074478]]},\u0026#34;meta\u0026#34;:{\u0026#34;requestPath\u0026#34;: {\u0026#34;classifier\u0026#34;:\u0026#34;seldonio/sklearnserver:1.7.0\u0026#34;}}} Seldon-analytics 1. Install helm chart를 이용해 설치합니다.\n1 2 3 helm install seldon-core-analytics seldon-core-analytics \\\\ --repo \u0026lt;https://storage.googleapis.com/seldon-charts\u0026gt; \\\\ --namespace seldon-system 2.Usage 설치된 seldon-core-analytics port-forward 합니다.\n1 kubectl port-forward svc/seldon-core-analytics-grafana 3000:80 -n seldon-system http://localhost:3000 에 접속합니다.\n기본 ID/PW는 다음과 같습니다. ID: admin PW: password dashboard에서 prediction analytics를 클릭합니다 다음과 같은 dashboard를 볼 수 있습니다. ","date":"2021-05-14T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-seldon-core-%EC%84%A4%EC%B9%98-istio-ver./","title":"밑바닥부터 시작하는 Seldon-Core - 설치 [Istio Ver.]"},{"content":"CKA를 준비하면서 공부한 요약 내용입니다.\n강의 What is CKA? 0. Tips ailias kubectl to k 1 2 alias k=\u0026#34;kubectl\u0026#34; k get po shortcuts pod = po service = svc namespace = ns replicasets = rs do not write all config file use dry run kubectl run nginx --image=nginx --dry-run=client -o yaml kubectl create deployment --image=nginx nginx --dry-run=client -o yaml export it to yaml file kubectl create deployment --image=nginx nginx --dry-run=client -o yaml \u0026gt; nginx-deployment.yaml 1. Pods 생성 yaml 1 2 3 4 5 6 7 8 9 # pod.yaml apiVersion: v1 kind: pod metadata: name: nginx spec: containers: - name: nginx-container image: nginx 1 kubectl apply -f pod.yaml run 1 kubectl run nginx --image=nginx 상태 기본\n1 2 3 4 5 6 7 \u0026gt; k get po NAME READY STATUS RESTARTS AGE newpods-6nl8r 1/1 Running 0 3m newpods-9sp8p 1/1 Running 0 3m newpods-tplx9 1/1 Running 0 3m nginx 1/1 Running 0 3m11s webapp 1/2 ImagePullBackOff 0 54s READY means running_containers in pod / total_containers in pod wide\n1 2 3 4 5 6 \u0026gt; k get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES newpods-6nl8r 0/1 ContainerCreating 0 14s \u0026lt;none\u0026gt; controlplane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; newpods-9sp8p 0/1 ContainerCreating 0 14s \u0026lt;none\u0026gt; controlplane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; newpods-tplx9 0/1 ContainerCreating 0 14s \u0026lt;none\u0026gt; controlplane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; nginx 0/1 ContainerCreating 0 25s \u0026lt;none\u0026gt; controlplane \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; NODE describe\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 \u0026gt; k describe po webapp Name: webapp Namespace: default Priority: 0 Node: controlplane/10.77.67.9 Start Time: Mon, 10 May 2021 03:54:38 +0000 Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Pending IP: 10.244.0.8 IPs: IP: 10.244.0.8 Containers: nginx: Container ID: docker://f9580f7e4b7c51b53f0cb0d94ff913b643706a65a103a58614c3c254cf26043f Image: nginx Image ID: docker-pullable://nginx@sha256:75a55d33ecc73c2a242450a9f1cc858499d468f077ea942867e662c247b5e412 Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Running Started: Mon, 10 May 2021 03:54:41 +0000 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-pz92t (ro) agentx: Container ID: Image: agentx Image ID: Port: \u0026lt;none\u0026gt; Host Port: \u0026lt;none\u0026gt; State: Waiting Reason: ImagePullBackOff Ready: False Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-pz92t (ro) Conditions: Type Status Initialized True Ready False ContainersReady False PodScheduled True Volumes: default-token-pz92t: Type: Secret (a volume populated by a Secret) SecretName: default-token-pz92t Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 17s default-scheduler Successfully assigned default/webapp to controlplane Normal Pulling 15s kubelet Pulling image \u0026#34;nginx\u0026#34; Normal Pulled 15s kubelet Successfully pulled image \u0026#34;nginx\u0026#34; in 170.242045ms Normal Created 15s kubelet Created container nginx Normal Started 14s kubelet Started container nginx Normal Pulling 14s kubelet Pulling image \u0026#34;agentx\u0026#34; Warning Failed 13s kubelet Failed to pull image \u0026#34;agentx\u0026#34;: rpc error: code = Unknown desc = Error response from daemon: pull access denied for agentx, repository does not exist or may require \u0026#39;docker login\u0026#39;: denied: requested access to the resource is denied Warning Failed 13s kubelet Error: ErrImagePull Normal BackOff 12s (x2 over 13s) kubelet Back-off pulling image \u0026#34;agentx\u0026#34; Warning Failed 12s (x2 over 13s) kubelet Error: ImagePullBackOff 삭제 1 2 \u0026gt; k delete po webapp pod \u0026#34;webapp\u0026#34; deleted 수정 1 k edit po redis 2. Replication Controller What is replica and why need controller?\n-\u0026gt; replication controller runs multiple instances of a single pod in the k8s cluster\n특징 high availability multiplie pod or single pod replication controller ensures that the specified number of pods are running at all times. Load Balancing \u0026amp; Scaling balance the load spans across multiple nodes 종류 replication controller old technology replica set recommended 생성 defintion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # ReplicaSet apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: myapp-pod name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx replicas: 3 selector: matchLabels: type: front-end 상태 1 2 3 \u0026gt; k get replicasets NAME DESIRED CURRENT READY AGE new-replica-set 4 4 0 11s 수정 1 2 \u0026gt; k edit replicaset replicaset.apps/new-replica-set edited 삭제 1 \u0026gt; k delete po --all -\u0026gt; 자동으로 다시 pod 생성 됨\n1 2 3 4 5 6 7 8 9 10 ot@controlplane:~# k get po NAME READY STATUS RESTARTS AGE new-replica-set-7mx8k 0/1 Terminating 0 4m22s new-replica-set-7p95n 0/1 ContainerCreating 0 17s new-replica-set-dcl5m 0/1 Terminating 0 4m22s new-replica-set-hhpm9 0/1 ContainerCreating 0 17s new-replica-set-hr54x 0/1 ContainerCreating 0 17s new-replica-set-m6dmb 0/1 Terminating 0 4m22s new-replica-set-v2vr5 0/1 ContainerCreating 0 17s new-replica-set-vmts5 0/1 Terminating 0 4m22s scale replace w\\ file 1 kubectl replace -f ~~.yaml scale w\\ file 1 kubectl scale --replicas=6 -f ~~.yaml scale w\\ resource 1 kubectl scale --replicas=6 replicaset myapp-replicaset edit 1 kubectl edit replicaset 실습 1 2 \u0026gt; k scale --replicas=5 rs new-replica-set replicaset.apps/new-replica-set scaled 3. Deployments rolling updates rolling back 4. Namespaces user의 실수부터 보호하기 위해서 처음 3개의 namespace가 생성됨 Default kube-system kube-public isolate resources between namespace own policies each namespace is guaranteed certain amount and does not to use more. to reach in same namespace mysql.connect(\u0026quot;db-service\u0026quot;) to reach in other namespace mysql.connect(db-service.dev.svc.cluster.local) {service-name}.{namespace}.{service}-{domain-name} 생성 1 2 k create ns dev-ns namespace/dev-ns created 목록 1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; k get ns NAME STATUS AGE default Active 88s dev Active 34s finance Active 34s kube-node-lease Active 93s kube-public Active 94s kube-system Active 95s manufacturing Active 33s marketing Active 34s prod Active 33s research Active 33s 상태 --namespace 1 kubectl get pods --namespace=kube-system -n 1 kubectl get pods -n kube-system change default 1 kubectl config set-context $(kubectl config current-context) --namespace=dev pod 생성 1 2 k run redis --image=redis --namespace=finance pod/redis created 전체 확인 1 k get po --all-namespaces 하나의 값만 찾고 싶을 때\n1 k get po --all-namespaces | grep -i blue 5. Services allow to communicate w\\ people, backend, frontend, extra data source\ntypes\nnode port make an internal pod accessible on a port on node cluster ip creates a virtual IP inside the cluster to enable communication b2n different services load balancer provisions a load balancer for service for cloud provider NodePort\ntarget port port port of service itself node port valid range: 30000 ~ 32767 1 2 3 4 5 6 7 8 9 10 apiVersion: v1 kind: SErvice metadata: name: myapp-service spec: type: NodePort ports: - targetPort: 80 port: 80 nodePort: 30008 if targetPort is not given same as port if nodePort is not given automatically allocated randomly distributed if same port is exists ClusterIP LoadBalancer\n상태 1 2 3 4 \u0026gt; k get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 3m46s 명령어 1 \u0026gt; kubectl expose pod redis --port=6379 --name redis-service expose 1 2 \u0026gt; kubectl run custom-nginx --image=nginx --port=8080 kubectl run httpd --image=httpd:alpine --port=80 --expose 6. Imperative vs Declaritive Infrastructure as Code Kubernetes Imperative Commands\ncreate objects update objects → hard to keep track Imperative Configuration Files\ncreate objects kubectl create -f ~.yaml update obejcts kubectl edit ~~ ~~ kubectl replace -f ~.yaml Declaritve\ncreate objects kubectl apply -f ~.yaml kubectl apply -f /path/to/config-files update objects kubectl apply -f ~.yaml ","date":"2021-05-10T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/cka-01.-core-concepts/","title":"(CKA) 01. Core Concepts"},{"content":"이번 포스트는 Pandas를 이용해 tf-idf를 계산하는 방법에 대해서 설명합니다.\nTF-IDF TF(Term Frequency): 한 문장에서 나타난 각 단어들의 빈도 IDF(Inverse Document Frequency): 문서들에 나타난 단어들의 빈도의 역수 TF-IDF: TF*IDF Data 데이터는 이미 빈도수가 계산되어 있다고 가정합니다.\n1 2 3 4 5 6 7 8 9 10 11 data = [ [0, 0, 1, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 2, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 2], [0, 0, 0, 0, 0, 0, 0, 3, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] ] columns = [ f\u0026#34;feature_{0}\u0026#34; for i in range(10) ] sample_data = pd.DataFrame(data, columns=columns) 데이터는 다음과 같습니다.\nfeature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 2 1 0 0 0 0 2 0 0 0 0 0 0 0 0 0 2 3 0 0 0 0 0 0 0 3 0 0 4 0 0 0 0 0 0 0 0 0 0 TF(Term Frequency) 각 문장은 하나의 row입니다. 우선, 각 문장에서 나타난 전체 단어의 개수를 구합니다.\n1 sample_df.sum(axis=1) 계산하면 다음과 같습니다.\n0 0 2 1 3 2 2 3 3 4 0 이 값을 각 row에 나눠줍니다.\n1 tf = sample_df.div(sample_df.sum(axis=1), axis=0) 계산하면 다음과 같습니다.\nfeature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 0 0 0 0.5 0 0.5 0 0 0 0 0 1 0 0 0 0 0.666667 0.333333 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1 3 0 0 0 0 0 0 0 1 0 0 4 nan nan nan nan nan nan nan nan nan nan 5번째 문장의 경우 단어의 합이 0 이기 때문에 nan값이 생겼습니다. 이를 처리하기 위해 na 값에 0을 채워줍니다.\n1 tf = tf.fillna(0) 전체 단어의 빈도수로 나누었기 때문에 tf의 각 row의 합은 1과 같아야 합니다.\n1 tf.sum(axis=1) 0 0 1 1 1 2 1 3 1 4 0 IDF(Inverse Document Frequency) 각 단어는 하나의 column입니다. 다음으로 각 단어들이 문장들에서 나타난 개수를 구합니다.\n1 (sample_df != 0).sum(axis=0) 0 feature_0 0 feature_1 0 feature_2 1 feature_3 0 feature_4 2 feature_5 1 feature_6 0 feature_7 1 feature_8 0 feature_9 1 이제 이 값을 각 column에 나눠준 후 log를 취합니다.\n1 idf = np.log(len(sample_df) / (sample_df != 0).sum(axis=0)) idf값은 다음과 같습니다.\n0 feature_0 inf feature_1 inf feature_2 1.60944 feature_3 inf feature_4 0.916291 feature_5 1.60944 feature_6 inf feature_7 1.60944 feature_8 inf feature_9 1.60944 0으로 나눠서 inf값이 생긴 곳에 0을 채워줍니다.\n1 idf = idf.replace(np.inf, 0) 0 feature_0 0 feature_1 0 feature_2 1.60944 feature_3 0 feature_4 0.916291 feature_5 1.60944 feature_6 0 feature_7 1.60944 feature_8 0 feature_9 1.60944 TF-IDF 이제 위에서 계산한 TF와 IDF를 곱해줍니다.\n1 tf_idf = tf * idf 다음과 같이 tf-idf를 구할 수 있습니다.\nfeature_0 feature_1 feature_2 feature_3 feature_4 feature_5 feature_6 feature_7 feature_8 feature_9 0 0 0 0.804719 0 0.458145 0 0 0 0 0 1 0 0 0 0 0.61086 0.536479 0 0 0 0 2 0 0 0 0 0 0 0 0 0 1.60944 3 0 0 0 0 0 0 0 1.60944 0 0 4 0 0 0 0 0 0 0 0 0 0 Reference https://ko.wikipedia.org/wiki/Tf-idf ","date":"2021-05-05T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/pandas%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-tf-idf-%EA%B5%AC%ED%95%98%EA%B8%B0/","title":"Pandas를 이용한 tf-idf 구하기"},{"content":" problem number: 13023 link: https://www.acmicpc.net/problem/13023 Define input, output Input: 사람 N과 M개의 친구 관계가 들어온다 Output: 5명이 연속해서 친구 관계가 있는지 확인한다. 설명 재귀 DFS로 풀어야 시간 초과 하지 않음! 재귀로 visit 을 1로 바꿔주고 해결되지 않고 재귀에서 나올 경우 다시 0 으로 돌려주어야 함.\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 import sys N, M = map(int, sys.stdin.readline().strip().split(\u0026#34; \u0026#34;)) relation = {} for _ in range(M): a, b = map(int, sys.stdin.readline().strip().split(\u0026#34; \u0026#34;)) relation.setdefault(a, []) relation.setdefault(b, []) relation[a] += [b] relation[b] += [a] stack = [[i] for i in relation.keys()] visit = [False] * N def dfs(cur, depth): visit[cur] = True if depth == 5: return 1 for friend in relation.get(cur, []): if not visit[friend]: result = dfs(friend, depth + 1) if result: return result visit[friend] = False for i in range(N): result = dfs(i, 1) visit[i] = False if result: break print(result if result else 0) ","date":"2021-04-18T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/abcde/","title":"ABCDE"},{"content":" problem number: 1107 link: https://www.acmicpc.net/problem/1107 Define input, output Input: target 채널, 고장난 스위치 개수, 고장난 스위치 목록 Output: 최소한의 클릭으로 target 채널에 도달하기 설명 신경써야 할 부분\n고장난 스위치 개수가 0이면 3번째 줄은 주어지지 않는다. -\u0026gt; 확인 안할 경우 ValueError 스위치가 전부 고장날 수 있다. -\u0026gt; 방법에 따라 NameError 반례 모음\nhttps://www.acmicpc.net/board/view/46120 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 import sys from itertools import product target = sys.stdin.readline().strip() f = int(sys.stdin.readline()) if 0 \u0026lt; f \u0026lt; 10: fault = sys.stdin.readline().strip().split(\u0026#34; \u0026#34;) remote = [] for i in range(10): if str(i) in fault: pass else: remote.append(str(i)) start_idx = 0 for start_idx, t in enumerate(target): if t not in remote: if start_idx \u0026gt; 0: start_idx -= 1 break def cal(c, target, start_idx): cnt = len(c) + start_idx c = target[:start_idx] + \u0026#34;\u0026#34;.join(c) cnt += abs(int(c) - int(target)) return cnt min_cnt = abs(int(target) - 100) for i in range(1, len(target) - start_idx + 2): comb = product(remote, repeat=i) m = min(map(lambda x: cal(x, target, start_idx), comb)) min_cnt = min(min_cnt, m) print(min_cnt) elif f == 0 : min_cnt = min(len(target), abs(int(target) - 100)) print(min_cnt) else: print(abs(int(target) - 100)) ","date":"2021-04-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%A6%AC%EB%AA%A8%EC%BB%A8/","title":"리모컨"},{"content":" problem number: 3085 link: https://www.acmicpc.net/problem/3085 Define input, output Input: 캔디의 종류가 써 있는 matrix Output: 가장 긴 캔디의 길이 설명 swap후 가장 긴 캔디의 길이를 구하고 다시 swap한다. -\u0026gt; 메모리를 줄일 수 있음.\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import sys def get_max_candy(candies): max_cnt = 1 for i in range(N): max_row = 1 max_col = 1 for j in range(1, N): # row candy if candies[i][j - 1] == candies[i][j]: max_row += 1 else: max_row = 1 # col candy if candies[j - 1][i] == candies[j][i]: max_col += 1 else: max_col = 1 max_cnt = max(max_cnt, max_row, max_col) return max_cnt N = int(sys.stdin.readline()) candies = [] for _ in range(N): candies.append(list(sys.stdin.readline().strip())) max_candy = 1 for i in range(N): for j in range(1, N): if candies[i][j - 1] != candies[i][j]: candies[i][j - 1], candies[i][j] = candies[i][j], candies[i][j - 1] max_candy = max(max_candy, get_max_candy(candies)) candies[i][j], candies[i][j - 1] = candies[i][j - 1], candies[i][j] if candies[j - 1][i] != candies[j][i]: candies[j - 1][i], candies[j][i] = candies[j][i], candies[j - 1][i] max_candy = max(max_candy, get_max_candy(candies)) candies[j][i], candies[j - 1][i] = candies[j - 1][i], candies[j][i] if max_candy == N: break print(max_candy) ","date":"2021-04-13T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%82%AC%ED%83%95-%EA%B2%8C%EC%9E%84/","title":"사탕 게임"},{"content":" problem number: 1476 link: https://www.acmicpc.net/problem/1476 Define input, output Input: 1년이 지날 때마다 E, S, M 이 1씩 증가 단 각각은 아래의 범위 만큼만 가질 수 있음 넘을 경우에는 1로 초기화 1 \u0026lt;= E \u0026lt;= 15 / 1 \u0026lt;= S \u0026lt;= 28 / 1 \u0026lt;= M \u0026lt;= 19 Output: e, s, m 이 주어졌을 때 몇 년이 흘렀는지 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 import sys E, S, M = map(int, sys.stdin.readline().rstrip().split(\u0026#34; \u0026#34;)) e = s = m = 1 cnt = 1 while True: if E == e and S == s and M == m: break e += 1 if e == 16: e = 1 s += 1 if s == 29: s = 1 m += 1 if m == 20: m = 1 cnt += 1 print(cnt) ","date":"2021-04-10T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%82%A0%EC%A7%9C-%EA%B3%84%EC%82%B0/","title":"날짜 계산"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd argocd를 branch에서 cd 이번 포스트에서는 argocd를 이용해 github branch 에서 cd(continuous delivery) 하는 법에 대해서 알아보겠습니다.\n1. github action 설정하기 1.1 CI CI를 위한 github action을 작성하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 docker-image-ci: runs-on: ubuntu-18.04 steps: - name: Checkout uses: actions/checkout@v2 with: submodules: recursive fetch-depth: 0 - name: Create shot SHA uses: benjlevesque/short-sha@v1.2 id: short-sha - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to GitHub Container Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.repository_owner }} password: ${{ secrets.CR_PAT }} - name: Setup python uses: actions/setup-python@v2 with: python-version: \u0026#39;3.8.5\u0026#39; - name: Generate html run: | pip install -r requirements-doc.txt cd docs make html - name: Build and push uses: docker/build-push-action@v2 with: push: true context: ./ file: docker/Dockerfile tags: | ghcr.io/aiden-jeon/sphinx-api:latest ghcr.io/aiden-jeon/sphinx-api:${{ steps.short-sha.outputs.sha }} main branch에 수정이 생기면 docs를 build하고 docker image를 만듭니다. docs를 build하기 위해서 requirements-doc.txt 를 추가합니다.\n1 2 3 # requirements-doc.txt sphinx-autoapi==1.6.0 sphinx-rtd-theme==0.5.1 1.2 CD CI를 위한 github action을 작성하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 docker-image-cd: runs-on: ubuntu-18.04 steps: - name: Create short SHA uses: benjlevesque/short-sha@v1.2 id: short-sha - name: Checkout deploy repo uses: actions/checkout@v2 with: repository: aiden-jeon/github-cicd token: ${{ secrets.CR_PAT }} path: github-cicd - name: Install yq run: | wget https://github.com/mikefarah/yq/releases/download/v4.4.1/yq_linux_amd64 -O ./yq chmod +x ./yq - name: Change image tag on values.yaml run: | cat ./github-cicd/sphinx-doc/values.yaml | ./yq e \u0026#39;.image.tag=\u0026#34;${{ steps.short-sha.outputs.sha }}\u0026#34;\u0026#39; - | tee ./github-cicd/sphinx-doc/values.yaml.tmp mv ./github-cicd/sphinx-doc/values.yaml.tmp ./github-cicd/sphinx-doc/values.yaml - name: Commit file run: | cd github-cicd git config --local user.email \u0026#34;ells2124@gmail.com\u0026#34; git config --local user.name \u0026#34;aiden-jeon\u0026#34; git add ./sphinx-doc/values.yaml git commit -m \u0026#34;Update sphinx-doc image tag.\u0026#34; - name: Push changes uses: ad-m/github-push-action@master with: directory: github-cicd repository: aiden-jeon/github-cicd github_token: ${{ secrets.CR_PAT }} branch: doc-pages force: true CI가 종료되면 doc-pages branch 의 values를 수정합니다.\n2. APP 생성 cd를 위한 app을 생성하겠습니다.\nNEW APP 을 눌러 추가하겠습니다.\n그림-3과 같이 config를 입력하고 create를 해줍니다. 완성되면 아래와 같이 app이 생성됩니다.\napp에 클릭해서 들어가면 다음과 같이 나옵니다. 여기서 SYNC 버튼을 눌러줍니다\nSYNCHRONIZE 버튼을 누릅니다.\n다음은 sync가 완료된 화면입니다.\ncommand창에서 정상적으로 떴는지 확인해봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 ❯ kubectl get all -n sphinx-doc NAME READY STATUS RESTARTS AGE pod/sphinx-branch-sphinx-doc-5c597645d-wr69x 1/1 Running 0 35s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sphinx-branch-sphinx-doc NodePort 10.107.46.101 \u0026lt;none\u0026gt; 80:30903/TCP 36s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/sphinx-branch-sphinx-doc 1/1 1 1 35s NAME DESIRED CURRENT READY AGE replicaset.apps/sphinx-branch-sphinx-doc-5c597645d 1 1 1 35s 4. 확인 다음 명령어로 접속할 ip와 포트를 얻습니다.\n1 2 3 4 5 6 7 8 9 10 11 ❯ minikube service -n sphinx-doc --url sphinx-branch-sphinx-doc 🏃 Starting tunnel for service sphinx-branch-sphinx-doc. |------------|--------------------------|-------------|------------------------| | NAMESPACE | NAME | TARGET PORT | URL | |------------|--------------------------|-------------|------------------------| | sphinx-doc | sphinx-branch-sphinx-doc | | http://127.0.0.1:63918 | |------------|--------------------------|-------------|------------------------| http://127.0.0.1:63918 ❗ Because you are using a Docker driver on darwin, the terminal needs to be open to run it. 정상적으로 실행이 되었는지 확인합니다.\n","date":"2021-04-09T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/argocd%EB%A5%BC-branch%EC%97%90%EC%84%9C-cd/","title":"argocd를 branch에서 cd"},{"content":" problem number: 2743 link: https://www.acmicpc.net/problem/2743 Define input, output Input: 문자열 Output: 문자열의 길이 설명 source code 1 2 3 4 5 import sys S = sys.stdin.readline().strip() print(len(S)) ","date":"2021-04-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%8B%A8%EC%96%B4-%EA%B8%B8%EC%9D%B4-%EC%9E%AC%EA%B8%B0/","title":"단어 길이 재기"},{"content":" problem number: 10820 link: https://www.acmicpc.net/problem/10820 Define input, output Input: 소문자, 대문자, 숫자, 공백으로 이루어진 어떤 문자열 Output: 소문자, 대문자, 숫자, 공백의 개수 설명 주어진 문자열 시작과 끝에 공백이 있음으로 strip 하면 안 된다.\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 import sys from collections import Counter for i in range(100): lower = 0 upper = 0 number = 0 blank = 0 line = sys.stdin.readline() if not line: break counts = Counter(line) for key in counts: if key == \u0026#34; \u0026#34;: blank += counts[key] elif ord(\u0026#34;0\u0026#34;) \u0026lt;= ord(key) \u0026lt;= ord(\u0026#34;9\u0026#34;): number += counts[key] elif ord(\u0026#34;a\u0026#34;) \u0026lt;= ord(key) \u0026lt;= ord(\u0026#34;z\u0026#34;): lower += counts[key] elif ord(\u0026#34;A\u0026#34;) \u0026lt;= ord(key) \u0026lt;= ord(\u0026#34;Z\u0026#34;): upper += counts[key] print(lower, upper, number, blank) ","date":"2021-04-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%AC%B8%EC%9E%90%EC%97%B4-%EB%B6%84%EC%84%9D/","title":"문자열 분석"},{"content":" problem number: 10808 link: https://www.acmicpc.net/problem/10808 Define input, output Input: 소문자로 이루어진 단어 S Output: 각 소문자별 count 설명 source code 1 2 3 4 5 6 7 8 import sys from collections import Counter S = sys.stdin.readline() S = Counter(S) answer = [str(S.get(chr(c), 0)) for c in range(ord(\u0026#34;a\u0026#34;), ord(\u0026#34;z\u0026#34;) + 1)] print(\u0026#34; \u0026#34;.join(answer)) ","date":"2021-04-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%95%8C%ED%8C%8C%EB%B2%B3-%EA%B0%9C%EC%88%98/","title":"알파벳 개수"},{"content":" problem number: 10809 link: https://www.acmicpc.net/problem/10809 Define input, output Input: 소문자로 이루어진 단어 S Output: 각 알파벳이 처음 나온 위치 source code 1 2 3 4 5 6 7 8 9 10 11 import sys S = sys.stdin.readline().strip() result = [-1] * (ord(\u0026#34;z\u0026#34;) - ord(\u0026#34;a\u0026#34;) + 1) for i, string in enumerate(S): if result[ord(string) - ord(\u0026#34;a\u0026#34;)] == -1: result[ord(string) - ord(\u0026#34;a\u0026#34;)] = i print(*result) ","date":"2021-04-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%95%8C%ED%8C%8C%EB%B2%B3-%EC%B0%BE%EA%B8%B0/","title":"알파벳 찾기"},{"content":" problem number: 1935 link: https://www.acmicpc.net/problem/1935 Define input, output Input: 후위표기식으로 표시된 식 Output: 값 설명 후위 표기식과 다르게 정방향으로 접근한다.\n후휘 표기식 자체가 컴퓨터에서 식을 간단하게 계산하기 위해서 만들어진 것을 생각하자.\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 import sys def calculator(after, before, cur): if cur == \u0026#34;+\u0026#34;: return before + after elif cur == \u0026#34;-\u0026#34;: return before - after elif cur == \u0026#34;*\u0026#34;: return before * after elif cur == \u0026#34;/\u0026#34;: return before / after N = int(sys.stdin.readline()) math = sys.stdin.readline().strip() alphabet = ord(\u0026#34;A\u0026#34;) replace = {} for _ in range(N): n = int(sys.stdin.readline()) replace[chr(alphabet)] = n alphabet += 1 stacks = [] for cur in math: if ord(\u0026#34;A\u0026#34;) \u0026lt;= ord(cur) \u0026lt;= ord(\u0026#34;Z\u0026#34;): stacks.append(replace[cur]) else: after = stacks.pop() before = stacks.pop() stacks.append(calculator(after, before, cur)) print(\u0026#34;{:.2f}\u0026#34;.format(stacks[0])) ","date":"2021-04-08T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%ED%9B%84%EC%9C%84-%ED%91%9C%EA%B8%B0%EC%8B%9D2/","title":"후위 표기식2"},{"content":" problem number: 1122 difficult: Easy https://leetcode.com/problems/relative-sort-array/ Define input, output Input: Two arrays are given, arr1 and arr2 arr2 is distinct and it is subset of arr1 Output: arr2_1: ordered values which are in arr2 arr2_2: values which are not in arr2 with ascending order concat arr2_1, arr2_2 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 from typing import List class Solution: def relativeSortArray(self, arr1: List[int], arr2: List[int]) -\u0026gt; List[int]: order_dict = {v: 0 for v in arr2} not_order_dict = {} result = [] for v in arr1: if v in order_dict: order_dict[v] += 1 else: not_order_dict.setdefault(v, 0) not_order_dict[v] += 1 for v in arr2: sub_result = [v] * order_dict[v] result.extend(sub_result) for v in sorted(not_order_dict): sub_result = [v] * not_order_dict[v] result.extend(sub_result) return result ","date":"2021-04-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/relative-sort-array/","title":"relative-sort-array"},{"content":" problem number: 17299 link: https://www.acmicpc.net/problem/17299 Define input, output Input: Output: 설명 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import sys from collections import Counter N = int(sys.stdin.readline()) array = list(map(int, sys.stdin.readline().strip().split(\u0026#34; \u0026#34;))) cnt = Counter(array) cnt_array = list(map(lambda x: (cnt[x], x), array)) stack = [] answer = [-1] * N for i, (val, mapping) in enumerate(cnt_array): while stack and cnt_array[stack[-1][0]][0] \u0026lt; val: j, v, m = stack.pop() answer[j] = mapping stack.append([i, val, mapping]) print(*answer) ","date":"2021-04-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%98%A4%EB%93%B1%ED%81%B0%EC%88%98/","title":"오등큰수"},{"content":" problem number: 1918 link: https://www.acmicpc.net/problem/1918 Define input, output Input: Output: 설명 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 import sys cals = list(sys.stdin.readline().rstrip()) stack = [\u0026#34;\u0026#34;] answer = \u0026#34;\u0026#34; for c in cals: if c in [\u0026#34;*\u0026#34;, \u0026#34;/\u0026#34;]: while stack and stack[-1] in [\u0026#34;*\u0026#34;, \u0026#34;/\u0026#34;]: answer += stack.pop() stack.append(c) elif c in [\u0026#34;+\u0026#34;, \u0026#34;-\u0026#34;]: while stack and stack[-1] != \u0026#34;(\u0026#34;: answer += stack.pop() stack.append(c) elif c == \u0026#34;)\u0026#34;: while stack and stack[-1] != \u0026#34;(\u0026#34;: answer += stack.pop() _ = stack.pop() elif c == \u0026#34;(\u0026#34;: stack.append(c) else: answer += c while stack: answer += stack.pop() print(answer) ","date":"2021-04-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%ED%9B%84%EC%9C%84-%ED%91%9C%EA%B8%B0%EC%8B%9D/","title":"후위 표기식"},{"content":"Introduction to XAI(eXplainable AI) 최근 많은 산업에서 딥러닝을 적용을 시도하고 있습니다. 하지만 모델의 결과가 어떤 과정을 통해서 그런 예측을 했는지 해석하기 어려운 \u0026lsquo;블랙박스\u0026rsquo; 문제는 딥러닝 적용을 blocking하는 큰 요인입니다.\n예를 들어 신규 대출을 받으려는 고객이 있습니다. 은행에서는 이 고객이 대출을 잘 갚을 수 있을지에 대해 심사를 하고 대출을 승인을 결정합니다. 이 은행에서는 딥러닝 모델을 이용해 고객들의 대출 심사를 합니다. 모델은 이 고객에게 대출을 거절했습니다. 고객과 대출 담당자는 대출이 거절된 이유가 고객의 나이, 직업 등 어떤 이유로 대출이 거절 되었는지 알고 싶습니다. 여기서 문제가 발생합니다. 우리는 딥러닝 모델의 결정을 해석할 수가 없습니다. 이러한 해석을 할 수 없다는 단점은 여러 산업에서 딥러닝 모델의 적용을 망설이게 합니다.\nInterpretable vs Accurate Trade-off 그럼 이런 의문이 들 수 도 있습니다. 해석이 힘든 딥러닝 모델 대신 해석하기 쉬운 선형회귀분석 같은 모델을 사용하면 되지 않을까? 하지만 복잡한 모델과 간단한 모델, 이 두 모델 사이에는 Trade-off 가 있습니다. 해석이 쉬운 모델들은 정확도가 떨어지며, 높은 정확도를 보이는 모델은 해석이 어렵습니다.\n그럼 해석하기도 쉽고 정확도도 높은 모델을 만들면 되지 않을까요? 방법은 두가지가 있습니다. 해석이 쉬운 모델의 정확도를 높이는 방법과 정확도가 높은 모델의 해석 가능성을 높이는 방법입니다. 그리고 최근 논문들을 보면 2번의 방법을 연구하는 추세입니다.\n해석이 쉬운 모델의 정확도를 높이는 방법 정확도가 높은 모델의 해석 가능성을 높이는 방법 대리 분석 (Surrogate Analysis) Surrogate Analysis를 우리 말로 하면 대리 분석입니다. 즉, 해석이 어려운 모델을 바로 해석하지 않고 해석 가능한 대리 모델을 만들고 이를 이용해 모델의 예측 결과를 해석합니다.\n글로벌 대리 분석 (Global Surrogate Analysis) 글로벌 대리 분석이란 전체 학습 데이터를 사용해 블랙박스 함수 f를 따라하는 유사함수 g를 만들고 g를 해석 가능하도록 변조하는 방법을 말합니다.\n글로벌 대리 분석 수행 과정 데이터 집합 X를 선택합니다. 이 때 집합은 학습 데이터 전체 또는 일부입니다. 선택한 데이터 집합 X에 대해 블랙박스 모델 f의 예측 결과를 구합니다. 이 집합과 예측 값은 해석가능한 모델을 fit 시키기 위한 데이터셋으로 사용됩니다. 해석 가능한 모델(g)을 고릅니다. 해석 가능한 모델을 2번에서 만든 데이터셋을 이용해 학습합니다. 데이터 X에 대하여 모델 f가 예측한 결과(2)와 모델 g의 예측 결과를 비교하면서 두 모델이 최대한 유사한 결과를 내도록 튜닝합니다. 설명 가능한 모델 g을 이용해 블랙박스 모델(f)을 해석합니다. 장점 유연함(Flexible) 모든 해석 가능한 모델에 사용할 수 있다. 모든 블랙박스 모델에 사용할 수 있다. 예를 들어, 선형분석이 편한 이용자와 의사결정나무가 편한 이용자가 있을 때, 한 블랙박스 모델을 선형분석, 의사결정나무 로 대리 분석 모델을 만들 수 있다. 직관적(Intuitive) 쉽게 실행할 수 있다. 쉽게 설명할 수 있다. 평가의 용이 R-Squared 척도와 같이 대리분석 모델이 얼마나 블랙박스 모델을 잘 설명했는지 알 수 있다. 단점 모델에 대한 결론을 낼 수는 있지만, 데이터에 대한 결론을 낼 수는 없다. 얼마나 대리 분석 모델을 fitting 시켜야 하는지 알 수 없다. 대리 분석 모델로 선택한 모델의 모든 장점과 단점이 따라온다. 로컬 대리 분석 (Local Surrogate Analysis) 글로벌 대리 분석에서 블랙박스 모델을 해석하려는 것과 반대로 로컬 대리 분석은 개별적인 예측을 설명하기 위한 방법입니다. 로컬 대리 분석에는 모델과 관련 없이(Model-Agnostic) 사용 할 수 있는 방법과 특정한 모델에서(model-type-specific) 사용할 수 있는 방법이 있습니다. LIME(Local Interpretable model-agnostic explanations)은 대표적인 모델과 관련 없이 사용할 수 있는 로컬 대리 분석 방법입니다. 특정한 모델에서 사용하는 방법은 DeepLIFT 등이 있습니다.\n로컬 대리 분석 방법 블랙박스 모델 예측을 설명하고자 하는 instance (x)를 하나 선택합니다. 데이터를 섞어(perturb) 새로운 데이터 집합(x\u0026rsquo;)을 만듭니다. 생성된 데이터 집합(x\u0026rsquo;)을 해석하려는 모델에 넣어 예측 값f(x\u0026rsquo;)을 계산합니다. 생성된 데이터와 예측 값은 대리 분석 모델의 데이터로 사용됩니다. 정의된 거리에 따라 새로운 데이터의 가중치(w)를 계산합니다. 가중치를 이용해 해석 가능한 모델을 학습합니다. g(f, x\u0026rsquo;, w) 로컬 모델을 이용해 예측을 설명합니다. 장점 설명이 선택적이고 대조적이다. LASSO나 짧은 트리를 이용하면 변수들을 선택할 수 있고 설명을 +,- 로 대조적으로 만들 수 있다. 이는 human-friendly 한 설명을 할 수 있다. fidelity measure 해석 가능한 모델이 블랙박스 모델의 예측과 얼마나 잘 일치하는 지를 평가하는 척도 이를 이용해 블랙박스 예측을 설명하는데 얼마나 신뢰할 수 있는지를 설명할 수 있다. 단점 설명 모델의 복잡도를 사전에 정의해야 한다. Tree의 깊이 등. 설명의 불안정성 논문에서 두 개의 매우 가까운 점에 대한 설명이 시뮬레이션된 환경에서 크게 다르다는 것을 보여주었다. 샘플링 과정을 반복하면 나오는 탐색은 다를 수 있다. 불안정하다는 것은 설명을 신뢰하기 어렵다는 뜻. Reference XAI 설명 가능한 인공지능, 인공지능을 해부하다 https://christophm.github.io/interpretable-ml-book/global.html https://www.microsoft.com/en-us/research/uploads/prod/2019/05/Explainable-AI-for-Science-and-Medicine-slides.pdf ","date":"2021-03-31T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/introduction-to-xai/","title":"Introduction to XAI"},{"content":"(LIME) Local Interpretable Model-agnostic Explanations​ Objective of LIME $$\\xi(x)=\\underset{g\\in G}{\\operatorname{argmin}} L(f,g,\\pi_{x})+\\Omega(g)$$\n​\nTrain $f$ 이번 포스트에서는 예제 데이터로 fetch_20newsgroups를 사용했습니다. fetch_20newsgroups 데이터에는 총 20개의 Class가 존재합니다. 하지만 문제를 단순하게 하기 위해서 이번 포스트에서는 20개 Class 모두를 사용하기 보다는 \u0026ldquo;atheism\u0026rdquo;, \u0026ldquo;christian\u0026rdquo; 2개의 카테고리만 이용하겠습니다. 이렇게 되면 이제 두 클래스를 분류하는 Binary Text Classification 문제가 되고 모델로는 Random Forest를 사용해 보겠습니다.\n그리고 학습된 Random forest를 $f$ 라고 정의하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import sklearn import sklearn.ensemble from sklearn.pipeline import make_pipeline from sklearn.datasets import fetch_20newsgroups categories = [\u0026#34;alt.atheism\u0026#34;, \u0026#34;soc.religion.christian\u0026#34;] newsgroups_train = fetch_20newsgroups(subset=\u0026#34;train\u0026#34;, categories=categories) newsgroups_test = fetch_20newsgroups(subset=\u0026#34;test\u0026#34;, categories=categories) class_names = [\u0026#34;atheism\u0026#34;, \u0026#34;christian\u0026#34;] vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=False) train_vectors = vectorizer.fit_transform(newsgroups_train.data) test_vectors = vectorizer.transform(newsgroups_test.data) rf = sklearn.ensemble.RandomForestClassifier(n_estimators=500) rf.fit(train_vectors, newsgroups_train.target) c = make_pipeline(vectorizer, rf) 다음으로 test data에서 $x$를 선택합니다. 선택된 $x$에 대해서 Random Forest 모델 $f$를 이용해 예측된 $f(x)$를 해석하고자 합니다. 하지만 Random Forest $f$ 는 블랙박스 모델이기 때문에 이를 직접적으로 해석할 수는 없습니다. 그래서 $f(x)$를 바로 해석하는 대신 해석할 수 있는 모델 $g$를 이용해 $f(x)$의 결과를 해석해 보겠습니다.\nnewsgroups_test에서 첫 번째 데이터를 이용해서 LIME의 동작 방법에 대해서 알아보겠습니다.\n1 text_instance, instance_label = newsgroups_test.data[0], newsgroups_test.target[0] 데이터를 한번 확인 해보겠습니다.\n1 2 \u0026gt; text_instance \u0026#39;From: crackle!dabbott@munnari.oz.au (NAME)\\nSubject: \u0026#34;Why I am not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: dabbott@augean.eleceng.adelaide.edu.au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of Adelaide\\nLines: 4\\n\\nCould the guy who wrote the article \u0026#34;Why I am not Bertrand Russell\u0026#34;\\nresend me a copy?\\n\\nSorry, I accidently deleted my copy and forgot your name.\\n\u0026#39; 이 데이터의 레이블은 1, \u0026ldquo;christian\u0026rdquo; 입니다.\n1 2 \u0026gt; instance_label 1 위에서 우리가 학습한 모델 $f$을 이용해 예측해 보면 모델이 정답인 1로 예측합니다.\n1 2 \u0026gt; c.predict([text_instance]) array([1]) 위에서 학습한 모델이 잘 예측을 하는 것을 확인했습니다. 이제 이 모델은 어떻게 1이라고 예측을 하게 됐을까요? 이제부터 그 과정에 대해서 알아보려고 합니다.\nInterpretable Data Representation 우선 데이터 $x$ 를 interpretable representation이 가능한 $x\u0026rsquo;$ 으로 변환합니다. Text의 경우 interpretable represenstion은 단어가 존재한다/존재하지 않는다 입니다.\n아래 코드는 단어를 숫자로 mapping 시켜주는 역할을 합니다.\n1 2 3 4 from lime.lime_text import TextDomainMapper, IndexedString indexed_string = IndexedString(text_instance, bow=True, split_expression=r\u0026#34;\\W+\u0026#34;, mask_string=None) domain_mapper = TextDomainMapper(indexed_string) ​ $x\u0026rsquo;$ 는 모든 단어가 존재하기 때문에 아래와 같이 모든 값이 존재한다를 뜻하는 1로 채워져 있습니다.\n1 2 3 4 [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.] Sampling for Local Exploration 이제 위에서 변환된 $x\u0026rsquo;$ 주변에서 $z\u0026rsquo;$을 샘플링 합니다. 샘플링 방법은 random 하게 값을 고른 후 1을 0으로 바꿔주면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 import numpy as np from sklearn.utils import check_random_state random_state = check_random_state(2020) num_samples = 10 doc_size = indexed_string.num_words() sample = random_state.randint(1, doc_size + 1, num_samples - 1) data = np.ones((num_samples, doc_size)) data[0] = np.ones(doc_size) 샘플링된 $z\u0026rsquo;$ 를 확인하면 다음과 같습니다. data의 첫번째는 $x\u0026rsquo;$ 이며, 나머지는 $z\u0026rsquo;$ 입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 \u0026gt; data array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.], [1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0.], [1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.], [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.], [1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], [1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.], [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]]) $z\u0026rsquo;$ 를 원래의 표현(텍스트) 로 복원시키면 이 값이 $z$ 가 됩니다.\n1 2 3 4 5 6 features_range = range(doc_size) inverse_data = [indexed_string.raw_string()] for i, size in enumerate(sample, start=1): inactive = random_state.choice(features_range, size, replace=False) data[i, inactive] = 0 inverse_data.append(indexed_string.inverse_removing(inactive)) inverse_data를 확인하면 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; inverse_data [\u0026#39;From: crackle!dabbott@munnari.oz.au (NAME)\\nSubject: \u0026#34;Why I am not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: dabbott@augean.eleceng.adelaide.edu.au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of Adelaide\\nLines: 4\\n\\nCould the guy who wrote the article \u0026#34;Why I am not Bertrand Russell\u0026#34;\\nresend me a copy?\\n\\nSorry, I accidently deleted my copy and forgot your name.\\n\u0026#39;, \u0026#39;: crackle!@munnari..au ()\\n: \u0026#34;Why I am not \u0026#34; ( request)\\n-: @..adelaide..au ( )\\nOrganization: \u0026amp; ., University \\n: 4\\n\\n wrote article \u0026#34;Why I am not \u0026#34;\\n a ?\\n\\nSorry, I accidently forgot .\\n\u0026#39;, \u0026#39;From: crackle!@munnari.oz.au (NAME)\\nSubject: \u0026#34;Why I am not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: @augean.eleceng.adelaide..au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of Adelaide\\nLines: \\n\\nCould the guy the article \u0026#34;Why I am not Bertrand Russell\u0026#34;\\n me a copy?\\n\\nSorry, I my copy and forgot your .\\n\u0026#39;, \u0026#39;From: crackle!dabbott@.oz.au (NAME)\\nSubject: \u0026#34;Why I am not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: dabbott@augean.eleceng.adelaide.edu.au (Derek Abbott)\\n: Electrical \u0026amp; Electronic Eng., University of Adelaide\\nLines: 4\\n\\n the guy who wrote the article \u0026#34;Why I am not Bertrand Russell\u0026#34;\\nresend a copy?\\n\\nSorry, I accidently deleted my copy and forgot your name.\\n\u0026#39;, \u0026#39;From: crackle!dabbott@munnari.oz.au (NAME)\\nSubject: \u0026#34;Why am not Bertrand Russell\u0026#34; (2nd request)\\n-: dabbott@augean.eleceng.adelaide.edu.au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of \\nLines: 4\\n\\nCould the guy who wrote the article \u0026#34;Why am not Bertrand Russell\u0026#34;\\nresend me a copy?\\n\\nSorry, accidently deleted my copy and forgot your name.\\n\u0026#39;, \u0026#39;From: crackle!dabbott@.. (NAME)\\n: \u0026#34; I not Russell\u0026#34; ( )\\n-To: dabbott@augean.... (Derek Abbott)\\n: \u0026amp; Eng., University Adelaide\\n: \\n\\n who article \u0026#34; I not Russell\u0026#34;\\nresend a copy?\\n\\n, I accidently my copy and name.\\n\u0026#39;, \u0026#39;From: crackle!dabbott@munnari.oz.au (NAME)\\nSubject: \u0026#34;Why I am not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: dabbott@.eleceng.adelaide.edu.au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of Adelaide\\nLines: 4\\n\\nCould the guy who wrote the \u0026#34;Why I am not Bertrand Russell\u0026#34;\\n me a copy?\\n\\nSorry, I accidently deleted my copy and forgot name.\\n\u0026#39;, \u0026#39;From: crackle!dabbott@munnari.oz.au ()\\n: \u0026#34;Why I not Bertrand Russell\u0026#34; (2nd request)\\nReply-To: dabbott@augean.eleceng.adelaide.edu.au (Derek Abbott)\\nOrganization: Electrical \u0026amp; Electronic Eng., University of Adelaide\\n: 4\\n\\n wrote article \u0026#34;Why I not Bertrand Russell\u0026#34;\\nresend me a copy?\\n\\nSorry, I accidently deleted my copy and forgot your name.\\n\u0026#39;, \u0026#39;From: crackle!@..au ()\\n: \u0026#34;Why I not Bertrand Russell\u0026#34; (2nd request)\\n-: @augean....au ( )\\nOrganization: Electrical \u0026amp; Electronic ., University Adelaide\\nLines: \\n\\n who article \u0026#34;Why I not Bertrand Russell\u0026#34;\\n ?\\n\\n, I deleted your .\\n\u0026#39;, \u0026#39;: !@.. ()\\n: \u0026#34; \u0026#34; ( )\\n-: @.eleceng... ( )\\n: \u0026amp; ., \\n: \\n\\n \u0026#34; \u0026#34;\\n me ?\\n\\n, .\\n\u0026#39;] ​\n샘플링된 $z$ 값을 해석하려는 모델 $f$ 에 넣어서 $g$를 학습할 때 사용할 $label$ 을 만듭니다.\n$$label = f(z)$$\n1 labels = c.predict_proba(inverse_data) 예측된 label 값은 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; labels array([[0.284, 0.716], [0.276, 0.724], [0.262, 0.738], [0.292, 0.708], [0.282, 0.718], [0.174, 0.826], [0.232, 0.768], [0.298, 0.702], [0.246, 0.754], [0.098, 0.902]]) ​\nSparse Linear Explanations 다음으로는 $g$를 학습시킬 때 필요한 $weight$를 계산해야 합니다.\n위에서 random 하게 뽑힌 $z\u0026rsquo;$들이 원본 데이터와 거리가 얼마나 먼 곳에서 있는지에 따라서 학습할 때 가중치로 사용합니다.\n$$\\pi_{x}=exp(-D(x,z)^{2}/\\sigma^{2}): \\text{sample weight}$$\n$D$는 거리를 계산하는 함수이며 각 데이터 특성별로 사용하는 Distance function은 다음과 같습니다.\ntext: cosine distance image: $L2$ distance ​\nDistance 우선 $x$ 와 $z$ 사이의 거리를 계산합니다. 이 예시는 text 이기 때문에 cosine distance 를 구했습니다.\n1 2 3 4 5 6 7 8 **import scipy as sp distance_metric = \u0026#34;cosine\u0026#34; def distance_fn(x): return sklearn.metrics.pairwise.pairwise_distances(x, x[0], metric=distance_metric).ravel() * 100 distances = distance_fn(sp.sparse.csr_matrix(data)) 계산된 거리는 다음과 같습니다.\n1 2 3 \u0026gt; distances array([ 0. , 40.59114742, 9.2514787 , 4.001634 , 4.001634 , 32.84492632, 4.001634 , 8.17749432, 35.83110521, 80.19704914]) ​\nKerenl function $pi_{x}$는 exponential kernel 입니다. 앞서 계산한 distance를 kernel에 넣어서 값을 변환시켜줍니다. 이 때 식의 sigma는 kernel width 로 해석됩니다.\n1 2 3 4 5 6 7 8 9 from functools import partial def kernel(d, kernel_width): return np.sqrt(np.exp(-(d ** 2) / kernel_width ** 2)) kernel_width = 25 kernel_fn = partial(kernel, kernel_width=kernel_width) weights = kernel_fn(distances) 계산된 weight는 다음과 같습니다.\n1 2 3 \u0026gt; weights array([1. , 0.26763986, 0.93381971, 0.98727124, 0.98727124, 0.42188127, 0.98727124, 0.94790866, 0.35804576, 0.005827 ]) ​\nFeature selection 다음으로 계산해야 할 것은 $\\Omega(g)$입니다. 논문에서는 이 부분을 K-LASSO로 대신했지만 실제 코드에서는 Ridge 또는 이용자가 준 값 k를 사용합니다. 설명할 변수들을 Ridge모델로 학습 후 선택합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 from sklearn.linear_model import Ridge labels_column = labels[:,1] num_features = 10 clf = Ridge(alpha=0.01, fit_intercept=True, random_state=random_state) clf.fit(data, labels_column, sample_weight=weights) coef = clf.coef_ weighted_data = coef * data[0] feature_weights = sorted(zip(range(data.shape[1]), weighted_data), reverse=True) used_features = np.array([x[0] for x in feature_weights[:num_features]]) 선택된 feature은 다음과 같습니다.\n1 2 \u0026gt; used_features array([50, 49, 48, 47, 46, 45, 44, 43, 42, 41]) ​\nTrain $g$ 선택된 변수들을 이용하여서 $g$를 학습합니다.\n1 2 3 easy_model = Ridge(alpha=1, fit_intercept=True, random_state=random_state) easy_model.fit(data[:, used_features], labels_column, sample_weight=weights) prediction_score = easy_model.score(data[:, used_features], labels_column, sample_weight=weights) 예측된 점수값은 다음과 같습니다.\n1 2 \u0026gt; prediction_score 0.7873828293020748 학습된 모델을 이용해 설명하려는 instance의 예측합니다.\n1 local_pred = easy_model.predict(data[0, used_features].reshape(1, -1)) 예측된 값은 다음과 같습니다.\n1 2 \u0026gt; local_pred array([0.71951834]) 설명 변수들을 coefficient 크기를 기준으로 정렬합니다.\n1 local_exp = sorted(zip(used_features, easy_model.coef_), key=lambda x: np.abs(x[1]), reverse=True) 정렬된 변수들의 순서는 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; local_exp [(49, -0.028394961697102435), (43, -0.016671889703629914), (48, -0.01667188970362991), (45, -0.011273894261784378), (44, -0.005261889646530821), (46, 0.003666088585078779), (47, 0.0036660885850787785), (42, 0.0036660885850787785), (41, -0.0025519331421568372), (50, 0.0009561320807047926)] feature를 원래 값(text)으로 복원합니다.\n1 2 3 4 5 6 7 8 9 10 11 \u0026gt; domain_mapper.map_exp_ids(local_exp) [(\u0026#39;your\u0026#39;, -0.028394961697102435), (\u0026#39;Sorry\u0026#39;, -0.016671889703629914), (\u0026#39;forgot\u0026#39;, -0.01667188970362991), (\u0026#39;deleted\u0026#39;, -0.011273894261784378), (\u0026#39;accidently\u0026#39;, -0.005261889646530821), (\u0026#39;my\u0026#39;, 0.003666088585078779), (\u0026#39;and\u0026#39;, 0.0036660885850787785), (\u0026#39;copy\u0026#39;, 0.0036660885850787785), (\u0026#39;a\u0026#39;, -0.0025519331421568372), (\u0026#39;name\u0026#39;, 0.0009561320807047926)] ​ LIME의 해석에 관한 부분은 공식 github repo를 참조해주세요.\nhttps://github.com/marcotcr/lime\n","date":"2021-03-31T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/lime-with-code/","title":"LIME with code"},{"content":" problem number: 17413 link: https://www.acmicpc.net/problem/17413 Define input, output Input: Strings, special token \u0026ldquo;\u0026lt;\u0026rdquo;, \u0026ldquo;\u0026gt;\u0026rdquo; \u0026ldquo;\u0026lt;\u0026hellip;\u0026gt;\u0026rdquo; is tag, and don\u0026rsquo;t reverse the tag Output: reversed string with tags source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sys string = sys.stdin.readline().strip() stack = [] reverse = \u0026#34;\u0026#34; is_tag = False for s in string: if s == \u0026#34; \u0026#34;: while stack: reverse += stack.pop() reverse += s elif s == \u0026#34;\u0026lt;\u0026#34;: while stack: reverse += stack.pop() is_tag = True reverse += s elif s == \u0026#34;\u0026gt;\u0026#34;: is_tag = False reverse += s elif is_tag: reverse += s else: stack.append(s) while stack: reverse += stack.pop() print(reverse) ","date":"2021-03-31T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%8B%A8%EC%96%B4-%EB%92%A4%EC%A7%91%EA%B8%B02/","title":"단어 뒤집기2"},{"content":" problem number: 10799 link: https://www.acmicpc.net/problem/10799 Define input, output Input: 쇠 막대기의 시작, 끝, 레이저 포인트에 대한 array Output: 레이저로 잘린 쇠막대기의 총 개수 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import sys points = list(sys.stdin.readline().strip()) total = 0 remain = [] for i, point in enumerate(points): if point == \u0026#34;)\u0026#34;: j = remain.pop() if i - j == 1: total += len(remain) else: total += 1 else: remain.append(i) print(total)``` ","date":"2021-03-31T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%87%A0%EB%A7%89%EB%8C%80%EA%B8%B0/","title":"쇠막대기"},{"content":" problem number: 17298 link: https://www.acmicpc.net/problem/17298 Define input, output Input: Array $$\u0026lt;a_{1}, a_{2}, \u0026hellip; ,a_{n}\u0026gt;$$ Output: Array : $$\u0026lt;a\u0026rsquo;{1}, a\u0026rsquo;{2}, \u0026hellip; ,a\u0026rsquo;_{n}\u0026gt;$$ $$b_{i} = max(a\u0026rsquo;{i+1}, a\u0026rsquo;{i+2}, \u0026hellip;, a\u0026rsquo;_{n})$$ $$a\u0026rsquo;{i} = \\text{if } b{i} \u0026gt; a_{i} \\text{ then } b_{i} \\text{ else }-1$$ source code 1 2 3 4 5 6 7 8 9 10 11 12 13 import sys N = int(sys.stdin.readline()) array = list(map(int, sys.stdin.readline().strip().split(\u0026#34; \u0026#34;))) stack = [] answer = [-1] * N for i, val in enumerate(array): while stack and array[stack[-1]] \u0026lt; val: answer[stack.pop()] = val stack.append(i) print(*answer) ","date":"2021-03-31T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%98%A4%ED%81%B0%EC%88%98/","title":"오큰수"},{"content":" problem number: 9093 link: https://www.acmicpc.net/problem/9093 Define input, output Input: sentence with words 단어의 기준은 space 대문자는 유지 Output: 문장에서 단어들의 위치는 보존하고 각 단어들의 역순으로 이루어진 문장 source code 1 2 3 4 5 6 7 8 9 10 import sys N = int(sys.stdin.readline()) for _ in range(N): words = sys.stdin.readline().split(\u0026#34; \u0026#34;) words = list(map(lambda x :x[::-1], words)) sentence = \u0026#34; \u0026#34;.join(words) print(sentence) ","date":"2021-03-30T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%8B%A8%EC%96%B4-%EB%92%A4%EC%A7%91%EA%B8%B0/","title":"단어 뒤집기"},{"content":" problem number: 1874 link: https://www.acmicpc.net/problem/1874 Define input, output Input: empty stack 1 // n Output: array \u0026lt;a1, a2, \u0026hellip;\u0026gt; ai = (push, pop) if cannot return NO 설명 스택 수열 동작 예시\nstack = [] / / 4\nstack = [1] / / 4\nstack = [1, 2] / / 4\nstack = [1, 2, 3] / / 4\nstack = [1, 2, 3, 4] / / 4\nstack = [1, 2, 3] / 4 / 3\nstack = [1, 2] / 4 3\nstack = [1, 2, 5] / 4 3\nstack = [1, 2, 5, 6] / 4 3\nstack = [1, 2, 5] / 4 3 6\nstack = [1, 2, 5, 7] / 4 3 6\nstack = [1, 2, 5, 7, 8] / 4 3 6\nstack = [1, 2, 5, 7] / 4 3 6 8\nstack = [1, 2, 5] / 4 3 6 8 7\nstack = [1, 2] / 4 3 6 8 7 5\nstack = [1] / 4 3 6 8 7 5 2\nstack = [] / 4 3 6 8 7 5 2 1\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 import sys N = int(sys.stdin.readline()) stack = [] result = [] s = 1 for _ in range(N): j = int(sys.stdin.readline()) while True: if s \u0026lt;= j: stack.append(s) result.append(\u0026#34;+\u0026#34;) s += 1 else: p = stack.pop() if j == p: result.append(\u0026#34;-\u0026#34;) else: result.append(\u0026#34;NO\u0026#34;) break if result[-1] == \u0026#34;NO\u0026#34;: break if result[-1] == \u0026#34;NO\u0026#34;: print(\u0026#34;NO\u0026#34;) else: for r in result: print(r) ","date":"2021-03-30T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%8A%A4%ED%83%9D%EC%88%98%EC%97%B4/","title":"스택수열"},{"content":" problem number: 1406 link: https://www.acmicpc.net/problem/1406 Define input, output Input: 문자열, 명령어 Output: 명령어에 의해 수정된 문자열 설명 입력으로 들어오는 크기의 최대값이 50000임\nindexing 으로 할 경우 P 를 받을 때 O(n) 이 발생 insert, pop 의 경우 값을 추가하거나 뺄 때 해당 index의 오른쪽 값들을 한 칸씩 미루어야 해서 시간 복잡도가 O(n) 이 발생함 source code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 import sys string = sys.stdin.readline().strip() N = int(sys.stdin.readline()) left = list(string) right = [] for _ in range(N): command = sys.stdin.readline().strip() if command[0] == \u0026#34;P\u0026#34;: left.append(command[-1]) elif command[0] == \u0026#34;L\u0026#34; and left: right.append(left.pop()) elif command[0] == \u0026#34;D\u0026#34; and right: left.append(right.pop()) elif command[0] == \u0026#34;B\u0026#34; and left: left.pop() print(\u0026#34;\u0026#34;.join(left + right[::-1])) ","date":"2021-03-30T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%97%90%EB%94%94%ED%84%B0/","title":"에디터"},{"content":" problem number: 1158 link: https://www.acmicpc.net/problem/1158 Define input, output Input: \u0026lt;a1, a2, .. an\u0026gt;, order k Output: \u0026lt;a^1, a^2, .. a^n\u0026gt; 설명 list를 이용할 경우 append or pop 에서 O(N)이 소모된다. 시간복잡도를 줄이기 위해서는 아래의 과정이 각각 O(1) 이어야 한다.\nleft pop, right append left append, right pop 이를 위해서 deque를 사용했다.\nsource code 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 import sys from collections import deque n, k = map(int, sys.stdin.readline().strip().split(\u0026#34; \u0026#34;)) deq = deque() for i in range(1, n + 1): deq.append(i) answer = [] while deq: for _ in range(k - 1): deq.append(deq.popleft()) answer += [deq.popleft()] answer = \u0026#34;, \u0026#34;.join(map(str, answer)) print(f\u0026#34;\u0026lt;{answer}\u0026gt;\u0026#34;) ","date":"2021-03-30T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EC%9A%94%EC%84%B8%ED%91%B8%EC%8A%A4-%EB%AC%B8%EC%A0%9C/","title":"요세푸스 문제"},{"content":"Module setup Contents 순서\nsetup.py 와 Makefile 설정하기 프로젝트별 pylint 설정하기 이번 포스트에서는 setup.py 와 Makefile을 설정하는 법에 대해 소개하겠습니다.\n포스트에서 사용된 코드는 github 에서 확인할 수 있습니다.\n1. setup.py setup.py 에서 사용되는 setuptools.setup은 개발하고 있는 소스 코드를 package 형식으로 배포 할 때 이용합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 ## setup.py from setuptools import find_packages, setup setup( name=\u0026#34;github_cicd\u0026#34;, version=\u0026#34;0.0.1\u0026#34;, packages=find_packages(\u0026#34;src\u0026#34;), python_requires=\u0026#34;\u0026gt;=3.*\u0026#34;, author=\u0026#34;Aiden-jeon\u0026#34;, author_email=\u0026#34;ells2124@gmail.com\u0026#34;, description=\u0026#34;Example of github-cicd\u0026#34;, keywords=\u0026#34;ML pipeline kubeflow automation\u0026#34;, url=\u0026#34;https://github.com/aiden-jeon/github-cicd\u0026#34;, project_urls={ \u0026#34;Documentation\u0026#34;: \u0026#34;https://github.com/aiden-jeon/github-cicd\u0026#34;, \u0026#34;Source Code\u0026#34;: \u0026#34;https://github.com/aiden-jeon/github-cicd\u0026#34;, }, ) name: 패키지가 어떤 이름으로 설치 될지를 나타냅니다.\nversion: 배포된 소스코드의 버전입니다.\npackages: 배포에 포함 될 package들을 의미합니다. 보통 setuptools.find_packages를 이용해 자동으로 찾습니다. 예를 들어서 src 폴더 밑에 module_1, module_2 가 있다면 두 모듈을 모두 찾아서 설치를 해줍니다. 이렇게 설치된 경우 아래와 같이 사용할 수 있습니다.\n1 2 import module_1 import module_2 만약, src 폴더 없이 module_1 폴더를 설치하고 싶을 경우에는 아래와 같이 설정을 해야 합니다.\n1 packages=[\u0026#34;module_1\u0026#34;] python_requires: 실행 가능한 파이썬 버전을 의미합니다.\n아래 코드를 이용해 작성이 완료된 setup.py를 설치할 수 있습니다.\n1 pip install - e . 사용한 python 버전의 site-packages에 가면 아래와 같이 패키지가 설치되어 있는 것을 확인 할 수 있습니다.\n1 2 3 \u0026gt; ls|grep github github-cicd.egg-link 2. Makefile 이번에는 Makefile을 작성해보겠습니다.\n2.1 Makefile을 사용하는 이유 Compiling the source code files can be tiring, especially when you have to include several source files and type the compiling command every time you need to compile. Makefiles are the solution to simplify this task.\nMakefiles are special format files that help build and manage the projects automatically.\n출처: link\n요약하면 compile을 반복적으로 해야할 때 이러한 작업을 간단히 하기 위해 사용합니다.\n2.2 코드 작성 Python에서는 협업을 위한 여러가지 tool이 존재합니다. 이러한 tool을 이용해 코드의 일관성, 타입체크, 코드 테스트를 수행할 수 있습니다. 그런데 이 과정이 반복적으로 이루어지기 때문에 이를 코드화하려고 합니다.\ninit Makefile을 수행하기 위해서는 제일 처음 필요한 패키지와 소스코드를 설치해야 합니다. 1 2 3 4 init: pip install -U pip pip install -r requirements.txt pip install -e . format 코드 format을 맞추기 위해 사용되는 툴은 주로 black 과 isort 가 있습니다. 이 때 isort 가 black과 충돌을 일으키지 않게 --profile black을 추가합니다. 1 2 3 format: black . isort . --skip-gitignore --profile black lint linting을 검사하기 위해 pytest 를 이용하고 pylint, flake8, mypy 를 사용하겠습니다. 1 2 lint: pytest src/mrxflow/ --pylint --flake8 --mypy test test를 수행하기 위해 pytest를 이용하겠습니다. 1 2 test: pytest tests -s --verbose --cov=src/ --cov-report=html --cov-report=term-missing 전체 코드는 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 SHELL := /bin/bash all: echo \u0026#39;Makefile for github-cicd\u0026#39; init: pip install -U pip pip install -r requirements.txt pip install -e . format: black . isort . --skip-gitignore --profile black lint: pytest src/mrxflow/ --pylint --flake8 --mypy test: pytest tests -s --verbose --cov=src/ --cov-report=html --cov-report=term-missing 3. 사용하기 setup.py와 Makefile의 작성을 다 했다면 다음과 같이 사용할 수 있습니다.\ninit Makefile에서 수행할 때 필요한 패키지를 설치합니다. 1 make init 다른 명령어 사용해보기 format 1 make format lint 1 make lint test 1 make test ","date":"2021-03-14T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/setup.py-%EC%99%80-makefile-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/","title":"setup.py 와 Makefile 설정하기"},{"content":"Module setup Contents 순서\nsetup.py 와 Makefile 설정하기 프로젝트별 pylint 설정하기 이번 포스트에서는 .pylintrc를 이용해 프로젝트별 pylint를 설정하는 법에 대해 설명하겠습니다. 포스트에서 사용된 코드는 github 에서 확인할 수 있습니다.\n1. generate rcfile 우선 .pylintrc 를 생성하겠습니다.\n1 pylint --generate-rcfile \u0026gt; .pylintrc .pylintrc를 확인해보면 여러가지 설정들이 있습니다.\n2. pylint 사용 다음으로 지난번 포스트에서 만든 Makefile을 이용해 소스코드를 검사해보겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 \u0026gt; make lint pytest src/ --pylint --flake8 --mypy ================= test session starts ================== platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 rootdir: /Users/jongseob/workspace/github-cicd plugins: flake8-1.0.7, mypy-0.8.0, cov-2.11.1, pylint-0.18.0 collected 4 items -------------------------------------------------------- Linting files . -------------------------------------------------------- src/calculator.py F..s [100%] ======================= FAILURES ======================= ______________ [pylint] src/calculator.py ______________ C: 1, 0: Missing module docstring (missing-module-docstring) C: 4, 4: Argument name \u0026#34;a\u0026#34; doesn\u0026#39;t conform to snake_case naming style (invalid-name) C: 4, 4: Argument name \u0026#34;b\u0026#34; doesn\u0026#39;t conform to snake_case naming style (invalid-name) R: 4, 4: Method could be a function (no-self-use) C: 26, 4: Argument name \u0026#34;a\u0026#34; doesn\u0026#39;t conform to snake_case naming style (invalid-name) C: 26, 4: Argument name \u0026#34;b\u0026#34; doesn\u0026#39;t conform to snake_case naming style (invalid-name) R: 26, 4: Method could be a function (no-self-use) ========================= mypy ========================= Success: no issues found in 1 source file =============== short test summary info ================ FAILED src/calculator.py::PYLINT ======== 1 failed, 2 passed, 1 skipped in 0.19s ======== make: *** [lint] Error 1 3가지 error가 있는 것을 볼 수 있습니다.\nmissing-module-docstring : class docstring이 없어서 나온 에러입니다. invalid-name : 변수 이름을 snake_case로 짓지 않아서 나온 에러입니다. no-self-use : self를 사용하지 않아서 나온 에러입니다. 에러를 발생하지 않기 위해서는 각 에러에 맞게 코드를 수정해야 합니다. 예를 들어서 invalid-name은 변수명 a와 b 대신 num_1, num_2 로 수정하면 해결 됩니다.\n그런데 가끔은 이러한 변수명을 사용해야 할 경우가 있습니다. 이런 경우 해결 할 수 있는 방법은 두 가지가 있습니다.\n해당 라인을 검사하지 않게 하기 간단한 해법으로는 pylint가 해당 라인을 검사하지 않게 하는 것입니다. 원래의 코드가 아래와 같습니다. 1 def add(self, a: float, b: float) -\u0026gt; float: 여기에 주석을 추가해주면 됩니다.\n1 def add(self, a: float, b: float) -\u0026gt; float: # pylint: disable=invalid-name 이렇게 코드를 수정하면 pylint가 더 이상 해당 line에서 동일한 에러를 발생시키지 않습니다.\n에러 자체를 검사하지 않기 그런데 코드 한줄이 아니라 코드 전체에서 해당 lint를 검사하지 않게 하고 싶을 수도 있습니다. 이 때는 .pylintrc에 직접 해당 lint를 검사하지 않게 추가해주면 됩니다. 1 2 disable=invalid-name, ... disable에 위와 같이 작성을 하면 더 이상 해당 lint를 검사하지 않습니다.\n이번 포스트에서는 위에서 발생한 모든 에러를 .pylintrc 에서 검사하지 않도록 했습니다. 이제 다시 lint를 확인해보겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 \u0026gt; make lint pytest src/ --pylint --flake8 --mypy ================= test session starts ================== platform darwin -- Python 3.8.6, pytest-6.2.1, py-1.10.0, pluggy-0.13.1 rootdir: /Users/jongseob/workspace/github-cicd plugins: flake8-1.0.7, mypy-0.8.0, cov-2.11.1, pylint-0.18.0 collected 4 items -------------------------------------------------------- Linting files . -------------------------------------------------------- src/calculator.py .... [100%] ========================= mypy ========================= Success: no issues found in 1 source file ================== 4 passed in 0.41s =================== 이제 더 이상 에러가 안나오는 것을 확인 할 수 있습니다.\n","date":"2021-03-14T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EB%B3%84-pylint-%EC%84%A4%EC%A0%95%ED%95%98%EA%B8%B0/","title":"프로젝트별 pylint 설정하기"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd 이번 포스트에서는 argocd를 이용해 helm chart를 cd(continuous delivery) 하는 법에 대해서 알아보겠습니다.\n1. 설치 argocd 설치는 tutorial을 따라했습니다.\n2. 실행 1 2 3 4 5 6 7 8 9 # 1. argocd namespace kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # access argo cd api server kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; #port forward kubectl port-forward svc/argocd-server -n argocd 8080:80 argocd 를 실행시켰으면 https://localhost:8080 으로 들어갑니다.\n기본으로 설정되어 있는 아이디는 admin이며 비밀번호는 아래 명령어로 확인할 수 있습니다.\nv1.8.0 이하는 아래 명령어로 확인할 수 있습니다.\n1 kubectl get pods -n argocd -l app.kubernetes.io/name=argocd-server -o name | cut -d\u0026#39;/\u0026#39; -f 2 v1.9.0 이상은 아래 명령어로 확인할 수 있습니다.\n1 kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d 3. APP 생성 처음 나오는 화면은 다음과 같이 아무런 app이 없습니다.\nNEW APP 을 눌러 추가하겠습니다.\n그림-3과 같이 config를 입력하고 create를 해줍니다. 완성되면 아래와 같이 app이 생성됩니다.\napp에 클릭해서 들어가면 다음과 같이 나옵니다. 여기서 SYNC 버튼을 눌러줍니다\nSYNCHRONIZE 버튼을 누릅니다.\n다음은 sync가 완료된 화면입니다.\ncommand창에서 정상적으로 떴는지 확인해봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; kubectl get all -n sphinx-doc NAME READY STATUS RESTARTS AGE pod/sphinx-api-doc-sphinx-doc-5576ff677b-d4xzl 1/1 Running 0 89s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sphinx-api-doc-sphinx-doc NodePort 10.105.22.158 \u0026lt;none\u0026gt; 80:31058/TCP 89s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/sphinx-api-doc-sphinx-doc 1/1 1 1 89s NAME DESIRED CURRENT READY AGE replicaset.apps/sphinx-api-doc-sphinx-doc-5576ff677b 1 1 1 89s 4. 확인 다음 명령어로 접속할 ip와 포트를 얻습니다.\n1 2 3 \u0026gt; minikube service -n sphinx-doc --url sphinx-api-doc-sphinx-doc http://192.168.64.2:31058 정상적으로 실행이 되었는지 확인합니다.\n","date":"2021-03-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/argocd%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-cd/","title":"argocd를 이용한 cd"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd 이번 포스트에서는 minikube를 이용해 이전 포스트에서 만든 ghcr package를 이용해 deployment를 만드는 법에 대해서 알아보겠습니다.\n0. requirements 본 포스트에서는 local환경에서 minikube를 사용하고 있습니다.\n1. deployment.yaml docker/deploy 폴더 밑에 deployment.yaml 파일을 작성하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: apps/v1 kind: Deployment metadata: name: sphinx-doc spec: selector: matchLabels: type: app service: sphinx-doc template: metadata: labels: type: app service: sphinx-doc spec: containers: - name: sphinx-doc image: ghcr.io/aiden-jeon/sphinx-api:latest imagePullPolicy: Always ports: - containerPort: 80 imagePullSecrets: - name: test-ghcr 이전 포스트에서 작성한 sphinx-api를 띄우는 deployment 입니다. apply하기에 앞서 secrets를 설정해주어야 합니다. 자세한 방법은 포스트를 참고해주세요.\nsecrets를 설정했다면 deployment.yaml 파일을 apply 해줍니다.\n1 2 3 \u0026gt; kubectl apply -f deployment.yaml deployment.apps/sphinx-doc created pod이 정상적으로 image를 pull 했는지 확인해봅니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 \u0026gt; k describe po Name: sphinx-doc-7cdd4b7c55-cqlk4 Namespace: default Priority: 0 Node: minikube/192.168.64.2 Start Time: Thu, 04 Mar 2021 15:37:04 +0900 Labels: pod-template-hash=7cdd4b7c55 service=sphinx-doc type=app Annotations: \u0026lt;none\u0026gt; Status: Running IP: 172.17.0.4 IPs: IP: 172.17.0.4 Controlled By: ReplicaSet/sphinx-doc-7cdd4b7c55 Containers: sphinx-doc: Container ID: docker://26991f0eed6db1208c3a3cd92d5f00bbe5ba37934baed29e964d021e92603315 Image: ghcr.io/aiden-jeon/sphinx-api:latest Image ID: docker-pullable://ghcr.io/aiden-jeon/sphinx-api@sha256:f35c4c0852edaab2bfd617f2a42cf3fe141a08cd3692ac3105f1cb6be3591135 Port: 80/TCP Host Port: 0/TCP State: Running Started: Thu, 04 Mar 2021 15:37:08 +0900 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-d7kw7 (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-d7kw7: Type: Secret (a volume populated by a Secret) SecretName: default-token-d7kw7 Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s node.kubernetes.io/unreachable:NoExecute op=Exists for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 5m10s default-scheduler Successfully assigned default/sphinx-doc-7cdd4b7c55-cqlk4 to minikube Warning FailedMount 5m9s kubelet MountVolume.SetUp failed for volume \u0026#34;default-token-d7kw7\u0026#34; : failed to sync secret cache: timed out waiting for the condition Normal Pulling 5m8s kubelet Pulling image \u0026#34;ghcr.io/aiden-jeon/sphinx-api:latest\u0026#34; Normal Pulled 5m6s kubelet Successfully pulled image \u0026#34;ghcr.io/aiden-jeon/sphinx-api:latest\u0026#34; in 1.246291235s Normal Created 5m6s kubelet Created container sphinx-doc Normal Started 5m6s kubelet Started container sphinx-doc 맨 마지막의 Events 항목을 보면 정상적으로 pull 했음을 확인할 수 있습니다.\n정상적으로 띄우고 있는지 확인하기 위해서 curl pod을 이용해 접근해 보겠습니다.\n1 kubectl run curl -it --image=curlimages/curl sh deployment에서 띄운 pod의 ip는 172.17.0.4 입니다. 다음 명령어로 정상적으로 띄우고 있는지 확인합니다.\n1 curl 172.17.0.4 정상적으로 띄우고 있다면 아래와 같이 출력됩니다.\n1 2 3 4 5 6 7 8 9 10 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html class=\u0026#34;writer-html5\u0026#34; lang=\u0026#34;en\u0026#34; \u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Welcome to example’s documentation! \u0026amp;mdash; example 0.1 documentation\u0026lt;/title\u0026gt; ... 2. service.yaml 위에서는 pod을 이용해 cluster 안에 있는 pod의 내용을 확인할 수 있었습니다. 이번에는 service를 이용해 로컬에서도 볼 수 있도록 하겠습니다.\ndocker/deploy/에서 service.yaml을 작성해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Service metadata: name: sphinx-doc-svc spec: type: NodePort ports: - port: 8000 targetPort: 80 protocol: TCP selector: type: app service: sphinx-doc apply를 합니다.\n1 2 3 \u0026gt; kubectl apply -f service.yaml service/sphinx-api-svc created 정상적으로 됐는지 확인합니다.\n1 2 3 4 5 \u0026gt; kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 44h sphinx-doc-svc NodePort 10.105.108.18 \u0026lt;none\u0026gt; 8000:30511/TCP 43s 다음 명령어로 ip로 접근할 수 있습니다.\n1 2 3 \u0026gt; minikube service -n default --url sphinx-doc-svc http://192.168.64.2:30511 정상적으로 document를 보여주는 것을 확인할 수 있습니다.\n3. ingress.yaml 다음으로 ip 주소를 domain 으로 바꾸기 위한 ingress를 작성해 보겠습니다.\ndocker/deploy/에서 ingress.yaml을 작성해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: sphinx-doc-ingress spec: rules: - host: docs.sphinx.com http: paths: - path: / pathType: Prefix backend: service: name: sphinx-doc-svc port: number: 8000 apply 해줍니다.\n1 2 3 \u0026gt; kubectl apply -f ingress.yaml ingress.networking.k8s.io/sphinx-doc-ingress created 정상적으로 됐는지 확인합니다.\n1 2 3 4 \u0026gt; kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE sphinx-doc-ingress \u0026lt;none\u0026gt; docs.sphinx.com 192.168.64.2 80 49s /etc/hosts를 열어줍니다. (환경에 따라 /etec/host 일 수 도 있습니다.)\n1 sudo vim /etc/hosts 파일을 열고 가장 마지막에 minikube ip와 ingress에 작성한 도메인을 입력합니다.\n1 192.168.64.2 docs.sphinx.com 위와 같이 적고 저장 후 닫습니다. http://docs.sphinx.com/ 를 들어가서 정상적으로 api document가 나오는지 확인합니다.\n","date":"2021-03-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/ghcr%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-kubernetes-deployment-%EB%A7%8C%EB%93%A4%EA%B8%B0/","title":"ghcr을 이용한 kubernetes deployment 만들기"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd 이번 포스트에서는 github action을 이용해 CI(Continuous Integreation)를 하는 법에 대해서 알아보겠습니다. 이번 포스트에서 사용하는 Dockerfile은 github 를 이용합니다. 내용은 이전 포스트를 확인해주세요.\n1. github package 사용 설정하기 이번 포스트에서는 ghcr(GitHub Container Registry for Docker images) 을 이용할 예정입니다. 개인 repo에서 ghcr을 사용하기 위해서는 따로 설정할 부분이 있습니다.\ngithub 페이지에서 오른쪽 위에 있는 프로필을 누른 후 Feature Preview를 열어줍니다. Improved continer support enable을 눌러줍니다. 그러면 아래와 같이 바뀌게 됩니다. 2. secrets 설정해주기 github action 으로 자동으로 push 하기 위해서는 github packages에 접근할 수 있는 key가 필요합니다.\n우선 github setting에서 deplotver settings / personal access tokens 로 들어 갑니다.\n여기서 generate new token을 누릅니다.\n토큰 이름으로 ghcr-token을 입력하고 packages와 관련된 권한을 주고 token을 생성합니다.\n다음과 같이 토큰이 생성됩니다. 이 토큰을 메모장에 잘 적어둡니다.\n그리고 ci를 진행할 github repo의 settings에 들어갑니다. secrets를 누른 후 New repository secret을 눌러줍니다. Name에는 CR_PAT을 value 에는 위에서 생성한 key를 입력합니다. 정상적으로 생성되면 아래와 같이 나오게 됩니다. 3. github action 작성하기 이제 준비가 끝났으니 github action을 작성할 차례입니다. .github/workflows 폴더를 생성합니다. 그리고 아래와 같은 파일을 작성합니다. 파일명은 docker-image-push.yml으로 하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: Docker Image CI/CD on: push: branches: [main] jobs: docker-image-ci: runs-on: ubuntu-18.04 steps: - name: Checkout uses: actions/checkout@v2 with: submodules: recursive fetch-depth: 0 - name: Create shot SHA uses: benjlevesque/short-sha@v1.2 id: short-sha - name: Set up Docker Buildx uses: docker/setup-buildx-action@v1 - name: Login to GitHub Container Registry uses: docker/login-action@v1 with: registry: ghcr.io username: ${{ github.repository_owner }} password: ${{ secrets.CR_PAT }} - name: Build and push uses: docker/build-push-action@v2 with: push: true context: ./ file: docker/Dockerfile tags: | ghcr.io/\u0026lt;github-name\u0026gt;/\u0026lt;package-name\u0026gt;:latest ghcr.io/\u0026lt;github-name\u0026gt;/\u0026lt;package-name\u0026gt;:${{ steps.short-sha.outputs.sha }} 내용에서 수정해야 할 부분은 아래 부분입니다.\n1 2 3 tags: | ghcr.io/\u0026lt;github-name\u0026gt;/\u0026lt;package-name\u0026gt;:latest ghcr.io/\u0026lt;github-name\u0026gt;/\u0026lt;package-name\u0026gt;:${{ steps.short-sha.outputs.sha }} github-name: pacakge를 저장할 github name을 적어줍니다. package-name: 저장할 package 이름을 적어줍니다. 예를 들어 저는 다음과 같이 적겠습니다.\n1 2 3 tags: | ghcr.io/aiden-jeon/sphinx-api:latest ghcr.io/aiden-jeon/sphinx-api:${{ steps.short-sha.outputs.sha }} 이제 작성한 workflow를 github에 push 해줍니다. github에 들어가보면 자동으로 action을 실행합니다. package에 가보면 아래과 같이 생성된 걸 확인할 수 있습니다. 4. local ghcr pull local에서 ghcr을 pull해서 정상적으로 build가 되었는지 확인해보겠습니다.\n아래 명령어로 ghcr docker에 로그인합니다.\n1 2 export CR_PAT=\u0026lt;MY_TOKEN\u0026gt; echo $CR_PAT | docker login ghcr.io -u \u0026lt;github-username\u0026gt; --password-stdin MY_TOKEN: 위에서 발급받은 key를 입력해줍니다. github-username: ghcr package가 있는 username을 입력합니다. 로그인을 한후 아까 만든 docker를 pull합니다. pull 명령어는 package를 누르면 확인할 수 있습니다. 1 docker pull ghcr.io/aiden-jeon/sphinx-api:dc3c4be 정상적으로 run 되는지 확인해보겠습니다.\n1 docker run -p 8000:80 --rm ghcr.io/aiden-jeon/sphinx-api:dc3c4be http://localhost:8000 을 들어가면 정상적으로 나오는 것을 확인할 수 있습니다.\n","date":"2021-03-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/github-action%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-ci/","title":"github action을 이용한 ci"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd argocd를 branch에서 cd 이번 포스트에서는 helm 을 이용해 이전 포스트에서 작성한 파일들을 자동화 하는 법에 대해서 알아 보겠습니다.\n1. helm start helm 을 시작하려는 repo 최상단에서 다음과 같이 입력해줍니다.\n1 2 3 \u0026gt; helm create sphinx-doc Creating sphinx-doc 그러면 아래와 같은 파일들이 생성됩니다.\n1 2 3 4 \u0026gt; cd sphinx-doc \u0026gt; ls Chart.yaml charts templates values.yaml 2. deployment.yaml 생성된 deployment.yaml 을 다음과 같이 수정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 {% raw %} apiVersion: apps/v1 kind: Deployment metadata: name: {{ template \u0026#34;sphinx-doc.fullname\u0026#34; . }} namespace: sphinx-doc labels: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} chart: {{ template \u0026#34;sphinx-doc.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: replicas: {{ .Values.replicaCount }} revisionHistoryLimit: 3 selector: matchLabels: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} release: {{ .Release.Name }} template: metadata: labels: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} release: {{ .Release.Name }} spec: imagePullSecrets: - name: \u0026#34;{{ .Values.image.pullSecrets }}\u0026#34; containers: - name: {{ .Chart.Name }} image: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag }}\u0026#34; imagePullPolicy: {{ .Values.image.pullPolicy }} ports: - name: http containerPort: 80 protocol: TCP livenessProbe: httpGet: path: / port: http readinessProbe: httpGet: path: / port: http resources: {{ toYaml .Values.resources | indent 12 }} {{- with .Values.nodeSelector }} nodeSelector: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.affinity }} affinity: {{ toYaml . | indent 8 }} {{- end }} {{- with .Values.tolerations }} tolerations: {{ toYaml . | indent 8 }} {{- end }} {% endraw %} 3. service.yaml 생성된 service.yaml 을 다음과 같이 수정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 {% raw %} apiVersion: v1 kind: Service metadata: namespace: sphinx-doc name: {{ template \u0026#34;sphinx-doc.fullname\u0026#34; . }} labels: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} chart: {{ template \u0026#34;sphinx-doc.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} spec: type: {{ .Values.service.type }} ports: - port: {{ .Values.service.port }} targetPort: http protocol: TCP name: http selector: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} release: {{ .Release.Name }} {% endraw %} 4. ingress.yaml 생성된 ingress.yaml 을 다음과 같이 수정합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 {% raw %} {{- if .Values.ingress.enabled -}} {{- $fullName := include \u0026#34;sphinx-doc.fullname\u0026#34; . -}} {{- $ingressPath := .Values.ingress.path -}} {{- $svcPort := .Values.service.port -}} {{- if semverCompare \u0026#34;\u0026gt;=1.14-0\u0026#34; .Capabilities.KubeVersion.GitVersion -}} apiVersion: networking.k8s.io/v1beta1 {{- else -}} apiVersion: extensions/v1beta1 {{- end }} kind: Ingress metadata: namespace: mrxflow name: {{ template \u0026#34;sphinx-doc.fullname\u0026#34; . }} labels: app: {{ template \u0026#34;sphinx-doc.name\u0026#34; . }} chart: {{ template \u0026#34;sphinx-doc.chart\u0026#34; . }} release: {{ .Release.Name }} heritage: {{ .Release.Service }} {{- with .Values.ingress.annotations }} annotations: {{- toYaml . | nindent 4 }} {{- end }} spec: {{- if .Values.ingress.tls }} tls: {{- range .Values.ingress.tls }} - hosts: {{- range .hosts }} - {{ . | quote }} {{- end }} secretName: {{ .secretName }} {{- end }} {{- end }} rules: {{- range .Values.ingress.hosts }} - host: {{ .host }} http: paths: - path: {{ $ingressPath }} backend: serviceName: {{ $fullName }} servicePort: {{ $svcPort }} {{- end }} {{- end }} {% endraw %} 5. values-local.yaml local에서 확인을 위한 value로 values-local.yaml 을 만들겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 replicaCount: 1 image: repository: ghcr.io/aiden-jeon/sphinx-api tag: dc3c4be pullPolicy: Always pullSecrets: test-ghcr service: type: NodePort port: 80 ingress: enabled: false annotations: kubernetes.io/ingress.class: nginx ingress.kubernetes.io/rewrite-target: / path: / hosts: - host: chart-example.local tls: [] resources: {} nodeSelector: {} tolerations: [] affinity: {} 3. helm install 이제 helm 을 이용해 install 해보겠습니다.\n우선 kubectl namespace를 생성합니다.\n1 2 3 \u0026gt; kubectl create ns sphinx-doc namespace/sphinx-doc created 새 namespace에 secret을 추가해줍니다.\n1 2 3 4 5 kubectl -n sphinx-doc create secret docker-registry test-ghcr \\ --docker-server=ghcr.io \\ --docker-username=aiden-jeon \\ --docker-password=\u0026lt;secret_key\u0026gt; \\ --docker-email=ells2124@gmail.com 사용하지 않는 serviceaccount.yaml hpa.yaml 을 삭제합니다.\n1 2 rm sphinx-doc/templates/serviceaccount.yaml rm sphinx-doc/templates/hpa.yaml 그리고 helm install을 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 \u0026gt; helm install sphinx-doc sphinx-doc/ --values sphinx-doc/values-local.yaml NAME: sphinx-doc LAST DEPLOYED: Thu Mar 4 16:59:41 2021 NAMESPACE: default STATUS: deployed REVISION: 1 NOTES: 1. Get the application URL by running these commands: export NODE_PORT=$(kubectl get --namespace default -o jsonpath=\u0026#34;{.spec.ports[0].nodePort}\u0026#34; services sphinx-doc) export NODE_IP=$(kubectl get nodes --namespace default -o jsonpath=\u0026#34;{.items[0].status.addresses[0].address}\u0026#34;) echo http://$NODE_IP:$NODE_PORT 정상적으로 설치되면 아래와 같이 자동으로 service 들이 뜨게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 \u0026gt; kubectl get all -n sphinx-doc NAME READY STATUS RESTARTS AGE pod/sphinx-doc-6f5b76f46c-tzt7d 1/1 Running 0 11s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sphinx-doc NodePort 10.103.77.217 \u0026lt;none\u0026gt; 80:32164/TCP 11s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/sphinx-doc 1/1 1 1 11s NAME DESIRED CURRENT READY AGE replicaset.apps/sphinx-doc-6f5b76f46c 1 1 1 11s 연결을 위한 ip를 확인합니다.\n1 2 3 \u0026gt; minikube service -n sphinx-doc --url sphinx-doc http://192.168.64.2:32164 정상적으로 api document 가 보이는지 확인합니다.\n","date":"2021-03-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/helm%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-deployment-chart-%EB%A7%8C%EB%93%A4%EA%B8%B0/","title":"helm을 이용한 deployment chart 만들기"},{"content":"CI/CD Contents 순서\nsphinx-autoapi 를 이용한 자동 api 문서 생성하기 github action을 이용한 ci ghcr을 이용한 kubernetes deployment 만들기 helm을 이용한 deployment chart 만들기 argocd를 이용한 cd sphinx-autoapi를 이용해 자동으로 python api 문서를 생성하는 법에 대해 알아보겠습니다.\n1. API 코드 작성 우선 간단한 계산기 api를 만들어 보겠습니다. src/cacluator.py 에 다음과 같은 Class를 생성했습니다. Api 에 사용한 docstring은 numpy style를 사용하도록 하겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 class Calculator: def add(self, a: float, b: float) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;add given two number Examples:: calculator = Calculator() calculator.add(3, 4) Parameters ---------- a : float b : float Returns ------- float \u0026#34;\u0026#34;\u0026#34; return a + b def mul(self, a: float, b: float) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;multiply given two number Examples:: calculator = Calculator() calculator.mul(3, 4) Parameters ---------- a : floatp b : float Returns ------- float \u0026#34;\u0026#34;\u0026#34; return a * b 2. sphinx 설치 sphinx-autoapi를 설치합니다.\n1 pip install sphinx-autoapi sphinx-theme sphinx_rtd_theme 3. sphinx-quickstart 우선 sphinx 환경을 설정해야 합니다. docs 폴더를 만든 후 아래 명령어를 이용해 빠르게 설정을 할 수 있습니다.\n1 2 3 mkdir docs cd docs sphinx-quickstart sphinx-quickstart 설정 과정은 아래와 같습니다.\ndefault 값으로 source 와 build 디렉토리를 분리하지 않겠습니다. (n 을 입력해주세요.) 1 2 3 4 5 6 7 8 9 10 11 Welcome to the Sphinx 3.5.1 quickstart utility. Please enter values for the following settings (just press Enter to accept a default value, if one is given in brackets). Selected root path: . You have two options for placing the build directory for Sphinx output. Either, you use a directory \u0026#34;_build\u0026#34; within the root path, or you separate \u0026#34;source\u0026#34; and \u0026#34;build\u0026#34; directories within the root path. \u0026gt; Separate source and build directories (y/n) [n]: n 프로젝트 관련 이름들은 다음과 같이 작성했습니다. 1 2 3 \u0026gt; 프로젝트 이름: example \u0026gt; 작성자 이름: aiden-jeon \u0026gt; 프로젝트 출시 버전 []: 0.1 프로젝트 언어는 en을 설정하겠습니다. (Enter를 입력해주세요.) 1 \u0026gt; 프로젝트 언어 [en]: 설정이 완료되면 docs/ 밑에 아래와 같은 파일들이 생성됩니다.\n1 2 \u0026gt; ls Makefile _build _static _templates conf.py index.rst make.bat 3. sphinx 설정하기 생성된 파일중 conf.py 는 sphinx와 관련된 설정들이 있는 파일입니다. extensions에 아래와 같이 추가합니다.\n1 2 3 4 5 extensions = [ \u0026#34;sphinx_rtd_theme\u0026#34;, \u0026#34;autoapi.extension\u0026#34;, \u0026#34;sphinx.ext.napoleon\u0026#34;, ] sphinx_rtd_theme: sphinx에서 지원하는 테마중 readthedocs 테마를 이용하기 위한 extension입니다. autoapi.extension: autoapi를 사용하기 위한 extension 입니다. sphinx.ext.napoleon: numpy style docstring을 인식하기 위한 extension 입니다. extension 밑에 autoapi 관련 설정도 입력합니다.\n1 2 3 4 autoapi_type = \u0026#34;python\u0026#34; autoapi_dirs = [ os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \u0026#34;src\u0026#34;) ] 테마는 readthedocs 테마를 이용하겠습니다.\n1 html_theme = \u0026#34;sphinx_rtd_theme\u0026#34; 수정된 전체 conf.py는 다음과 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import os # -- Project information ----------------------------------------------------- project = \u0026#34;example\u0026#34; copyright = \u0026#34;2021, aiden-jeon\u0026#34; author = \u0026#34;aiden-jeon\u0026#34; # The full version, including alpha/beta/rc tags release = \u0026#34;0.1\u0026#34; # -- General configuration --------------------------------------------------- extensions = [ \u0026#34;sphinx_rtd_theme\u0026#34;, \u0026#34;autoapi.extension\u0026#34;, \u0026#34;sphinx.ext.napoleon\u0026#34;, ] autoapi_type = \u0026#34;python\u0026#34; autoapi_dirs = [ os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \u0026#34;src\u0026#34;) ] templates_path = [\u0026#34;_templates\u0026#34;] exclude_patterns = [\u0026#34;_build\u0026#34;, \u0026#34;Thumbs.db\u0026#34;, \u0026#34;.DS_Store\u0026#34;] # -- Options for HTML output ------------------------------------------------- html_theme = \u0026#34;readthedocs\u0026#34; html_static_path = [\u0026#34;_static\u0026#34;] 4. html build 이제 설정된 값으로 html 을 build 해보겠습니다.\n1 make html build가 성공적으로 되었다면 _build/html 경로에 다음과 같이 파일들이 생성됩니다.\n1 2 3 4 \u0026gt; cd _build/html \u0026gt; ls _sources autoapi index.html py-modindex.html searchindex.js _static genindex.html objects.inv search.html index.html 을 열면 아래와 같은 api documentation을 볼 수 있습니다. calculator를 눌러보면 위에서 작성한 docstring이 잘 나오는 것을 볼 수 있습니다. 5. docker 이제 생성된 document를 서빙할 수 있는 docker를 만들어 보겠습니다. docker/Dockerfile 에 다음과 같은 Dockerfile을 만들겠습니다.\n1 2 3 FROM nginx:latest COPY docs/_build/html /usr/share/nginx/html RUN chmod -R +rx /usr/share/nginx/html 이제 build를 하겠습니다.\n1 docker build . -f docker/Dockerfile -t docs 생성된 도커를 실행시켜봅니다.\n1 docker run -p 8000:80 --rm docs http://localhost:8000 로 접속하면 정상적으로 api document 문서가 보이는 것을 확인할 수 있습니다.\n해당 내용은 github 에서 확인할 수 있습니다.\n","date":"2021-03-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/sphinx-autoapi-%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EB%8F%99-api-%EB%AC%B8%EC%84%9C-%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0/","title":"sphinx-autoapi 를 이용한 자동 api 문서 생성하기"},{"content":"kubernetes 에서 사용하는 image는 기본적으로 dockerhub에서 pull 된다. 기본적으로 Local에 있는 패키지는 사용하지 않는다. 회사에서는 도커 이미지를 dockerhub와 같은 곳에 public 하게 공유할 수가 없다. 그래서 github 에서 제공하는 (GHCR) GitHub Container Registry for Docker images 을 이용해서 도커 이미지를 관리하고 있다.\nGHCR 을 이용하기 위해서는 github access key 가 필요하다. 우선 github access token을 발급 받아 보자.\n1. ghcr 을 위한 github access token 발급 받기 우선 github setting에서 deplotver settings / personal access tokens 로 들어간다.\n여기서 generate new token을 누른다.\n토큰 이름으로 ghcr-token을 입력하고 packages와 관련된 권한을 주고 token을 생성한다.\n다음과 같이 토큰이 생성된다. 이제 이 토큰을 잘 저장해둔다.\n2. kubernetes secret 다음으로 kubernetes 에 secret 을 생성해준다.\n1 2 3 4 5 kubectl -n \u0026lt;k8s-namespace\u0026gt; create secret docker-registry \u0026lt;k8s-docker-registry-secret-name\u0026gt; \\ --docker-server=ghcr.io \\ --docker-username=\u0026lt;github-username\u0026gt; \\ --docker-password=\u0026lt;github-personal-access-token\u0026gt; \\ --docker-email=\u0026lt;email-address\u0026gt; k8s-docker-registry-secret-name: secret 이름 github-username: github 유저 네임 github-personal-access-token: 발급받은 토큰 email-address: github 이메일 예를 들면 아래와 같이 사용할 수 있다.\n1 2 3 4 5 \u0026gt; kubectl -n default create secret docker-registry test-ghcr \\ --docker-server=ghcr.io \\ --docker-username=aiden-jeon \\ --docker-password=\u0026lt;github-personal-access-token\u0026gt; \\ --docker-email=ells2124@gmail.com 정상적으로 실행될 경우 아래와 같이 출력된다.\n1 secret/test-ghcr created 3. yaml 파일 작성 이제 deployment를 위한 yaml 파일을 작성한다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 apiVersion: apps/v1 kind: Deployment metadata: name: test-deploy spec: selector: matchLabels: type: app service: test-deploy template: metadata: labels: type: app service: test-deploy spec: containers: - name: test-deploy image: ghcr.io/{user-name}/{package-name}:{package-tag} imagePullPolicy: Always ports: - containerPort: 80 imagePullSecrets: - name: test-ghcr image에 ghcr 의 도커 이미지를 입력해주면 된다. 그리고 imagePullSecrets에 좀 전에 생성한 secret 의 이름을 입력하면 된다.\n","date":"2021-03-03T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/kubernetes%EC%97%90%EC%84%9C-ghcr-package-pull-%ED%95%98%EA%B8%B0/","title":"kubernetes에서 ghcr package pull 하기"},{"content":"0. 참고 자료 검색 빵형의 개발도상국 1. mp4 동영상 다운로드 youtube 영상 다운로드 ssyoutube로 바꾸면 다운로드 할 수 있는 사이트로 연결됨. https://www.youtube.com/watch?v=naRRqAGIAqQ https://www.ssyoutube.com/watch?v=naRRqAGIAqQ 2. 프로젝트를 위한 가상환경 만들기 1 2 3 pyenv virtualenv 3.7.8 tracking_idol pyenv activate tracking_idol pip install opencv-python==4.1.2.30 opencv-contrib-python==4.1.2.30 numpy opencv 최신 버전은 mac catalina 버전에서 실행이 안된다. https://solarianprogrammer.com/2019/10/21/install-opencv-python-macos/ 3. opencv 를 이용해서 동영상 실행시켜보기 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import cv2 video_path = \u0026#34;tracking_myidol/dumbi_dumbdi.mp4\u0026#34; cap = cv2.VideoCapture(video_path) ret, img = cap.read() cv2.imshow(\u0026#34;img\u0026#34;, img) while cap.isOpened(): ret, img = cap.read() if not ret: exit() cv2.imshow(\u0026#34;img\u0026#34;, img) if cv2.waitKey(1) == ord(\u0026#34;q\u0026#34;): break 4. ROI Setting ROI (Region of Interest) 관심 영역, 영상처리할 때 쓰는 보편적인 용어 1 2 3 4 5 ret, img = cap.read() cv2.namedWindow(\u0026#34;Select Window\u0026#34;) cv2.imshow(\u0026#34;Select Window\u0026#34;, img) rect = cv2.selectROI(\u0026#34;Select Window\u0026#34;, img, fromCenter=False, showCrosshair=True) cv2.destroyWindow(\u0026#34;Select Window\u0026#34;) 동영상 제일 처음 frame을 불러와서 cv2.selectROI 를 이용해 처음에 ROI 를 세팅할 수 있는 창을 만든다.\n5. tracker user가 선택한 ROI를 rect 라는 변수에 저장을 하는데, 이를 계속해서 추적하는 obejct tracker가 필요하다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # init tracker tracker = cv2.TrackerCSRT_create() tracker.init(img, rect) while cap.isOpened(): ret, img = cap.read() if not ret: exit() success, box = tracker.update(img) (left, top, w, h) = list(map(int, box)) cv2.rectangle( img, pt1=(left, top), pt2=(left + w, top + h), color=(255, 255, 255), thickness=3, ) cv2.imshow(\u0026#34;img\u0026#34;, img) if cv2.waitKey(1) == ord(\u0026#34;q\u0026#34;): break 이렇게 하면 Select Window 창에서 선택된 ROI를 계속해서 tracking 하는 것을 볼 수 있다.\n6. 예상되는 문제점 첫 frame에 원하는 인물이 나오지 않을 경우 다음 frame으로 넘길 수 있는 기능이 필요 (2020.08.17 해결) while loop으로 q 외에 다른 키를 누를 경우 frame 넘길 수 있게 처리함. 그리고 rect가 없을 경우 (0,0,0,0) 의 값을 갖는데 bounding box가 (0,0,0,0) 일 경우는 없다고 판단해서 합이 0이 아니면 break로 while loop 을 break 시킴. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cv2.namedWindow(\u0026#34;Select Window\u0026#34;) while cap.isOpened(): i += 1 print(i) ret, img = cap.read() cv2.imshow(\u0026#34;Select Window\u0026#34;, img) key = cv2.waitKey(10) # Quit when \u0026#39;q\u0026#39; is pressed if key == ord(\u0026#39;q\u0026#39;): break rect = cv2.selectROI(\u0026#34;Select Window\u0026#34;, img, fromCenter=False, showCrosshair=True) print(rect) if sum(rect) != 0: cv2.destroyWindow(\u0026#34;Select Window\u0026#34;) break 처음 ROI 설정 할 경우 앉은 상태에서 시작할 때, 일어나는 사람의 전체 size를 tracking 하지 못함 인물이 다른 인물에 가려져서 안보일 경우 어떻게 처리해야 할지. 7. 전체코드 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 import cv2 video_path = \u0026#34;tracking_myidol/dumbi_dumbdi.mp4\u0026#34; cap = cv2.VideoCapture(video_path) cap.set(cv2.CAP_PROP_POS_FRAMES, 80) ## ROI Setting if not cap.isOpened: exit() cv2.namedWindow(\u0026#34;Select Window\u0026#34;) while cap.isOpened(): ret, img = cap.read() cv2.imshow(\u0026#34;Select Window\u0026#34;, img) key = cv2.waitKey(10) # Quit when \u0026#39;q\u0026#39; is pressed if key == ord(\u0026#39;q\u0026#39;): break rect = cv2.selectROI(\u0026#34;Select Window\u0026#34;, img, fromCenter=False, showCrosshair=True) print(rect) if sum(rect) != 0: cv2.destroyWindow(\u0026#34;Select Window\u0026#34;) break # init tracker tracker = cv2.TrackerCSRT_create() tracker.init(img, rect) while cap.isOpened(): ret, img = cap.read() if not ret: exit() success, box = tracker.update(img) (left, top, w, h) = list(map(int, box)) cv2.rectangle( img, pt1=(left, top), pt2=(left + w, top + h), color=(255, 255, 255), thickness=3, ) cv2.imshow(\u0026#34;img\u0026#34;, img) if cv2.waitKey(1) == ord(\u0026#34;q\u0026#34;): break ","date":"2020-10-04T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/%EB%8F%99%EC%98%81%EC%83%81%EC%97%90%EC%84%9C-%EC%9B%90%ED%95%98%EB%8A%94-%EC%9D%B8%EB%AC%BC-%EB%94%B0%EB%9D%BC%EA%B0%80%EA%B8%B0/","title":"동영상에서 원하는 인물 따라가기"},{"content":"준비 우선 버츄얼 박스와 윈도우10을 다운로드 받습니다.\nVirtual box 다운로드 Window10 다운로드 설치하기 Virtualbox를 실행시키면 아래와 같은 화면이 나옵니다. (2020.07 기준 virutalbox 버전은 6.1.12 입니다.)\n여기서 새로 만들기 버튼을 눌러줍니다.\n이름에 Windows 10 이라고 입력해줍니다. 그러면 밑의 버전도 이름에 맞추어서 자동으로 변경됩니다.\n추천 메모리 크기는 기본값인 2048로 둡니다.\n계속하기를 누르면 하드 디스크 크기를 설정해주어야 합니다.\nVMDK를 골라줍니다.\n저는 32gb 로 설정해주었습니다.\n설치가 완료되면 다음과 같이 나옵니다.\n이제 여기서 설정 버튼을 눌러줍니다.\n우선 시스템에서 광 디스크를 하드 디스크 밑으로 내려 줍니다. 이는 윈도우가 CD로 설치 된 후 하드 디스크로 부팅시키기 위함입니다.\n그리고 저장소에서 비어있음을 누르고 빨간 네모가 쳐진 시디 모양의 아이콘을 눌러줍니다.\n그리고 가상 광학 디스크 선택/만들기를 눌러줍니다.\n추가를 누르고\n아까 다운로드 받은 windows10.ios 를 선택해줍니다.\n그러면 아래와 같이 win10.ios 가 생기게 됩니다. 선택을 누르고 확인을 누르면 됩니다.\n이제 시작을 눌러줍니다.\n윈도우 설치 시작을 누르면 윈도우10 설치 과정을 따라 가면 됩니다.\n윈도우를 설치하실 때에는 제품 키가 없음을 눌러서 진행하시면 됩니다.\n윈도우 버전은 본인의 필요에 따라 선택하면 됩니다. 가장 기본적인 기능은 Windows10 Home 입니다.\n약관에 동의하시고 사용자 지정 설치로 진행합니다.\n이후에는 본인의 취향에 맞추어서 설정을 선택하면서 진행하면 됩니다.\n","date":"2020-07-15T00:00:00Z","permalink":"https://aiden-jeon.github.io/blog/p/macos%EC%97%90-virtualbox-windows10-%EC%84%A4%EC%B9%98%ED%95%98%EA%B8%B0/","title":"MacOS에 Virtualbox Windows10 설치하기"}]